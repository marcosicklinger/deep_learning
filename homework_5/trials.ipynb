{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit6bfc133e9737495b83ae1f902c3270be",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0,3,45,5,3,2,8,5,0,4,2,4,5,7,6,8,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.remove(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[3, 45, 5, 3, 2, 8, 5, 0, 4, 2, 4, 5, 7, 6, 8, 0]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 45]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "list(set(a)-set([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scripts import mnist, train_utils, architectures, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MLPCustom(\n  (layers): Sequential(\n    (0): Flatten()\n    (1): Linear(in_features=784, out_features=16, bias=True)\n    (2): ReLU()\n    (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): Linear(in_features=16, out_features=32, bias=True)\n    (5): ReLU()\n    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Linear(in_features=32, out_features=64, bias=True)\n    (8): ReLU()\n    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): Linear(in_features=64, out_features=10, bias=True)\n    (11): ReLU()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    {\"n_in\": 784, \"n_out\": 16, \"batchnorm\": False},\n",
    "    {\"n_out\": 32, \"batchnorm\": True},\n",
    "    {\"n_out\": 64, \"batchnorm\": True},\n",
    "    {\"n_out\": 10, \"batchnorm\": True}\n",
    "]\n",
    "net = architectures.MLPCustom(layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[tensor([0.0205, 0.0292, 0.0177,  ..., 0.0224, 0.0023, 0.0195],\n",
       "        grad_fn=<ViewBackward>),\n",
       " tensor([0.0192, 0.0108, 0.0196, 0.0104, 0.0230, 0.0265, 0.0009, 0.0174, 0.0185,\n",
       "         0.0105, 0.0260, 0.0160, 0.0315, 0.0027, 0.0118, 0.0313],\n",
       "        grad_fn=<AbsBackward>),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        grad_fn=<AbsBackward>),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        grad_fn=<AbsBackward>),\n",
       " tensor([0.1449, 0.1347, 0.1354, 0.1508, 0.1591, 0.0585, 0.2240, 0.1679, 0.2359,\n",
       "         0.2481, 0.1333, 0.1536, 0.2199, 0.2184, 0.0987, 0.2391, 0.2218, 0.0429,\n",
       "         0.0913, 0.2087, 0.1382, 0.2180, 0.2392, 0.2365, 0.0301, 0.0269, 0.1886,\n",
       "         0.1191, 0.1891, 0.0598, 0.1176, 0.1218, 0.0680, 0.1679, 0.0834, 0.0975,\n",
       "         0.0752, 0.1322, 0.1437, 0.1893, 0.0864, 0.1981, 0.2277, 0.1117, 0.0417,\n",
       "         0.2261, 0.1613, 0.0468, 0.1224, 0.2328, 0.0944, 0.2382, 0.1320, 0.1615,\n",
       "         0.0749, 0.1045, 0.1582, 0.1743, 0.1955, 0.0500, 0.0289, 0.0699, 0.0267,\n",
       "         0.1902, 0.0051, 0.1054, 0.1441, 0.0233, 0.1909, 0.2249, 0.2124, 0.1342,\n",
       "         0.0616, 0.2383, 0.0125, 0.1110, 0.1079, 0.0233, 0.1876, 0.0103, 0.0426,\n",
       "         0.0069, 0.1643, 0.0207, 0.0745, 0.1947, 0.1754, 0.1614, 0.0702, 0.1734,\n",
       "         0.0671, 0.0199, 0.1506, 0.0940, 0.2467, 0.1524, 0.0936, 0.0540, 0.0336,\n",
       "         0.1371, 0.1733, 0.1674, 0.0858, 0.1867, 0.2294, 0.0802, 0.0250, 0.2206,\n",
       "         0.0051, 0.1476, 0.2141, 0.0373, 0.0789, 0.0276, 0.0712, 0.1965, 0.0824,\n",
       "         0.0748, 0.1603, 0.0665, 0.0202, 0.0322, 0.2421, 0.0457, 0.2162, 0.0461,\n",
       "         0.1658, 0.1723, 0.0241, 0.1348, 0.0015, 0.1922, 0.0391, 0.1109, 0.0626,\n",
       "         0.1819, 0.2197, 0.0691, 0.2029, 0.1644, 0.0358, 0.0418, 0.1424, 0.0418,\n",
       "         0.2239, 0.0380, 0.2088, 0.1180, 0.0655, 0.1851, 0.0591, 0.0442, 0.0881,\n",
       "         0.2114, 0.0927, 0.1352, 0.1541, 0.0073, 0.0576, 0.0356, 0.1048, 0.1465,\n",
       "         0.0468, 0.2124, 0.2272, 0.1915, 0.0756, 0.1796, 0.0309, 0.0644, 0.2364,\n",
       "         0.0838, 0.0657, 0.0543, 0.2469, 0.1571, 0.1130, 0.0676, 0.1583, 0.1295,\n",
       "         0.1440, 0.0560, 0.1998, 0.0493, 0.0891, 0.1092, 0.1048, 0.0751, 0.2476,\n",
       "         0.2191, 0.1951, 0.0351, 0.0884, 0.2230, 0.1104, 0.1570, 0.0026, 0.1079,\n",
       "         0.0810, 0.2109, 0.0510, 0.0970, 0.1862, 0.0798, 0.2454, 0.1052, 0.0366,\n",
       "         0.1221, 0.1170, 0.2490, 0.1501, 0.0129, 0.2055, 0.1278, 0.2164, 0.0425,\n",
       "         0.0726, 0.1591, 0.1007, 0.2059, 0.0197, 0.1454, 0.2167, 0.1307, 0.2362,\n",
       "         0.2377, 0.0178, 0.2070, 0.0975, 0.1173, 0.1236, 0.0630, 0.1276, 0.0517,\n",
       "         0.1635, 0.0010, 0.0727, 0.0716, 0.1413, 0.2403, 0.1802, 0.2169, 0.1053,\n",
       "         0.0915, 0.2478, 0.2116, 0.1126, 0.0125, 0.1819, 0.1629, 0.0316, 0.1926,\n",
       "         0.1842, 0.2474, 0.0410, 0.1483, 0.1056, 0.0259, 0.2154, 0.0746, 0.1718,\n",
       "         0.2482, 0.1966, 0.1381, 0.1346, 0.0374, 0.1953, 0.0687, 0.0322, 0.0393,\n",
       "         0.0266, 0.1150, 0.1877, 0.0165, 0.2101, 0.0161, 0.1520, 0.0127, 0.0410,\n",
       "         0.1451, 0.1364, 0.0181, 0.0430, 0.2279, 0.1962, 0.0801, 0.2391, 0.1304,\n",
       "         0.2094, 0.1070, 0.0052, 0.1957, 0.2008, 0.1139, 0.0449, 0.1511, 0.1059,\n",
       "         0.2467, 0.2191, 0.0482, 0.1928, 0.1797, 0.0496, 0.0763, 0.2477, 0.1212,\n",
       "         0.0452, 0.1712, 0.2147, 0.1782, 0.0885, 0.0462, 0.0160, 0.0185, 0.0960,\n",
       "         0.1323, 0.1333, 0.0814, 0.2268, 0.0050, 0.0147, 0.2257, 0.0359, 0.0472,\n",
       "         0.0165, 0.1035, 0.0418, 0.0291, 0.0969, 0.2007, 0.2482, 0.0908, 0.2357,\n",
       "         0.1221, 0.0352, 0.0950, 0.2031, 0.2446, 0.0069, 0.1540, 0.0948, 0.0715,\n",
       "         0.0831, 0.1763, 0.2190, 0.2211, 0.0933, 0.0095, 0.0855, 0.1515, 0.1756,\n",
       "         0.0307, 0.0054, 0.1700, 0.1158, 0.2449, 0.1305, 0.0886, 0.2474, 0.1962,\n",
       "         0.1694, 0.0990, 0.0798, 0.1341, 0.1804, 0.2018, 0.0945, 0.1898, 0.2265,\n",
       "         0.1517, 0.0463, 0.0122, 0.0803, 0.0049, 0.0469, 0.2046, 0.1510, 0.1241,\n",
       "         0.1801, 0.1133, 0.0361, 0.1593, 0.1704, 0.0923, 0.0529, 0.0131, 0.2238,\n",
       "         0.0796, 0.0029, 0.1848, 0.1843, 0.0246, 0.0579, 0.1298, 0.0844, 0.2492,\n",
       "         0.2111, 0.0015, 0.1601, 0.2490, 0.0262, 0.1057, 0.0646, 0.0939, 0.0614,\n",
       "         0.1427, 0.0270, 0.1696, 0.2193, 0.0035, 0.0315, 0.0455, 0.1420, 0.0182,\n",
       "         0.0768, 0.1886, 0.1982, 0.0999, 0.1805, 0.1066, 0.1060, 0.1825, 0.2132,\n",
       "         0.0250, 0.0030, 0.0743, 0.1457, 0.1268, 0.0614, 0.0180, 0.1857, 0.2364,\n",
       "         0.1030, 0.1714, 0.2454, 0.1458, 0.1307, 0.0508, 0.0869, 0.0512, 0.0372,\n",
       "         0.0390, 0.0814, 0.1927, 0.0004, 0.0059, 0.1550, 0.1929, 0.1419, 0.2448,\n",
       "         0.2024, 0.2270, 0.1141, 0.2455, 0.1411, 0.1354, 0.1169, 0.1318, 0.1100,\n",
       "         0.0720, 0.0172, 0.1938, 0.2265, 0.0064, 0.2299, 0.0135, 0.1849, 0.0031,\n",
       "         0.1709, 0.0914, 0.1553, 0.1903, 0.1904, 0.1370, 0.2368, 0.1467, 0.0375,\n",
       "         0.0023, 0.2102, 0.2118, 0.0378, 0.0558, 0.0911, 0.1896, 0.0863, 0.1643,\n",
       "         0.0691, 0.2145, 0.0799, 0.2278, 0.2464, 0.1838, 0.0143, 0.0525, 0.0006,\n",
       "         0.0341, 0.0564, 0.1342, 0.0234, 0.0803, 0.1586, 0.1767, 0.0593, 0.1544,\n",
       "         0.2450, 0.2084, 0.2038, 0.1689, 0.0035, 0.0128, 0.2033, 0.0210],\n",
       "        grad_fn=<ViewBackward>),\n",
       " tensor([0.0087, 0.1076, 0.1496, 0.1564, 0.0470, 0.2465, 0.0871, 0.1160, 0.0991,\n",
       "         0.0468, 0.1018, 0.0507, 0.1783, 0.2176, 0.0302, 0.0343, 0.0017, 0.0455,\n",
       "         0.1721, 0.1069, 0.0225, 0.2090, 0.1044, 0.1875, 0.1736, 0.1037, 0.0610,\n",
       "         0.2355, 0.0796, 0.0886, 0.2197, 0.0920], grad_fn=<AbsBackward>),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        grad_fn=<AbsBackward>),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<AbsBackward>),\n",
       " tensor([0.1030, 0.1546, 0.1060,  ..., 0.1613, 0.0682, 0.0374],\n",
       "        grad_fn=<ViewBackward>),\n",
       " tensor([0.1594, 0.1511, 0.1705, 0.1726, 0.0825, 0.0292, 0.1173, 0.0585, 0.0447,\n",
       "         0.1437, 0.1759, 0.1522, 0.1563, 0.0547, 0.0516, 0.1000, 0.1278, 0.1578,\n",
       "         0.1220, 0.1172, 0.0521, 0.1603, 0.0107, 0.1570, 0.0871, 0.0206, 0.0656,\n",
       "         0.0680, 0.1431, 0.0770, 0.0271, 0.1364, 0.0467, 0.1446, 0.0542, 0.1534,\n",
       "         0.1454, 0.0719, 0.1196, 0.1718, 0.0375, 0.0741, 0.0185, 0.0345, 0.1750,\n",
       "         0.1591, 0.0141, 0.1307, 0.0460, 0.0077, 0.1459, 0.0629, 0.0632, 0.1417,\n",
       "         0.1235, 0.0101, 0.1338, 0.1398, 0.1383, 0.0894, 0.1656, 0.0937, 0.0456,\n",
       "         0.0201], grad_fn=<AbsBackward>),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], grad_fn=<AbsBackward>),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        grad_fn=<AbsBackward>),\n",
       " tensor([8.8726e-02, 6.3026e-02, 6.6181e-02, 6.6596e-02, 5.9958e-02, 6.7838e-02,\n",
       "         1.0990e-01, 4.7541e-02, 9.4527e-03, 1.1328e-01, 6.2514e-03, 5.7234e-02,\n",
       "         6.7049e-02, 4.6305e-02, 2.5010e-02, 4.6939e-02, 9.3047e-02, 9.3786e-02,\n",
       "         1.0692e-01, 1.4679e-02, 5.8778e-02, 5.3392e-02, 1.1687e-01, 1.6636e-02,\n",
       "         7.5050e-03, 9.1310e-02, 1.3043e-02, 1.5067e-02, 8.2042e-02, 5.3795e-02,\n",
       "         4.9955e-02, 2.6325e-02, 9.1184e-02, 8.3258e-02, 7.9378e-04, 3.5930e-02,\n",
       "         7.1445e-02, 5.7168e-02, 2.8042e-02, 1.1583e-01, 8.3808e-02, 2.5378e-02,\n",
       "         4.3543e-02, 3.1428e-02, 9.5183e-02, 1.0570e-01, 1.0418e-01, 3.9562e-03,\n",
       "         7.3341e-02, 1.1738e-01, 1.1509e-01, 2.5466e-04, 4.2863e-02, 1.7058e-02,\n",
       "         8.1591e-02, 8.1049e-04, 8.9877e-02, 7.9975e-02, 6.4605e-02, 9.8203e-02,\n",
       "         8.0450e-02, 1.0549e-01, 1.1122e-01, 1.0819e-01, 6.5462e-02, 6.7471e-02,\n",
       "         7.7716e-02, 5.7996e-02, 4.5163e-02, 6.4657e-02, 7.7492e-02, 5.8465e-02,\n",
       "         8.9659e-02, 5.0612e-03, 8.3386e-02, 3.4310e-02, 6.7754e-02, 4.2710e-02,\n",
       "         1.1233e-01, 5.6968e-02, 1.1780e-02, 1.0242e-01, 3.2631e-02, 2.0946e-02,\n",
       "         6.9983e-02, 3.7473e-02, 8.1192e-02, 3.4454e-02, 6.7124e-02, 7.4464e-03,\n",
       "         6.6329e-03, 1.0956e-01, 3.4173e-02, 8.1510e-02, 9.3264e-02, 4.3530e-02,\n",
       "         5.4545e-02, 2.5144e-02, 3.2393e-02, 5.1724e-02, 5.9480e-02, 3.4386e-02,\n",
       "         8.2959e-02, 3.5666e-02, 1.0626e-01, 5.6347e-02, 5.8957e-02, 4.0680e-02,\n",
       "         1.9246e-03, 1.0450e-01, 1.0067e-01, 5.2871e-02, 1.2972e-02, 1.1323e-02,\n",
       "         1.0034e-01, 8.6012e-02, 1.1174e-01, 1.0387e-01, 1.1256e-01, 7.9627e-02,\n",
       "         4.0256e-02, 5.1217e-02, 1.8283e-02, 4.1295e-02, 1.1159e-01, 7.9303e-03,\n",
       "         8.1487e-02, 1.6355e-02, 6.6516e-03, 1.2464e-01, 6.5874e-02, 1.1986e-01,\n",
       "         5.9858e-02, 1.1943e-01, 8.7781e-03, 1.0432e-01, 3.5027e-02, 2.4055e-02,\n",
       "         7.0467e-02, 1.2235e-01, 4.4448e-02, 1.8577e-02, 4.4124e-02, 1.2872e-02,\n",
       "         9.2760e-03, 1.1385e-01, 1.1480e-01, 2.0597e-02, 4.3144e-02, 9.6976e-02,\n",
       "         8.7564e-02, 1.1078e-01, 1.2192e-01, 1.6460e-02, 8.7942e-02, 2.2214e-02,\n",
       "         8.5065e-02, 8.1314e-02, 5.7376e-02, 7.8431e-02, 9.3318e-02, 1.0379e-01,\n",
       "         1.0654e-01, 1.1384e-01, 7.7332e-02, 1.1929e-02, 8.1598e-02, 1.0071e-01,\n",
       "         3.4393e-02, 1.2424e-01, 5.2100e-02, 1.2394e-02, 1.0401e-01, 2.2053e-02,\n",
       "         2.5471e-02, 9.5673e-02, 4.2922e-03, 3.8614e-02, 2.5654e-02, 9.2317e-02,\n",
       "         1.0856e-01, 2.3640e-02, 9.0001e-02, 9.2480e-02, 1.1570e-01, 8.9046e-04,\n",
       "         1.0671e-01, 7.9639e-02, 7.6407e-02, 5.3189e-02, 1.4866e-02, 1.0277e-01,\n",
       "         7.1629e-02, 4.4620e-02, 1.0700e-02, 2.2483e-02, 2.3880e-02, 1.0091e-01,\n",
       "         3.4202e-02, 9.5634e-03, 4.4107e-02, 7.0939e-02, 1.0202e-01, 6.1733e-02,\n",
       "         4.5614e-02, 1.0909e-02, 1.0318e-01, 5.8791e-02, 1.5438e-03, 1.1652e-02,\n",
       "         2.9278e-02, 4.2281e-02, 4.7501e-02, 3.5271e-02, 8.4970e-02, 5.0105e-02,\n",
       "         6.4900e-03, 4.7667e-02, 6.5351e-02, 9.0830e-02, 8.3342e-02, 8.2048e-02,\n",
       "         8.6026e-02, 1.0828e-01, 1.2246e-02, 1.1493e-01, 1.2394e-02, 6.7814e-02,\n",
       "         5.2161e-02, 8.2323e-02, 7.6183e-02, 4.3221e-02, 7.4392e-02, 2.0158e-02,\n",
       "         9.5244e-02, 1.2042e-01, 3.4690e-03, 7.2395e-03, 1.3701e-02, 9.4978e-02,\n",
       "         1.0864e-01, 3.8987e-02, 3.9632e-02, 7.4590e-02, 1.9805e-04, 3.0950e-02,\n",
       "         1.2562e-02, 4.1147e-02, 1.2253e-01, 6.8843e-02, 4.4622e-02, 1.0852e-01,\n",
       "         3.3694e-02, 4.7827e-02, 1.1862e-01, 1.1680e-01, 9.2321e-02, 8.8824e-02,\n",
       "         1.0031e-01, 7.5817e-02, 6.5167e-02, 3.8523e-02, 7.0064e-02, 2.6140e-02,\n",
       "         6.1017e-02, 1.1674e-01, 1.2048e-01, 7.4563e-02, 5.5408e-02, 8.4445e-02,\n",
       "         4.9216e-03, 5.7642e-03, 1.0124e-02, 5.5268e-02, 6.2415e-02, 5.8198e-02,\n",
       "         8.8980e-02, 7.2324e-02, 2.5408e-02, 5.6226e-02, 4.3860e-02, 5.6373e-02,\n",
       "         7.0677e-02, 3.1188e-02, 8.0297e-02, 1.0163e-01, 2.2505e-02, 9.5026e-02,\n",
       "         4.5772e-02, 8.2001e-02, 1.0852e-01, 6.4430e-02, 6.6175e-02, 1.0371e-01,\n",
       "         8.1788e-02, 6.3423e-02, 5.2792e-02, 5.2099e-02, 5.9126e-02, 5.2888e-02,\n",
       "         1.0896e-01, 2.3204e-02, 4.2740e-02, 3.9014e-02, 3.4801e-02, 8.7877e-02,\n",
       "         1.4159e-02, 9.9326e-02, 3.9623e-03, 4.0740e-02, 1.2022e-01, 2.3699e-02,\n",
       "         5.3962e-02, 7.7350e-03, 3.3361e-02, 3.4905e-02, 2.3013e-02, 1.1935e-02,\n",
       "         1.5384e-02, 1.1171e-01, 1.8190e-02, 9.0810e-02, 6.9599e-02, 8.6858e-02,\n",
       "         3.1751e-02, 1.6002e-02, 1.0266e-01, 4.5131e-02, 1.8028e-02, 9.8706e-02,\n",
       "         4.6154e-02, 1.1781e-04, 8.5979e-03, 9.0064e-03, 5.0547e-02, 9.4784e-02,\n",
       "         3.4117e-03, 7.1322e-02, 9.5836e-02, 3.9136e-02, 3.3291e-02, 6.6993e-02,\n",
       "         3.9152e-02, 1.2110e-01, 7.8821e-02, 1.9147e-02, 3.3544e-02, 1.3625e-02,\n",
       "         1.6628e-02, 5.6839e-02, 1.6357e-02, 2.3570e-02, 5.0183e-02, 6.2771e-02,\n",
       "         6.8187e-02, 1.6565e-02, 2.1018e-02, 1.1060e-01, 1.9701e-02, 6.7378e-02,\n",
       "         8.8577e-02, 1.0733e-01, 1.1545e-01, 4.8100e-02, 2.9817e-02, 2.3236e-02,\n",
       "         1.4725e-02, 8.0131e-02, 4.9245e-02, 1.1489e-01, 1.4244e-02, 2.5622e-02,\n",
       "         7.7032e-02, 6.1696e-02, 1.0445e-01, 1.2472e-01, 8.8788e-02, 2.1659e-02,\n",
       "         1.1108e-01, 1.1831e-01, 8.0937e-02, 8.5173e-03, 8.3007e-02, 2.3216e-02,\n",
       "         4.5597e-02, 4.0618e-02, 2.6767e-03, 5.1858e-02, 7.2162e-03, 9.1111e-02,\n",
       "         4.2953e-02, 9.2430e-02, 6.9535e-02, 9.9352e-02, 6.9930e-02, 1.0039e-01,\n",
       "         1.9630e-02, 2.0589e-02, 1.0928e-01, 1.1625e-01, 1.0376e-01, 7.3768e-02,\n",
       "         1.1202e-01, 3.9503e-02, 1.1490e-01, 7.5127e-02, 6.7376e-02, 1.2127e-01,\n",
       "         7.8001e-02, 9.8539e-03, 9.4777e-02, 3.8912e-02, 9.5574e-02, 6.4271e-02,\n",
       "         9.0574e-02, 1.9649e-02, 1.1107e-01, 5.2434e-03, 4.4750e-02, 1.2409e-03,\n",
       "         6.8088e-02, 3.4041e-03, 4.2307e-02, 6.3435e-02, 3.4445e-02, 8.7770e-02,\n",
       "         1.2480e-01, 7.0602e-02, 4.8219e-02, 1.2142e-01, 1.1817e-01, 3.0073e-02,\n",
       "         3.6462e-02, 6.5589e-02, 1.2304e-01, 1.1724e-01, 8.8661e-03, 1.7826e-02,\n",
       "         1.1914e-01, 1.7562e-02, 5.8660e-02, 6.7075e-02, 8.2490e-02, 1.1043e-01,\n",
       "         3.4558e-02, 3.5268e-02, 8.7256e-02, 1.1638e-01, 1.0100e-01, 6.8757e-02,\n",
       "         1.3794e-02, 3.4621e-02, 8.6334e-02, 7.7349e-02, 9.4019e-02, 1.1735e-01,\n",
       "         1.0489e-01, 7.3044e-02, 4.8971e-02, 5.3729e-02, 6.5665e-02, 6.4554e-02,\n",
       "         8.5613e-02, 1.1584e-02, 1.0759e-01, 4.8376e-02, 8.5610e-02, 1.1774e-01,\n",
       "         8.7080e-02, 3.9964e-02, 1.0667e-01, 8.3860e-02, 2.7864e-02, 1.2374e-01,\n",
       "         6.5913e-02, 5.7406e-02, 1.2474e-02, 9.0809e-02, 1.0655e-01, 1.2413e-01,\n",
       "         1.1601e-01, 1.0754e-01, 5.3463e-02, 7.9003e-02, 1.1253e-01, 1.2505e-02,\n",
       "         6.0250e-02, 7.4470e-03, 3.0530e-03, 2.5321e-03, 1.7698e-02, 1.1258e-01,\n",
       "         8.2867e-02, 4.4293e-03, 3.6210e-02, 8.1180e-02, 6.5922e-02, 9.8397e-02,\n",
       "         5.2021e-02, 1.0541e-01, 2.5717e-02, 7.8905e-02, 9.5034e-02, 9.7379e-02,\n",
       "         9.8860e-02, 1.1171e-01, 7.9418e-02, 8.7675e-02, 1.1442e-01, 2.2559e-02,\n",
       "         3.1569e-02, 1.2300e-01, 1.1578e-01, 6.2837e-02, 6.6083e-02, 3.3481e-02,\n",
       "         1.7571e-02, 3.2044e-03, 7.8270e-02, 7.0098e-02, 4.5666e-02, 3.3488e-02,\n",
       "         3.9762e-02, 6.4664e-02, 9.1123e-02, 5.4191e-02, 6.3429e-02, 1.2429e-01,\n",
       "         9.1891e-02, 1.2242e-01, 9.3673e-02, 8.2730e-02, 2.5746e-02, 7.4789e-02,\n",
       "         4.9383e-02, 4.7844e-02, 1.6223e-02, 1.3177e-02, 1.1190e-01, 1.1269e-02,\n",
       "         3.5813e-02, 9.9543e-02, 1.8837e-02, 3.0820e-02, 3.2811e-02, 3.1180e-02,\n",
       "         6.3200e-02, 1.1735e-01, 3.3285e-02, 2.4488e-02, 6.7631e-02, 7.8565e-04,\n",
       "         1.2413e-01, 1.2056e-02, 1.5971e-02, 1.4759e-03, 5.4000e-02, 7.1390e-02,\n",
       "         8.1164e-02, 7.9433e-02, 8.2084e-02, 3.8177e-02, 3.5127e-02, 7.1831e-02,\n",
       "         4.1718e-02, 1.1930e-01, 1.6070e-02, 1.4261e-02, 4.5872e-02, 1.1521e-01,\n",
       "         5.1054e-02, 6.7871e-02, 1.3169e-02, 8.1725e-02, 9.9573e-02, 7.3150e-02,\n",
       "         2.8794e-02, 1.0276e-01, 7.5112e-02, 2.8387e-03, 1.8484e-02, 7.2372e-02,\n",
       "         1.0248e-01, 1.0799e-01, 2.7096e-02, 7.9758e-02, 8.3437e-03, 8.6376e-02,\n",
       "         6.4270e-02, 9.2262e-02, 1.3306e-02, 7.0178e-02, 4.1599e-02, 8.2684e-02,\n",
       "         9.4479e-03, 3.7438e-02, 7.7938e-02, 3.1734e-02, 5.6706e-03, 1.0143e-01,\n",
       "         2.8401e-02, 1.2382e-01, 1.8611e-02, 8.4712e-02, 1.0900e-01, 9.6864e-02,\n",
       "         7.5536e-02, 9.1340e-02, 1.1580e-01, 9.4225e-02, 6.5398e-02, 1.0031e-01,\n",
       "         7.3598e-02, 4.9461e-02, 5.5930e-02, 6.5018e-02, 9.8154e-02, 4.9272e-02,\n",
       "         1.7537e-02, 9.0614e-02, 5.2457e-02, 5.0402e-02, 8.5631e-02, 9.7328e-02,\n",
       "         1.0957e-01, 9.8393e-02, 8.6181e-02, 4.6498e-02, 1.0700e-01, 5.1595e-02,\n",
       "         1.7769e-02, 6.7869e-02, 8.2719e-02, 1.7691e-02, 7.5171e-02, 2.4417e-02,\n",
       "         9.8782e-02, 9.1393e-02, 4.9829e-02, 1.1358e-01],\n",
       "        grad_fn=<ViewBackward>),\n",
       " tensor([0.1109, 0.1230, 0.1066, 0.0100, 0.0169, 0.0160, 0.0470, 0.0663, 0.0378,\n",
       "         0.0821], grad_fn=<AbsBackward>)]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "[pars.abs().flatten() for pars in net.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[torch.Size([16, 784]),\n",
       " torch.Size([16]),\n",
       " torch.Size([16]),\n",
       " torch.Size([16]),\n",
       " torch.Size([32, 16]),\n",
       " torch.Size([32]),\n",
       " torch.Size([32]),\n",
       " torch.Size([32]),\n",
       " torch.Size([64, 32]),\n",
       " torch.Size([64]),\n",
       " torch.Size([64]),\n",
       " torch.Size([64]),\n",
       " torch.Size([10, 64]),\n",
       " torch.Size([10])]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "[pars.shape for pars in net.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([0.0205, 0.0292, 0.0177,  ..., 0.0663, 0.0378, 0.0821],\n",
       "        grad_fn=<CatBackward>),\n",
       " torch.Size([16090]))"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "fl1 = torch.cat([pars.abs().flatten() for pars in net.parameters()], dim=0)\n",
    "fl1, fl1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([0.0205, 0.0292, 0.0177,  ..., 0.0663, 0.0378, 0.0821],\n",
       "        grad_fn=<ViewBackward>),\n",
       " torch.Size([12872]))"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "fl1 = fl1[fl1.nonzero()].reshape(fl1[fl1.nonzero()].shape[0])\n",
    "fl1, fl1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([0.0205, 0.0292, 0.0177,  ..., 0.0663, 0.0378, 0.0821],\n",
       "        grad_fn=<ViewBackward>),\n",
       " torch.Size([12872]))"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "fl1 = fl1.reshape(fl1.shape[0])\n",
    "fl1, fl1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([0.0000, 0.0085, 0.0085,  ..., 0.2490, 0.2492, 1.0000],\n",
       "        grad_fn=<NotImplemented>),\n",
       " torch.Size([12752]))"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "fl2 = torch.unique(torch.cat([pars.abs().flatten() for pars in net.parameters()], dim=0).sort()[0])\n",
    "fl2, fl2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([16090])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "flat = torch.cat([pars.abs().flatten() for pars in net.parameters()], dim=0)\n",
    "flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = torch.cat([pars.abs().flatten() for pars in net.parameters()], dim=0)\n",
    "\n",
    "flat = flat.sort()[0]\n",
    "pruning_rate = 0.2\n",
    "position = int(pruning_rate*flat.shape[0])\n",
    "thresh = flat[position]\n",
    "\n",
    "mask = [torch.where( pars.abs() >= thresh, torch.ones(pars.shape), torch.zeros(pars.shape) ) for pars in net.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 0., 0.,  ..., 1., 1., 1.],\n        [1., 1., 0.,  ..., 1., 1., 1.],\n        ...,\n        [1., 1., 0.,  ..., 0., 0., 1.],\n        [1., 0., 1.,  ..., 1., 0., 1.],\n        [1., 1., 0.,  ..., 1., 0., 1.]])\ntensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\ntensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.]])\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0.])\ntensor([[1., 1., 1.,  ..., 1., 1., 0.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        ...,\n        [1., 0., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.]])\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\ntensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n         1., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 0., 1., 1.],\n        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n         1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n         1., 1., 1., 0., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 0., 1., 1.],\n        [1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n         1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "for m in mask:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor([[-0.0205,  0.0292,  0.0177,  ...,  0.0309,  0.0136,  0.0104],\n        [ 0.0208,  0.0000,  0.0000,  ...,  0.0185, -0.0223,  0.0323],\n        [ 0.0184,  0.0088,  0.0000,  ..., -0.0299,  0.0247,  0.0301],\n        ...,\n        [-0.0324,  0.0250, -0.0000,  ..., -0.0000, -0.0000, -0.0338],\n        [ 0.0328,  0.0000,  0.0155,  ...,  0.0176, -0.0000,  0.0229],\n        [-0.0192,  0.0222,  0.0000,  ...,  0.0224,  0.0000,  0.0195]],\n       requires_grad=True)\nParameter containing:\ntensor([-0.0192,  0.0108, -0.0196,  0.0104,  0.0230,  0.0265,  0.0000, -0.0174,\n         0.0185,  0.0105,  0.0260,  0.0160,  0.0315,  0.0000, -0.0118, -0.0313],\n       requires_grad=True)\nParameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       requires_grad=True)\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       requires_grad=True)\nParameter containing:\ntensor([[ 0.1449,  0.1347,  0.1354,  0.1508, -0.1591,  0.0585,  0.2240,  0.1679,\n         -0.2359, -0.2481, -0.1333,  0.1536,  0.2199,  0.2184,  0.0987,  0.2391],\n        [ 0.2218, -0.0429,  0.0913,  0.2087,  0.1382,  0.2180, -0.2392,  0.2365,\n         -0.0301, -0.0269, -0.1886, -0.1191, -0.1891, -0.0598, -0.1176,  0.1218],\n        [ 0.0680,  0.1679, -0.0834,  0.0975,  0.0752,  0.1322,  0.1437, -0.1893,\n         -0.0864,  0.1981,  0.2277, -0.1117, -0.0417,  0.2261, -0.1613, -0.0468],\n        [-0.1224, -0.2328,  0.0944, -0.2382,  0.1320,  0.1615, -0.0749,  0.1045,\n          0.1582,  0.1743, -0.1955,  0.0500,  0.0289,  0.0699, -0.0267,  0.1902],\n        [ 0.0000,  0.1054,  0.1441, -0.0233,  0.1909, -0.2249, -0.2124,  0.1342,\n         -0.0616,  0.2383,  0.0125,  0.1110,  0.1079, -0.0233, -0.1876, -0.0103],\n        [ 0.0426,  0.0000,  0.1643, -0.0207,  0.0745, -0.1947,  0.1754,  0.1614,\n          0.0702, -0.1734, -0.0671,  0.0199, -0.1506,  0.0940,  0.2467, -0.1524],\n        [-0.0936, -0.0540, -0.0336, -0.1371, -0.1733,  0.1674, -0.0858, -0.1867,\n          0.2294, -0.0802, -0.0250,  0.2206, -0.0000, -0.1476,  0.2141, -0.0373],\n        [ 0.0789,  0.0276, -0.0712, -0.1965, -0.0824,  0.0748,  0.1603,  0.0665,\n          0.0202, -0.0322,  0.2421, -0.0457, -0.2162,  0.0461,  0.1658, -0.1723],\n        [-0.0241, -0.1348, -0.0000,  0.1922, -0.0391,  0.1109, -0.0626, -0.1819,\n          0.2197,  0.0691, -0.2029,  0.1644,  0.0358, -0.0418, -0.1424,  0.0418],\n        [-0.2239,  0.0380,  0.2088, -0.1180, -0.0655, -0.1851, -0.0591,  0.0442,\n         -0.0881,  0.2114, -0.0927,  0.1352, -0.1541,  0.0000, -0.0576,  0.0356],\n        [ 0.1048, -0.1465, -0.0468,  0.2124,  0.2272, -0.1915,  0.0756,  0.1796,\n          0.0309, -0.0644,  0.2364,  0.0838, -0.0657, -0.0543,  0.2469,  0.1571],\n        [-0.1130, -0.0676,  0.1583, -0.1295,  0.1440,  0.0560,  0.1998, -0.0493,\n         -0.0891, -0.1092, -0.1048,  0.0751,  0.2476,  0.2191, -0.1951, -0.0351],\n        [ 0.0884, -0.2230,  0.1104, -0.1570, -0.0000,  0.1079,  0.0810,  0.2109,\n         -0.0510, -0.0970, -0.1862,  0.0798,  0.2454,  0.1052, -0.0366,  0.1221],\n        [ 0.1170,  0.2490, -0.1501, -0.0129, -0.2055, -0.1278,  0.2164,  0.0425,\n          0.0726, -0.1591, -0.1007, -0.2059, -0.0197,  0.1454,  0.2167,  0.1307],\n        [-0.2362, -0.2377, -0.0178, -0.2070, -0.0975, -0.1173, -0.1236, -0.0630,\n         -0.1276,  0.0517, -0.1635,  0.0000,  0.0727,  0.0716,  0.1413,  0.2403],\n        [ 0.1802,  0.2169,  0.1053, -0.0915,  0.2478, -0.2116, -0.1126,  0.0125,\n         -0.1819, -0.1629, -0.0316,  0.1926,  0.1842, -0.2474,  0.0410, -0.1483],\n        [ 0.1056,  0.0259,  0.2154, -0.0746, -0.1718,  0.2482, -0.1966,  0.1381,\n         -0.1346,  0.0374,  0.1953, -0.0687, -0.0322,  0.0393, -0.0266, -0.1150],\n        [-0.1877,  0.0165,  0.2101, -0.0161,  0.1520,  0.0127, -0.0410, -0.1451,\n         -0.1364, -0.0181,  0.0430,  0.2279, -0.1962, -0.0801, -0.2391, -0.1304],\n        [ 0.2094,  0.1070, -0.0000, -0.1957,  0.2008,  0.1139,  0.0449,  0.1511,\n          0.1059, -0.2467,  0.2191, -0.0482, -0.1928, -0.1797,  0.0496, -0.0763],\n        [-0.2477,  0.1212, -0.0452,  0.1712,  0.2147,  0.1782, -0.0885,  0.0462,\n          0.0160,  0.0185, -0.0960, -0.1323, -0.1333, -0.0814, -0.2268, -0.0000],\n        [-0.0147, -0.2257,  0.0359, -0.0472, -0.0165,  0.1035,  0.0418, -0.0291,\n         -0.0969, -0.2007,  0.2482, -0.0908,  0.2357, -0.1221, -0.0352, -0.0950],\n        [ 0.2031,  0.2446, -0.0000,  0.1540,  0.0948, -0.0715, -0.0831, -0.1763,\n          0.2190, -0.2211,  0.0933,  0.0095, -0.0855, -0.1515, -0.1756, -0.0307],\n        [-0.0000,  0.1700, -0.1158,  0.2449, -0.1305, -0.0886, -0.2474, -0.1962,\n          0.1694, -0.0990,  0.0798,  0.1341, -0.1804, -0.2018,  0.0945, -0.1898],\n        [ 0.2265,  0.1517,  0.0463,  0.0122, -0.0803, -0.0000, -0.0469,  0.2046,\n          0.1510,  0.1241, -0.1801,  0.1133,  0.0361, -0.1593, -0.1704,  0.0923],\n        [-0.0529, -0.0131, -0.2238,  0.0796,  0.0000, -0.1848,  0.1843, -0.0246,\n         -0.0579, -0.1298,  0.0844, -0.2492, -0.2111, -0.0000,  0.1601,  0.2490],\n        [ 0.0262,  0.1057,  0.0646,  0.0939,  0.0614, -0.1427,  0.0270, -0.1696,\n         -0.2193,  0.0000,  0.0315, -0.0455, -0.1420,  0.0182, -0.0768, -0.1886],\n        [-0.1982, -0.0999, -0.1805, -0.1066,  0.1060, -0.1825,  0.2132, -0.0250,\n         -0.0000,  0.0743, -0.1457,  0.1268, -0.0614,  0.0180,  0.1857, -0.2364],\n        [-0.1030,  0.1714,  0.2454, -0.1458,  0.1307,  0.0508, -0.0869,  0.0512,\n         -0.0372, -0.0390,  0.0814, -0.1927,  0.0000,  0.0000,  0.1550, -0.1929],\n        [ 0.1419,  0.2448, -0.2024, -0.2270, -0.1141, -0.2455,  0.1411, -0.1354,\n         -0.1169, -0.1318,  0.1100, -0.0720, -0.0172, -0.1938, -0.2265,  0.0000],\n        [ 0.2299,  0.0135, -0.1849,  0.0000,  0.1709,  0.0914,  0.1553,  0.1903,\n         -0.1904, -0.1370,  0.2368, -0.1467,  0.0375,  0.0000, -0.2102,  0.2118],\n        [ 0.0378, -0.0558, -0.0911, -0.1896, -0.0863,  0.1643,  0.0691,  0.2145,\n          0.0799,  0.2278,  0.2464, -0.1838, -0.0143, -0.0525, -0.0000, -0.0341],\n        [ 0.0564,  0.1342,  0.0234,  0.0803,  0.1586, -0.1767, -0.0593,  0.1544,\n          0.2450, -0.2084,  0.2038, -0.1689,  0.0000, -0.0128, -0.2033,  0.0210]],\n       requires_grad=True)\nParameter containing:\ntensor([ 0.0087,  0.1076, -0.1496,  0.1564,  0.0470, -0.2465,  0.0871,  0.1160,\n         0.0991,  0.0468,  0.1018, -0.0507,  0.1783, -0.2176,  0.0302,  0.0343,\n        -0.0000,  0.0455, -0.1721,  0.1069,  0.0225, -0.2090,  0.1044, -0.1875,\n         0.1736, -0.1037,  0.0610, -0.2355,  0.0796,  0.0886,  0.2197, -0.0920],\n       requires_grad=True)\nParameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       requires_grad=True)\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\nParameter containing:\ntensor([[ 0.1030,  0.1546, -0.1060,  ..., -0.0526, -0.0094,  0.0000],\n        [ 0.0260,  0.1647,  0.0500,  ...,  0.1721, -0.1287, -0.1328],\n        [ 0.1020,  0.0974,  0.1677,  ...,  0.0546,  0.0442,  0.1213],\n        ...,\n        [-0.1347,  0.0000, -0.1201,  ..., -0.0622, -0.0995, -0.1187],\n        [-0.0397,  0.0127,  0.0366,  ..., -0.0949,  0.1251, -0.1583],\n        [-0.0732, -0.0640, -0.0166,  ..., -0.1613,  0.0682, -0.0374]],\n       requires_grad=True)\nParameter containing:\ntensor([ 0.1594,  0.1511, -0.1705, -0.1726,  0.0825, -0.0292,  0.1173, -0.0585,\n        -0.0447,  0.1437,  0.1759,  0.1522,  0.1563,  0.0547,  0.0516,  0.1000,\n         0.1278, -0.1578,  0.1220,  0.1172,  0.0521, -0.1603, -0.0107, -0.1570,\n         0.0871,  0.0206,  0.0656,  0.0680,  0.1431, -0.0770, -0.0271,  0.1364,\n        -0.0467,  0.1446,  0.0542, -0.1534,  0.1454, -0.0719, -0.1196, -0.1718,\n         0.0375,  0.0741,  0.0185,  0.0345, -0.1750,  0.1591,  0.0141,  0.1307,\n        -0.0460, -0.0000, -0.1459, -0.0629, -0.0632, -0.1417,  0.1235,  0.0101,\n         0.1338, -0.1398, -0.1383, -0.0894,  0.1656, -0.0937,  0.0456, -0.0201],\n       requires_grad=True)\nParameter containing:\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       requires_grad=True)\nParameter containing:\ntensor([[-0.0887, -0.0630,  0.0662, -0.0666,  0.0600,  0.0678, -0.1099, -0.0475,\n         -0.0095,  0.1133, -0.0000,  0.0572, -0.0670,  0.0463, -0.0250, -0.0469,\n         -0.0930, -0.0938, -0.1069, -0.0147, -0.0588,  0.0534, -0.1169, -0.0166,\n         -0.0000,  0.0913,  0.0130, -0.0151, -0.0820, -0.0538, -0.0500, -0.0263,\n          0.0912,  0.0833,  0.0000,  0.0359,  0.0714, -0.0572, -0.0280, -0.1158,\n          0.0838,  0.0254, -0.0435, -0.0314,  0.0952, -0.1057,  0.1042, -0.0000,\n          0.0733, -0.1174, -0.1151, -0.0000,  0.0429, -0.0171,  0.0816,  0.0000,\n          0.0899, -0.0800,  0.0646, -0.0982, -0.0804,  0.1055,  0.1112,  0.1082],\n        [-0.0655,  0.0675,  0.0777, -0.0580, -0.0452,  0.0647, -0.0775, -0.0585,\n          0.0897,  0.0000, -0.0834, -0.0343,  0.0678,  0.0427, -0.1123, -0.0570,\n          0.0118, -0.1024,  0.0326,  0.0209, -0.0700,  0.0375, -0.0812,  0.0345,\n         -0.0671, -0.0000, -0.0000,  0.1096,  0.0342,  0.0815, -0.0933,  0.0435,\n         -0.0545, -0.0251,  0.0324, -0.0517,  0.0595,  0.0344,  0.0830, -0.0357,\n         -0.1063, -0.0563, -0.0590,  0.0407, -0.0000,  0.1045,  0.1007,  0.0529,\n          0.0130,  0.0113, -0.1003,  0.0860, -0.1117,  0.1039, -0.1126,  0.0796,\n          0.0403,  0.0512, -0.0183,  0.0413, -0.1116,  0.0000, -0.0815,  0.0164],\n        [-0.0000,  0.1246, -0.0659, -0.1199, -0.0599, -0.1194, -0.0088,  0.1043,\n         -0.0350, -0.0241,  0.0705,  0.1223, -0.0444, -0.0186, -0.0441, -0.0129,\n         -0.0093,  0.1138,  0.1148, -0.0206,  0.0431,  0.0970, -0.0876,  0.1108,\n          0.1219, -0.0165, -0.0879,  0.0222,  0.0851, -0.0813, -0.0574,  0.0784,\n         -0.0933,  0.1038,  0.1065,  0.1138, -0.0773, -0.0119,  0.0816,  0.1007,\n          0.0344, -0.1242, -0.0521,  0.0124, -0.1040,  0.0221,  0.0255,  0.0957,\n         -0.0000, -0.0386,  0.0257,  0.0923, -0.1086, -0.0236,  0.0900,  0.0925,\n         -0.1157,  0.0000, -0.1067,  0.0796,  0.0764, -0.0532, -0.0149,  0.1028],\n        [-0.0716, -0.0446, -0.0107,  0.0225,  0.0239,  0.1009, -0.0342,  0.0096,\n          0.0441, -0.0709,  0.1020,  0.0617, -0.0456,  0.0109,  0.1032,  0.0588,\n         -0.0000, -0.0117,  0.0293, -0.0423,  0.0475,  0.0353,  0.0850,  0.0501,\n         -0.0000,  0.0477,  0.0654,  0.0908, -0.0833, -0.0820, -0.0860,  0.1083,\n          0.0122,  0.1149, -0.0124,  0.0678, -0.0522, -0.0823,  0.0762,  0.0432,\n         -0.0744, -0.0202, -0.0952, -0.1204,  0.0000, -0.0000, -0.0137, -0.0950,\n          0.1086,  0.0390, -0.0396,  0.0746,  0.0000,  0.0309,  0.0126,  0.0411,\n          0.1225,  0.0688,  0.0446, -0.1085,  0.0337, -0.0478, -0.1186, -0.1168],\n        [-0.0923,  0.0888, -0.1003, -0.0758, -0.0652,  0.0385,  0.0701, -0.0261,\n          0.0610, -0.1167, -0.1205,  0.0746, -0.0554,  0.0844,  0.0000, -0.0000,\n         -0.0101, -0.0553,  0.0624, -0.0582,  0.0890, -0.0723, -0.0254, -0.0562,\n          0.0439, -0.0564,  0.0707, -0.0312, -0.0803,  0.1016, -0.0225, -0.0950,\n         -0.0458,  0.0820, -0.1085,  0.0644,  0.0662,  0.1037, -0.0818,  0.0634,\n         -0.0528,  0.0521, -0.0591,  0.0529, -0.1090, -0.0232, -0.0427, -0.0390,\n          0.0348, -0.0879,  0.0142,  0.0993, -0.0000,  0.0407,  0.1202,  0.0237,\n          0.0540, -0.0000, -0.0334, -0.0349,  0.0230,  0.0119, -0.0154,  0.1117],\n        [ 0.0182, -0.0908, -0.0696, -0.0869, -0.0318, -0.0160,  0.1027,  0.0451,\n         -0.0180,  0.0987,  0.0462, -0.0000,  0.0086,  0.0090,  0.0505,  0.0948,\n          0.0000, -0.0713, -0.0958,  0.0391, -0.0333, -0.0670, -0.0392,  0.1211,\n         -0.0788, -0.0191,  0.0335,  0.0136, -0.0166,  0.0568, -0.0164,  0.0236,\n         -0.0502,  0.0628, -0.0682,  0.0166,  0.0210, -0.1106,  0.0197,  0.0674,\n          0.0886,  0.1073, -0.1154, -0.0481,  0.0298,  0.0232,  0.0147,  0.0801,\n         -0.0492,  0.1149, -0.0142,  0.0256,  0.0770,  0.0617,  0.1045, -0.1247,\n          0.0888,  0.0217,  0.1111,  0.1183,  0.0809, -0.0000,  0.0830, -0.0232],\n        [-0.0456, -0.0406,  0.0000,  0.0519,  0.0000, -0.0911,  0.0430, -0.0924,\n          0.0695,  0.0994, -0.0699, -0.1004,  0.0196, -0.0206,  0.1093,  0.1163,\n         -0.1038,  0.0738, -0.1120, -0.0395,  0.1149,  0.0751,  0.0674,  0.1213,\n          0.0780, -0.0099, -0.0948, -0.0389,  0.0956,  0.0643,  0.0906,  0.0196,\n         -0.1111, -0.0000,  0.0448,  0.0000,  0.0681, -0.0000, -0.0423, -0.0634,\n         -0.0344, -0.0878, -0.1248, -0.0706,  0.0482, -0.1214, -0.1182,  0.0301,\n         -0.0365,  0.0656,  0.1230, -0.1172,  0.0089,  0.0178, -0.1191, -0.0176,\n         -0.0587,  0.0671,  0.0825,  0.1104, -0.0346,  0.0353,  0.0873, -0.1164],\n        [-0.1010,  0.0688, -0.0138,  0.0346,  0.0863,  0.0773,  0.0940,  0.1173,\n          0.1049, -0.0730, -0.0490, -0.0537,  0.0657, -0.0646,  0.0856,  0.0116,\n          0.1076,  0.0484, -0.0856, -0.1177,  0.0871,  0.0400, -0.1067,  0.0839,\n         -0.0279, -0.1237,  0.0659, -0.0574,  0.0125, -0.0908,  0.1066,  0.1241,\n         -0.1160,  0.1075,  0.0535,  0.0790, -0.1125, -0.0125,  0.0603,  0.0000,\n          0.0000,  0.0000, -0.0177, -0.1126,  0.0829,  0.0000, -0.0362, -0.0812,\n         -0.0659, -0.0984, -0.0520,  0.1054,  0.0257,  0.0789, -0.0950, -0.0974,\n         -0.0989, -0.1117, -0.0794, -0.0877,  0.1144, -0.0226,  0.0316,  0.1230],\n        [-0.1158,  0.0628,  0.0661,  0.0335, -0.0176,  0.0000,  0.0783, -0.0701,\n          0.0457,  0.0335, -0.0398, -0.0647, -0.0911,  0.0542,  0.0634, -0.1243,\n          0.0919,  0.1224, -0.0937,  0.0827, -0.0257,  0.0748, -0.0494, -0.0478,\n         -0.0162,  0.0132,  0.1119,  0.0113, -0.0358, -0.0995,  0.0188, -0.0308,\n          0.0328, -0.0312,  0.0632,  0.1174,  0.0333,  0.0245, -0.0676, -0.0000,\n          0.1241, -0.0121, -0.0160,  0.0000, -0.0540,  0.0714, -0.0812,  0.0794,\n          0.0821, -0.0382, -0.0351,  0.0718,  0.0417, -0.1193,  0.0161, -0.0143,\n          0.0459,  0.1152, -0.0511,  0.0679, -0.0132,  0.0817,  0.0996,  0.0731],\n        [ 0.0288, -0.1028, -0.0751,  0.0000, -0.0185, -0.0724, -0.1025, -0.1080,\n         -0.0271, -0.0798,  0.0000,  0.0864, -0.0643, -0.0923,  0.0133, -0.0702,\n         -0.0416, -0.0827,  0.0094,  0.0374, -0.0779,  0.0317,  0.0000, -0.1014,\n          0.0284,  0.1238, -0.0186,  0.0847, -0.1090, -0.0969, -0.0755, -0.0913,\n          0.1158,  0.0942,  0.0654,  0.1003, -0.0736,  0.0495, -0.0559,  0.0650,\n          0.0982,  0.0493, -0.0175,  0.0906, -0.0525,  0.0504,  0.0856,  0.0973,\n         -0.1096,  0.0984,  0.0862,  0.0465, -0.1070, -0.0516, -0.0178,  0.0679,\n         -0.0827,  0.0177, -0.0752,  0.0244,  0.0988, -0.0914, -0.0498, -0.1136]],\n       requires_grad=True)\nParameter containing:\ntensor([ 0.1109,  0.1230,  0.1066, -0.0100,  0.0169, -0.0160, -0.0470,  0.0663,\n        -0.0378,  0.0821], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param, m in zip(net.parameters(), mask):\n",
    "        param.data *= m\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}