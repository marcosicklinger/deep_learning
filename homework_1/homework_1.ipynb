{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Homework 1\n",
    "### Marco Sicklinger, 03/2021"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Question 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as pt\n",
    "import random"
   ]
  },
  {
   "source": [
    "Defining more sophisticated `print` function (the one used in the laboratory)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(obj, title=None):\n",
    "    if title is not None:\n",
    "        print(title)\n",
    "    print(obj)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "source": [
    "Creating class for multi-layer perceptrons:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(pt.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create members to simulate layers\n",
    "        self._h_layer_1 = pt.nn.Linear(in_features = 5, out_features = 11, bias = False)\n",
    "        self._h_layer_2 = pt.nn.Linear(in_features = 11, out_features = 16, bias = False)\n",
    "        self._h_layer_3 = pt.nn.Linear(in_features = 16, out_features = 13, bias = False)\n",
    "        self._h_layer_4 = pt.nn.Linear(in_features = 13, out_features = 8, bias = False)\n",
    "        self._o_layer = pt.nn.Linear(in_features = 8, out_features = 4, bias = False)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        out = self._h_layer_1(X)\n",
    "        out = pt.nn.functional.relu(out)\n",
    "\n",
    "        out = self._h_layer_2(out)\n",
    "        out = pt.nn.functional.relu(out)\n",
    "\n",
    "        out = self._h_layer_3(out)\n",
    "        out = pt.nn.functional.relu(out)\n",
    "\n",
    "        out = self._h_layer_4(out)\n",
    "        out = pt.nn.functional.relu(out)\n",
    "\n",
    "        out = self._o_layer(out)\n",
    "        out = pt.nn.functional.softmax(out, dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "        "
   ]
  },
  {
   "source": [
    "## Question 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Creating instance of `MultiLayerPerceptron` class:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultiLayerPerceptron()"
   ]
  },
  {
   "source": [
    "#### Print summary with standard method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Multi-Layer Perceptron\nMultiLayerPerceptron(\n  (_h_layer_1): Linear(in_features=5, out_features=11, bias=False)\n  (_h_layer_2): Linear(in_features=11, out_features=16, bias=False)\n  (_h_layer_3): Linear(in_features=16, out_features=13, bias=False)\n  (_h_layer_4): Linear(in_features=13, out_features=8, bias=False)\n  (_o_layer): Linear(in_features=8, out_features=4, bias=False)\n)\n\n\n"
     ]
    }
   ],
   "source": [
    "pretty_print(mlp, \"Multi-Layer Perceptron\")"
   ]
  },
  {
   "source": [
    "#### Print summary with `torchsummary.summary` "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\n├─Linear: 1-1                            55\n├─Linear: 1-2                            176\n├─Linear: 1-3                            208\n├─Linear: 1-4                            104\n├─Linear: 1-5                            32\n=================================================================\nTotal params: 575\nTrainable params: 575\nNon-trainable params: 0\n=================================================================\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Linear: 1-1                            55\n",
       "├─Linear: 1-2                            176\n",
       "├─Linear: 1-3                            208\n",
       "├─Linear: 1-4                            104\n",
       "├─Linear: 1-5                            32\n",
       "=================================================================\n",
       "Total params: 575\n",
       "Trainable params: 575\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "summary(mlp)"
   ]
  },
  {
   "source": [
    "## Question 3\n",
    "\n",
    "### *No bias* case\n",
    "Since the network has a total of six layers, input and output layers included, we need five matrices to store the weights.  \n",
    "\n",
    "A first matrix $W^{(1)}$ is needed between the input and the first hidden layer. Since we have a \"5-noded\" input layer and an \"11-noded\" hidden layer, $W^{(1)}$ will be of the form\n",
    "$$\n",
    "W^{(1)}=\n",
    "\\begin{pmatrix}\n",
    "w^{(1)}_{1,1} & \\dotsm & w^{(1)}_{1,5}\\\\\n",
    "\\dotsm & \\ddots & \\dotsm\\\\\n",
    "w^{(1)}_{11,1} & \\dotsm & w^{(1)}_{11,5}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "that is a $11\\times5$ order matrix.\n",
    "\n",
    "A second matrix of weights is needed between the first and the second hidden layers: since the former belongs to $\\mathbb{R}^{11}$ and the latter to $\\mathbb{R}^{16}$, we will need a $16\\times 11$ matrix this time:\n",
    "$$\n",
    "W^{(2)}=\n",
    "\\begin{pmatrix}\n",
    "w^{(2)}_{1,1} & \\dotsm & w^{(2)}_{1,11}\\\\\n",
    "\\dotsm & \\ddots & \\dotsm\\\\\n",
    "w^{(2)}_{16,1} & \\dotsm & w^{(2)}_{16,11}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "that is a $11\\times5$ order matrix.\n",
    "\n",
    "Since hidden layers two and three belongs respectively to $\\mathbb{R}^{16}$ and $\\mathbb{R}^{13}$, now we need a $13\\times16$ order matrix:\n",
    "$$\n",
    "W^{(3)}=\n",
    "\\begin{pmatrix}\n",
    "w^{(3)}_{1,1} & \\dotsm & w^{(3)}_{1,16}\\\\\n",
    "\\dotsm & \\ddots & \\dotsm\\\\\n",
    "w^{(3)}_{13,1} & \\dotsm & w^{(3)}_{13,16}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Similar reasoning brings us to deduce that, in order to deal with the rest of the layers, we need \n",
    "$$\n",
    "W^{(4)}=\n",
    "\\begin{pmatrix}\n",
    "w^{(4)}_{1,1} & \\dotsm & w^{(4)}_{1,13}\\\\\n",
    "\\dotsm & \\ddots & \\dotsm\\\\\n",
    "w^{(4)}_{8,1} & \\dotsm & w^{(4)}_{8,13}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "W^{(5)}=\n",
    "\\begin{pmatrix}\n",
    "w^{(5)}_{1,1} & \\dotsm & w^{(5)}_{1,8}\\\\\n",
    "\\dotsm & \\ddots & \\dotsm\\\\\n",
    "w^{(5)}_{4,1} & \\dotsm & w^{(5)}_{4,8}\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "that are of order $8\\times13$ and $4\\times8$, respectively.\n",
    "\n",
    "This leads to the need of a number of parameters corresponding\n",
    "$$\n",
    "N_p=11\\cdot5+16\\cdot11+13\\cdot16+8\\cdot13+4\\cdot8=575.\n",
    "$$\n",
    "\n",
    "### *Bias* case\n",
    "In this case, to the number of parameters computed for the previous case, one must add the number of biases needed for each hidden layer layer.\n",
    "\n",
    "Since the first hidden layer belongs to $\\mathbb{R}^{11}$, here we need a bias vector\n",
    "$$\n",
    "b^{(1)}=\n",
    "\\begin{pmatrix}\n",
    "b^{(1)}_{1} \\\\\n",
    "\\vdots \\\\\n",
    "b^{(1)}_{11}\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "which is a $11\\times1$ vector. With a similar anrgument, the bias vectors for the rest of the hidden layers are\n",
    "$$\n",
    "b^{(2)}=\n",
    "\\begin{pmatrix}\n",
    "b^{(2)}_{1} \\\\\n",
    "\\vdots \\\\\n",
    "b^{(2)}_{16}\n",
    "\\end{pmatrix},\\,\\,\\,\\,\\,\n",
    "b^{(3)}=\n",
    "\\begin{pmatrix}\n",
    "b^{(3)}_{1} \\\\\n",
    "\\vdots \\\\\n",
    "b^{(3)}_{13}\n",
    "\\end{pmatrix},\\,\\,\\,\\,\\,\n",
    "b^{(4)}=\n",
    "\\begin{pmatrix}\n",
    "b^{(4)}_{1} \\\\\n",
    "\\vdots \\\\\n",
    "b^{(4)}_{8}\n",
    "\\end{pmatrix},\\,\\,\\,\\,\\,\n",
    "b^{(5)}=\n",
    "\\begin{pmatrix}\n",
    "b^{(5)}_{1} \\\\\n",
    "\\vdots \\\\\n",
    "b^{(5)}_{4}\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "that are $16\\times1$, $13\\times1$, $8\\times1$ and $4\\times 1$ vectors, respectively.\n",
    "\n",
    "The total number of parameters needed goes to\n",
    "$$\n",
    "N_p=575+11\\cdot1+16\\cdot1+13\\cdot1+8\\cdot1+4\\cdot1=575+52=627.\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Question 4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_h_layer_1.weight tensor([[-0.4297, -0.0136, -0.3942, -0.1517, -0.2996],\n        [ 0.0147, -0.2304,  0.1973, -0.3352,  0.0939],\n        [-0.1556, -0.2982,  0.4182,  0.3420, -0.3270],\n        [-0.2083,  0.3594,  0.2050,  0.1442,  0.1136],\n        [-0.1405, -0.0239,  0.0274,  0.0546,  0.4459],\n        [-0.0478,  0.1582, -0.0895,  0.2197, -0.4237],\n        [-0.1930, -0.3382,  0.4398, -0.4250,  0.2419],\n        [-0.0558, -0.0730,  0.0956,  0.2117, -0.4464],\n        [ 0.0547, -0.2603,  0.3320, -0.2044, -0.3372],\n        [ 0.1796, -0.2481,  0.0447,  0.3643, -0.2935],\n        [-0.1501, -0.0088,  0.3412, -0.3513, -0.3680]])\n\n\n1-norm of tensor _h_layer_1.weight\n12.421456336975098\n\n\n2-norm of tensor _h_layer_1.weight\n1.9512170553207397\n\n\n-------------------------------------------------------\n\n\n_h_layer_2.weight tensor([[-0.2671, -0.1591, -0.2032, -0.0022, -0.1790, -0.2512, -0.0206, -0.1491,\n          0.0902,  0.1483,  0.2389],\n        [-0.2307, -0.1003,  0.1809,  0.0156,  0.2890,  0.0381, -0.0540, -0.1388,\n         -0.1522, -0.1192, -0.0437],\n        [-0.1074, -0.0290, -0.1291, -0.2420, -0.2359,  0.2901,  0.2384,  0.0221,\n         -0.2241,  0.0031,  0.1526],\n        [ 0.1922, -0.0280, -0.2538, -0.2285,  0.1765, -0.1293, -0.0165, -0.2922,\n          0.2142,  0.2939, -0.0473],\n        [ 0.1578,  0.1013,  0.1630,  0.2153, -0.0526, -0.2215, -0.1253,  0.1801,\n         -0.1172,  0.1568,  0.0202],\n        [ 0.0124,  0.2448,  0.2186,  0.2633,  0.2539,  0.1506,  0.2756,  0.1853,\n          0.2518,  0.1894, -0.2519],\n        [-0.2760, -0.1107,  0.0193,  0.2263,  0.2727, -0.2179, -0.0980,  0.0784,\n          0.0628, -0.0661,  0.0760],\n        [-0.1796, -0.2425, -0.2433,  0.2460, -0.0373,  0.0694, -0.1282,  0.1884,\n          0.2498,  0.0337,  0.0422],\n        [ 0.0801, -0.1726, -0.0605,  0.1383, -0.1202,  0.2031,  0.2605, -0.2823,\n          0.2791, -0.0201, -0.0256],\n        [-0.2709, -0.2192,  0.0476,  0.1231,  0.2166, -0.0744, -0.1322,  0.0020,\n          0.0526,  0.1491,  0.2885],\n        [ 0.2326, -0.2712,  0.2679, -0.2280,  0.0207, -0.2502,  0.1556,  0.1731,\n          0.1294, -0.1029, -0.2379],\n        [-0.2530, -0.2981,  0.2514, -0.0812,  0.1358,  0.2571, -0.2250,  0.2957,\n         -0.2032,  0.2526,  0.0606],\n        [-0.0658, -0.1953,  0.2156, -0.2545,  0.0896, -0.1492, -0.2361, -0.2851,\n          0.2444, -0.2286, -0.2221],\n        [-0.2003,  0.0287,  0.1634,  0.1395, -0.2367,  0.1716,  0.0332,  0.0216,\n         -0.2193,  0.0579,  0.2387],\n        [-0.2135, -0.0489, -0.2433,  0.2900, -0.0836,  0.0647,  0.1155, -0.2521,\n          0.1741, -0.1507, -0.2853],\n        [-0.0944, -0.1904, -0.2573, -0.2849,  0.1587, -0.3006, -0.2422, -0.0193,\n          0.1745, -0.2723, -0.1770]])\n\n\n1-norm of tensor _h_layer_2.weight\n28.983272552490234\n\n\n2-norm of tensor _h_layer_2.weight\n2.474442958831787\n\n\n-------------------------------------------------------\n\n\n_h_layer_3.weight tensor([[-0.1703, -0.0026, -0.1039, -0.1077,  0.1273, -0.0846,  0.0874,  0.0429,\n          0.1802,  0.1211, -0.2187,  0.2302,  0.1602, -0.0977, -0.1810, -0.1393],\n        [ 0.1724, -0.0613, -0.1706, -0.1019,  0.1130, -0.2177,  0.1153, -0.0368,\n         -0.0399,  0.0101, -0.1560, -0.1791, -0.1921, -0.0373, -0.1883,  0.1982],\n        [-0.0620, -0.1305,  0.0039,  0.0148,  0.1360, -0.2290, -0.0848,  0.0836,\n          0.0322,  0.1634,  0.0164,  0.1446, -0.1630,  0.0962, -0.1520, -0.0390],\n        [-0.0601, -0.0166,  0.1801,  0.1948,  0.0306, -0.1440, -0.1112, -0.1546,\n          0.1493,  0.1080, -0.2070, -0.1118,  0.0333,  0.1713, -0.1634,  0.0243],\n        [-0.0682, -0.1474, -0.1776,  0.1495, -0.1866, -0.0988,  0.0790, -0.0350,\n          0.2064,  0.0849,  0.1531,  0.0194,  0.1576,  0.1071,  0.0979,  0.1208],\n        [-0.0353,  0.1893,  0.2022,  0.0754,  0.0466, -0.1531, -0.2155, -0.1528,\n          0.2105,  0.1943, -0.2290, -0.2313,  0.2332,  0.0270, -0.0300,  0.1083],\n        [ 0.0732, -0.0207, -0.0243, -0.2374,  0.2293,  0.0296, -0.2238,  0.0741,\n         -0.1024,  0.0822, -0.0181,  0.1430,  0.1523,  0.0104,  0.0773,  0.1001],\n        [-0.1343, -0.1304, -0.1676,  0.1036,  0.0818,  0.0261,  0.1234,  0.1819,\n          0.0620,  0.0451,  0.0502,  0.1746,  0.1189, -0.1334,  0.0603, -0.0456],\n        [-0.0874,  0.2139, -0.1749,  0.2184,  0.0341, -0.0722, -0.2248, -0.0505,\n         -0.2477,  0.2080, -0.0299,  0.1500, -0.1470,  0.1480, -0.2079,  0.1877],\n        [-0.1047,  0.2069,  0.0595, -0.1191, -0.2043,  0.1609, -0.1693,  0.0916,\n          0.0711,  0.1037,  0.0080,  0.0134, -0.1871,  0.1955,  0.1988,  0.2453],\n        [-0.1069,  0.2253, -0.1953, -0.1506,  0.2454, -0.2047, -0.1376,  0.0238,\n          0.1004,  0.0212, -0.0615,  0.0299,  0.0997,  0.2261, -0.1118,  0.1683],\n        [ 0.0946,  0.0643,  0.0404,  0.2027, -0.1092,  0.1024, -0.0556, -0.1877,\n         -0.1406, -0.1015, -0.0453, -0.2200, -0.1089,  0.1866,  0.2352,  0.1778],\n        [-0.1997,  0.0199, -0.0795,  0.0323, -0.2064, -0.0367,  0.0423,  0.1807,\n          0.0767,  0.1747, -0.1366,  0.0185, -0.1684, -0.2359,  0.2407, -0.1888]])\n\n\n1-norm of tensor _h_layer_3.weight\n25.67839241027832\n\n\n2-norm of tensor _h_layer_3.weight\n2.0365188121795654\n\n\n-------------------------------------------------------\n\n\n_h_layer_4.weight tensor([[-0.2029, -0.2674,  0.1163,  0.0366, -0.0173,  0.0890,  0.1105, -0.2685,\n         -0.1547,  0.1813, -0.1646, -0.1642,  0.2512],\n        [-0.1558,  0.0992,  0.0454,  0.2356,  0.2351, -0.2180,  0.0768, -0.0876,\n         -0.2299, -0.1718, -0.1330,  0.0508, -0.2438],\n        [-0.0765, -0.1731,  0.2210, -0.1524, -0.1859, -0.1919, -0.1184,  0.2520,\n          0.0441, -0.0771, -0.1740,  0.1008, -0.1113],\n        [-0.0958,  0.0151,  0.1697,  0.0954,  0.0533,  0.1309,  0.2264, -0.2174,\n          0.1549, -0.0878, -0.1213,  0.0563,  0.2257],\n        [-0.1797, -0.2743, -0.0731, -0.0924, -0.0281,  0.1595,  0.1435,  0.2562,\n          0.0398, -0.0627, -0.1148, -0.1472, -0.0247],\n        [-0.1636, -0.2230,  0.2293, -0.0953,  0.0235,  0.1947,  0.2665, -0.1028,\n         -0.2729, -0.0659,  0.0474,  0.1846, -0.1029],\n        [-0.1771, -0.2772, -0.1432, -0.2595, -0.2297,  0.2670, -0.2311,  0.0873,\n         -0.1237,  0.0158,  0.0543, -0.1653, -0.1660],\n        [ 0.1332,  0.1792, -0.1595, -0.1912, -0.0523, -0.2471, -0.2323, -0.0398,\n          0.1249, -0.0349, -0.0201, -0.2711,  0.2631]])\n\n\n1-norm of tensor _h_layer_4.weight\n15.25011157989502\n\n\n2-norm of tensor _h_layer_4.weight\n1.6888892650604248\n\n\n-------------------------------------------------------\n\n\n_o_layer.weight tensor([[ 0.1797,  0.1881, -0.1146, -0.2358, -0.2791, -0.1129,  0.3268, -0.0384],\n        [-0.0392, -0.1667,  0.3368,  0.2355, -0.3315, -0.2435,  0.2104,  0.0609],\n        [ 0.0079, -0.1887, -0.3337, -0.1642, -0.3419,  0.0927,  0.1301, -0.0936],\n        [-0.3483, -0.1347,  0.0340, -0.2635, -0.3135, -0.3308,  0.2883, -0.2204]])\n\n\n1-norm of tensor _o_layer.weight\n6.386416435241699\n\n\n2-norm of tensor _o_layer.weight\n1.2748936414718628\n\n\n-------------------------------------------------------\n\n\n"
     ]
    }
   ],
   "source": [
    "for par_name, par in mlp.state_dict().items():\n",
    "    print(par_name, par)\n",
    "    print('\\n')\n",
    "    norm_1 = par.norm(1).item()\n",
    "    pretty_print(norm_1, \"1-norm of tensor {}\".format(par_name))\n",
    "    norm_2 = par.norm(2).item()\n",
    "    pretty_print(norm_2, \"2-norm of tensor {}\".format(par_name))\n",
    "    print('-------------------------------------------------------')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}