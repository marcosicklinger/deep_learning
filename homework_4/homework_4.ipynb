{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Homework 4\n",
    "### Marco Sicklinger, April 2021"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Prerequisites"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts import mnist"
   ]
  },
  {
   "source": [
    "### Accuracy function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    '''\n",
    "    Accuracy function implemented during lab\n",
    "    '''\n",
    "    classes_prediction = y_hat.argmax(dim=1)\n",
    "    match_ground_truth = classes_prediction == y\n",
    "    correct_matches = match_ground_truth.sum()\n",
    "    return (correct_matches / y_hat.shape[0]).item()"
   ]
  },
  {
   "source": [
    "### Average meter class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "source": [
    "### Model class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=.2), # we add a dropout here. it's referred to the previous layer (with 32 neurons)\n",
    "\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.Linear(256, 192),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=192),\n",
    "            nn.Linear(192, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "source": [
    "### Criterion "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "source": [
    "### Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size_train = 256\n",
    "minibatch_size_test = 512\n",
    "\n",
    "trainloader, testloader, trainset, testset = mnist.get_data(batch_size_train=minibatch_size_test, batch_size_test=minibatch_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-02T13:59:00.154419</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 251.565 248.518125 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \nL 244.365 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p46de118fb7)\">\n    <image height=\"218\" id=\"imageeba158f566\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m9834c78772\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m9834c78772\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m9834c78772\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m9834c78772\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m9834c78772\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m9834c78772\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m9834c78772\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m35c113821a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m35c113821a\" y=\"11.082857\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m35c113821a\" y=\"49.911429\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m35c113821a\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m35c113821a\" y=\"127.568571\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m35c113821a\" y=\"166.397143\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m35c113821a\" y=\"205.225714\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 224.64 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 224.64 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p46de118fb7\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "_ = plt.imshow(trainset.data[15].numpy(), cmap=\"gray\")"
   ]
  },
  {
   "source": [
    "### Training function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, criterion, optimizer, trainloader, epochs = 20):\n",
    "    # creating container for saving loss and accuracy measurments\n",
    "    performance_training = [[None],[None],[None]]\n",
    "    steps = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('\\n')\n",
    "        print('Epoch {}\\n'.format(i+1))\n",
    "        steps += 1\n",
    "\n",
    "        ############\n",
    "        # TRAINING #\n",
    "        ############\n",
    "        loss_meter_train = AverageMeter()\n",
    "        accuracy_meter_train = AverageMeter()\n",
    "        model.train()\n",
    "        for X, y in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc = accuracy(y_hat, y)\n",
    "            loss_meter_train.update(val=loss.item(), n=X.shape[0])\n",
    "            accuracy_meter_train.update(val=acc, n=X.shape[0])\n",
    "        print('TRAINING loss {:.4f} (avg {:.4f}) - TRAINING accuracy {:.4f}\\n'.format(loss_meter_train.sum, loss_meter_train.avg, accuracy_meter_train.avg))\n",
    "        # saving values of loss and accuracy\n",
    "        performance_training[0].append(loss_meter_train.sum)\n",
    "        performance_training[1].append(loss_meter_train.avg)\n",
    "        performance_training[2].append(accuracy_meter_train.avg)\n",
    "\n",
    "        if accuracy_meter_train.avg >= 0.9999:\n",
    "            print('######################')\n",
    "            print('# TRAINING COMPLETED #')\n",
    "            print('######################')\n",
    "            break\n",
    "\n",
    "    return performance_training, steps"
   ]
  },
  {
   "source": [
    "## Assignment 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Training\n",
    "Two optimizers have been used."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Adam optimizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "learning_rate = .00005\n",
    "optimizer_1 = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "Epoch 1\n",
      "\n",
      "TRAINING loss 77000.8445 (avg 1.2833) - TRAINING accuracy 0.6480\n",
      "\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "\n",
      "TRAINING loss 32812.9643 (avg 0.5469) - TRAINING accuracy 0.8745\n",
      "\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "\n",
      "TRAINING loss 22357.2342 (avg 0.3726) - TRAINING accuracy 0.9088\n",
      "\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "\n",
      "TRAINING loss 17600.4146 (avg 0.2933) - TRAINING accuracy 0.9245\n",
      "\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "\n",
      "TRAINING loss 14654.3591 (avg 0.2442) - TRAINING accuracy 0.9363\n",
      "\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "\n",
      "TRAINING loss 12640.5459 (avg 0.2107) - TRAINING accuracy 0.9440\n",
      "\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "\n",
      "TRAINING loss 11056.2859 (avg 0.1843) - TRAINING accuracy 0.9510\n",
      "\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "\n",
      "TRAINING loss 9861.5050 (avg 0.1644) - TRAINING accuracy 0.9556\n",
      "\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "\n",
      "TRAINING loss 8887.1753 (avg 0.1481) - TRAINING accuracy 0.9600\n",
      "\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "\n",
      "TRAINING loss 8014.5519 (avg 0.1336) - TRAINING accuracy 0.9634\n",
      "\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "\n",
      "TRAINING loss 7395.0406 (avg 0.1233) - TRAINING accuracy 0.9670\n",
      "\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "\n",
      "TRAINING loss 6770.8705 (avg 0.1128) - TRAINING accuracy 0.9688\n",
      "\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "\n",
      "TRAINING loss 6212.6074 (avg 0.1035) - TRAINING accuracy 0.9710\n",
      "\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "\n",
      "TRAINING loss 5752.5850 (avg 0.0959) - TRAINING accuracy 0.9738\n",
      "\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "\n",
      "TRAINING loss 5356.8796 (avg 0.0893) - TRAINING accuracy 0.9754\n",
      "\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "\n",
      "TRAINING loss 4910.1546 (avg 0.0818) - TRAINING accuracy 0.9777\n",
      "\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "\n",
      "TRAINING loss 4633.9791 (avg 0.0772) - TRAINING accuracy 0.9785\n",
      "\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "\n",
      "TRAINING loss 4351.6863 (avg 0.0725) - TRAINING accuracy 0.9799\n",
      "\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "\n",
      "TRAINING loss 4002.1189 (avg 0.0667) - TRAINING accuracy 0.9813\n",
      "\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "\n",
      "TRAINING loss 3781.0648 (avg 0.0630) - TRAINING accuracy 0.9827\n",
      "\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "\n",
      "TRAINING loss 3601.7568 (avg 0.0600) - TRAINING accuracy 0.9834\n",
      "\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "\n",
      "TRAINING loss 3415.0274 (avg 0.0569) - TRAINING accuracy 0.9842\n",
      "\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "\n",
      "TRAINING loss 3166.4778 (avg 0.0528) - TRAINING accuracy 0.9855\n",
      "\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "\n",
      "TRAINING loss 2974.0465 (avg 0.0496) - TRAINING accuracy 0.9863\n",
      "\n",
      "\n",
      "\n",
      "Epoch 25\n",
      "\n",
      "TRAINING loss 2745.3238 (avg 0.0458) - TRAINING accuracy 0.9877\n",
      "\n",
      "\n",
      "\n",
      "Epoch 26\n",
      "\n",
      "TRAINING loss 2639.0141 (avg 0.0440) - TRAINING accuracy 0.9883\n",
      "\n",
      "\n",
      "\n",
      "Epoch 27\n",
      "\n",
      "TRAINING loss 2495.2000 (avg 0.0416) - TRAINING accuracy 0.9890\n",
      "\n",
      "\n",
      "\n",
      "Epoch 28\n",
      "\n",
      "TRAINING loss 2293.2022 (avg 0.0382) - TRAINING accuracy 0.9903\n",
      "\n",
      "\n",
      "\n",
      "Epoch 29\n",
      "\n",
      "TRAINING loss 2175.0241 (avg 0.0363) - TRAINING accuracy 0.9905\n",
      "\n",
      "\n",
      "\n",
      "Epoch 30\n",
      "\n",
      "TRAINING loss 2065.7987 (avg 0.0344) - TRAINING accuracy 0.9911\n",
      "\n",
      "\n",
      "\n",
      "Epoch 31\n",
      "\n",
      "TRAINING loss 1949.9598 (avg 0.0325) - TRAINING accuracy 0.9919\n",
      "\n",
      "\n",
      "\n",
      "Epoch 32\n",
      "\n",
      "TRAINING loss 1790.5106 (avg 0.0298) - TRAINING accuracy 0.9923\n",
      "\n",
      "\n",
      "\n",
      "Epoch 33\n",
      "\n",
      "TRAINING loss 1738.6906 (avg 0.0290) - TRAINING accuracy 0.9923\n",
      "\n",
      "\n",
      "\n",
      "Epoch 34\n",
      "\n",
      "TRAINING loss 1654.5911 (avg 0.0276) - TRAINING accuracy 0.9924\n",
      "\n",
      "\n",
      "\n",
      "Epoch 35\n",
      "\n",
      "TRAINING loss 1612.1741 (avg 0.0269) - TRAINING accuracy 0.9931\n",
      "\n",
      "\n",
      "\n",
      "Epoch 36\n",
      "\n",
      "TRAINING loss 1432.8010 (avg 0.0239) - TRAINING accuracy 0.9940\n",
      "\n",
      "\n",
      "\n",
      "Epoch 37\n",
      "\n",
      "TRAINING loss 1410.1861 (avg 0.0235) - TRAINING accuracy 0.9946\n",
      "\n",
      "\n",
      "\n",
      "Epoch 38\n",
      "\n",
      "TRAINING loss 1281.6420 (avg 0.0214) - TRAINING accuracy 0.9947\n",
      "\n",
      "\n",
      "\n",
      "Epoch 39\n",
      "\n",
      "TRAINING loss 1275.8469 (avg 0.0213) - TRAINING accuracy 0.9947\n",
      "\n",
      "\n",
      "\n",
      "Epoch 40\n",
      "\n",
      "TRAINING loss 1170.0342 (avg 0.0195) - TRAINING accuracy 0.9954\n",
      "\n",
      "\n",
      "\n",
      "Epoch 41\n",
      "\n",
      "TRAINING loss 1077.5529 (avg 0.0180) - TRAINING accuracy 0.9957\n",
      "\n",
      "\n",
      "\n",
      "Epoch 42\n",
      "\n",
      "TRAINING loss 1070.5555 (avg 0.0178) - TRAINING accuracy 0.9956\n",
      "\n",
      "\n",
      "\n",
      "Epoch 43\n",
      "\n",
      "TRAINING loss 1020.6944 (avg 0.0170) - TRAINING accuracy 0.9960\n",
      "\n",
      "\n",
      "\n",
      "Epoch 44\n",
      "\n",
      "TRAINING loss 957.5152 (avg 0.0160) - TRAINING accuracy 0.9965\n",
      "\n",
      "\n",
      "\n",
      "Epoch 45\n",
      "\n",
      "TRAINING loss 877.3154 (avg 0.0146) - TRAINING accuracy 0.9966\n",
      "\n",
      "\n",
      "\n",
      "Epoch 46\n",
      "\n",
      "TRAINING loss 893.1110 (avg 0.0149) - TRAINING accuracy 0.9964\n",
      "\n",
      "\n",
      "\n",
      "Epoch 47\n",
      "\n",
      "TRAINING loss 794.8176 (avg 0.0132) - TRAINING accuracy 0.9971\n",
      "\n",
      "\n",
      "\n",
      "Epoch 48\n",
      "\n",
      "TRAINING loss 729.9645 (avg 0.0122) - TRAINING accuracy 0.9974\n",
      "\n",
      "\n",
      "\n",
      "Epoch 49\n",
      "\n",
      "TRAINING loss 759.7936 (avg 0.0127) - TRAINING accuracy 0.9969\n",
      "\n",
      "\n",
      "\n",
      "Epoch 50\n",
      "\n",
      "TRAINING loss 707.5265 (avg 0.0118) - TRAINING accuracy 0.9974\n",
      "\n",
      "\n",
      "\n",
      "Epoch 51\n",
      "\n",
      "TRAINING loss 628.7079 (avg 0.0105) - TRAINING accuracy 0.9980\n",
      "\n",
      "\n",
      "\n",
      "Epoch 52\n",
      "\n",
      "TRAINING loss 618.4785 (avg 0.0103) - TRAINING accuracy 0.9977\n",
      "\n",
      "\n",
      "\n",
      "Epoch 53\n",
      "\n",
      "TRAINING loss 588.3090 (avg 0.0098) - TRAINING accuracy 0.9979\n",
      "\n",
      "\n",
      "\n",
      "Epoch 54\n",
      "\n",
      "TRAINING loss 548.3035 (avg 0.0091) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 55\n",
      "\n",
      "TRAINING loss 527.5000 (avg 0.0088) - TRAINING accuracy 0.9980\n",
      "\n",
      "\n",
      "\n",
      "Epoch 56\n",
      "\n",
      "TRAINING loss 515.8770 (avg 0.0086) - TRAINING accuracy 0.9981\n",
      "\n",
      "\n",
      "\n",
      "Epoch 57\n",
      "\n",
      "TRAINING loss 486.3052 (avg 0.0081) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 58\n",
      "\n",
      "TRAINING loss 471.3323 (avg 0.0079) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 59\n",
      "\n",
      "TRAINING loss 414.5110 (avg 0.0069) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 60\n",
      "\n",
      "TRAINING loss 392.5402 (avg 0.0065) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 61\n",
      "\n",
      "TRAINING loss 402.6465 (avg 0.0067) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 62\n",
      "\n",
      "TRAINING loss 421.7801 (avg 0.0070) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 63\n",
      "\n",
      "TRAINING loss 405.5806 (avg 0.0068) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 64\n",
      "\n",
      "TRAINING loss 374.8956 (avg 0.0062) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 65\n",
      "\n",
      "TRAINING loss 332.5708 (avg 0.0055) - TRAINING accuracy 0.9990\n",
      "\n",
      "\n",
      "\n",
      "Epoch 66\n",
      "\n",
      "TRAINING loss 328.2735 (avg 0.0055) - TRAINING accuracy 0.9990\n",
      "\n",
      "\n",
      "\n",
      "Epoch 67\n",
      "\n",
      "TRAINING loss 325.7663 (avg 0.0054) - TRAINING accuracy 0.9990\n",
      "\n",
      "\n",
      "\n",
      "Epoch 68\n",
      "\n",
      "TRAINING loss 314.1092 (avg 0.0052) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 69\n",
      "\n",
      "TRAINING loss 310.3685 (avg 0.0052) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 70\n",
      "\n",
      "TRAINING loss 264.1690 (avg 0.0044) - TRAINING accuracy 0.9993\n",
      "\n",
      "\n",
      "\n",
      "Epoch 71\n",
      "\n",
      "TRAINING loss 270.9187 (avg 0.0045) - TRAINING accuracy 0.9991\n",
      "\n",
      "\n",
      "\n",
      "Epoch 72\n",
      "\n",
      "TRAINING loss 307.3825 (avg 0.0051) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 73\n",
      "\n",
      "TRAINING loss 273.1398 (avg 0.0046) - TRAINING accuracy 0.9991\n",
      "\n",
      "\n",
      "\n",
      "Epoch 74\n",
      "\n",
      "TRAINING loss 251.5298 (avg 0.0042) - TRAINING accuracy 0.9992\n",
      "\n",
      "\n",
      "\n",
      "Epoch 75\n",
      "\n",
      "TRAINING loss 238.9652 (avg 0.0040) - TRAINING accuracy 0.9992\n",
      "\n",
      "\n",
      "\n",
      "Epoch 76\n",
      "\n",
      "TRAINING loss 212.3144 (avg 0.0035) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 77\n",
      "\n",
      "TRAINING loss 209.0134 (avg 0.0035) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 78\n",
      "\n",
      "TRAINING loss 223.2046 (avg 0.0037) - TRAINING accuracy 0.9992\n",
      "\n",
      "\n",
      "\n",
      "Epoch 79\n",
      "\n",
      "TRAINING loss 255.5198 (avg 0.0043) - TRAINING accuracy 0.9990\n",
      "\n",
      "\n",
      "\n",
      "Epoch 80\n",
      "\n",
      "TRAINING loss 206.8453 (avg 0.0034) - TRAINING accuracy 0.9993\n",
      "\n",
      "\n",
      "\n",
      "Epoch 81\n",
      "\n",
      "TRAINING loss 174.4820 (avg 0.0029) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 82\n",
      "\n",
      "TRAINING loss 193.9461 (avg 0.0032) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 83\n",
      "\n",
      "TRAINING loss 196.7932 (avg 0.0033) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 84\n",
      "\n",
      "TRAINING loss 175.3975 (avg 0.0029) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 85\n",
      "\n",
      "TRAINING loss 164.3029 (avg 0.0027) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 86\n",
      "\n",
      "TRAINING loss 168.1204 (avg 0.0028) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 87\n",
      "\n",
      "TRAINING loss 151.9074 (avg 0.0025) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 88\n",
      "\n",
      "TRAINING loss 138.5498 (avg 0.0023) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 89\n",
      "\n",
      "TRAINING loss 137.2807 (avg 0.0023) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 90\n",
      "\n",
      "TRAINING loss 132.1841 (avg 0.0022) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 91\n",
      "\n",
      "TRAINING loss 179.8180 (avg 0.0030) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 92\n",
      "\n",
      "TRAINING loss 170.7853 (avg 0.0028) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 93\n",
      "\n",
      "TRAINING loss 135.8141 (avg 0.0023) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 94\n",
      "\n",
      "TRAINING loss 141.4170 (avg 0.0024) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 95\n",
      "\n",
      "TRAINING loss 145.9253 (avg 0.0024) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 96\n",
      "\n",
      "TRAINING loss 126.8696 (avg 0.0021) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 97\n",
      "\n",
      "TRAINING loss 133.6806 (avg 0.0022) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 98\n",
      "\n",
      "TRAINING loss 122.9063 (avg 0.0020) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 99\n",
      "\n",
      "TRAINING loss 119.1958 (avg 0.0020) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 100\n",
      "\n",
      "TRAINING loss 125.4206 (avg 0.0021) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 101\n",
      "\n",
      "TRAINING loss 121.2342 (avg 0.0020) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 102\n",
      "\n",
      "TRAINING loss 109.3275 (avg 0.0018) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 103\n",
      "\n",
      "TRAINING loss 99.7327 (avg 0.0017) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 104\n",
      "\n",
      "TRAINING loss 128.9395 (avg 0.0021) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 105\n",
      "\n",
      "TRAINING loss 142.9764 (avg 0.0024) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 106\n",
      "\n",
      "TRAINING loss 109.5434 (avg 0.0018) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 107\n",
      "\n",
      "TRAINING loss 107.3492 (avg 0.0018) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 108\n",
      "\n",
      "TRAINING loss 95.2108 (avg 0.0016) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 109\n",
      "\n",
      "TRAINING loss 94.0232 (avg 0.0016) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 110\n",
      "\n",
      "TRAINING loss 124.0976 (avg 0.0021) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 111\n",
      "\n",
      "TRAINING loss 100.5458 (avg 0.0017) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 112\n",
      "\n",
      "TRAINING loss 119.5079 (avg 0.0020) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 113\n",
      "\n",
      "TRAINING loss 110.3733 (avg 0.0018) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 114\n",
      "\n",
      "TRAINING loss 85.5366 (avg 0.0014) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 115\n",
      "\n",
      "TRAINING loss 79.9008 (avg 0.0013) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 116\n",
      "\n",
      "TRAINING loss 75.4023 (avg 0.0013) - TRAINING accuracy 0.9999\n",
      "\n",
      "\n",
      "\n",
      "Epoch 117\n",
      "\n",
      "TRAINING loss 70.7516 (avg 0.0012) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 118\n",
      "\n",
      "TRAINING loss 79.1025 (avg 0.0013) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 119\n",
      "\n",
      "TRAINING loss 133.9858 (avg 0.0022) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 120\n",
      "\n",
      "TRAINING loss 91.4779 (avg 0.0015) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 121\n",
      "\n",
      "TRAINING loss 71.2080 (avg 0.0012) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 122\n",
      "\n",
      "TRAINING loss 67.6889 (avg 0.0011) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 123\n",
      "\n",
      "TRAINING loss 77.8112 (avg 0.0013) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 124\n",
      "\n",
      "TRAINING loss 60.8923 (avg 0.0010) - TRAINING accuracy 0.9999\n",
      "\n",
      "\n",
      "\n",
      "Epoch 125\n",
      "\n",
      "TRAINING loss 64.7081 (avg 0.0011) - TRAINING accuracy 0.9999\n",
      "\n",
      "\n",
      "\n",
      "Epoch 126\n",
      "\n",
      "TRAINING loss 61.7861 (avg 0.0010) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 127\n",
      "\n",
      "TRAINING loss 72.0534 (avg 0.0012) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 128\n",
      "\n",
      "TRAINING loss 79.4413 (avg 0.0013) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 129\n",
      "\n",
      "TRAINING loss 72.4719 (avg 0.0012) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 130\n",
      "\n",
      "TRAINING loss 83.8283 (avg 0.0014) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 131\n",
      "\n",
      "TRAINING loss 65.1373 (avg 0.0011) - TRAINING accuracy 0.9999\n",
      "\n",
      "\n",
      "\n",
      "Epoch 132\n",
      "\n",
      "TRAINING loss 60.1300 (avg 0.0010) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 133\n",
      "\n",
      "TRAINING loss 105.4659 (avg 0.0018) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 134\n",
      "\n",
      "TRAINING loss 92.7359 (avg 0.0015) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 135\n",
      "\n",
      "TRAINING loss 60.0191 (avg 0.0010) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 136\n",
      "\n",
      "TRAINING loss 69.4725 (avg 0.0012) - TRAINING accuracy 0.9999\n",
      "\n",
      "\n",
      "\n",
      "Epoch 137\n",
      "\n",
      "TRAINING loss 50.6585 (avg 0.0008) - TRAINING accuracy 0.9999\n",
      "\n",
      "\n",
      "\n",
      "Epoch 138\n",
      "\n",
      "TRAINING loss 56.7982 (avg 0.0009) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 139\n",
      "\n",
      "TRAINING loss 51.1975 (avg 0.0009) - TRAINING accuracy 0.9999\n",
      "\n",
      "######################\n",
      "# TRAINING COMPLETED #\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "performance_training, completed_epochs = training(model, criterion, optimizer_1, trainloader, epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-02T16:54:37.223356</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 385.78125 277.314375 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \nL 378.58125 22.318125 \nL 43.78125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"me90b7529e7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#me90b7529e7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(55.818182 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"102.792761\" xlink:href=\"#me90b7529e7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(96.430261 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.58609\" xlink:href=\"#me90b7529e7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(140.22359 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"190.379419\" xlink:href=\"#me90b7529e7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(184.016919 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"234.172748\" xlink:href=\"#me90b7529e7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(227.810248 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"277.966077\" xlink:href=\"#me90b7529e7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(268.422327 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"321.759406\" xlink:href=\"#me90b7529e7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 120 -->\n      <g transform=\"translate(312.215656 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"365.552735\" xlink:href=\"#me90b7529e7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 140 -->\n      <g transform=\"translate(356.008985 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- epoch -->\n     <g transform=\"translate(195.953125 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m476405e74b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m476405e74b\" y=\"229.874489\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 233.673707)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m476405e74b\" y=\"199.068687\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 202.867906)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m476405e74b\" y=\"168.262886\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 172.062104)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m476405e74b\" y=\"137.457084\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.6 -->\n      <g transform=\"translate(20.878125 141.256303)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m476405e74b\" y=\"106.651283\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.8 -->\n      <g transform=\"translate(20.878125 110.450502)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m476405e74b\" y=\"75.845481\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 79.6447)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m476405e74b\" y=\"45.03968\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 1.2 -->\n      <g transform=\"translate(20.878125 48.838899)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- accuracy -->\n     <g transform=\"translate(14.798438 153.5975)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p4a46aff5d7)\" d=\"M 58.999432 75.845481 \nL 363.363068 75.845481 \nL 363.363068 75.845481 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p4a46aff5d7)\" d=\"M 58.999432 229.874489 \nL 363.363068 229.874489 \nL 363.363068 229.874489 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p4a46aff5d7)\" d=\"M 61.189098 32.201761 \nL 63.378765 145.638683 \nL 65.568431 172.480112 \nL 67.758098 184.691582 \nL 69.947764 192.254549 \nL 72.137431 197.42431 \nL 74.327097 201.491343 \nL 76.516763 204.558525 \nL 78.70643 207.059775 \nL 80.896096 209.299931 \nL 85.275429 212.492648 \nL 87.465096 213.925793 \nL 89.654762 215.106739 \nL 91.844429 216.122575 \nL 94.034095 217.269385 \nL 98.413428 218.703057 \nL 100.603094 219.600449 \nL 104.982427 220.628238 \nL 107.172094 221.107601 \nL 109.36176 221.745665 \nL 113.741093 222.82683 \nL 118.120426 223.468936 \nL 120.310092 223.987494 \nL 131.258425 225.411009 \nL 135.637758 225.735796 \nL 137.827424 226.196273 \nL 140.01709 226.254329 \nL 142.206757 226.584321 \nL 144.396423 226.599198 \nL 148.775756 227.108249 \nL 153.155089 227.254213 \nL 157.534422 227.622288 \nL 159.724088 227.581739 \nL 164.103421 228.00056 \nL 166.293088 227.923984 \nL 175.051754 228.364211 \nL 179.431086 228.520317 \nL 186.000086 228.664508 \nL 190.379419 228.866779 \nL 196.948418 228.833302 \nL 205.707084 229.038198 \nL 229.793415 229.301489 \nL 231.983081 229.218531 \nL 236.362414 229.426567 \nL 240.741747 229.369291 \nL 249.500413 229.48452 \nL 256.069412 229.535152 \nL 258.259079 229.412869 \nL 284.535076 229.61846 \nL 288.914409 229.507447 \nL 293.293742 229.598907 \nL 297.673075 229.633117 \nL 299.862741 229.555911 \nL 304.242074 229.567694 \nL 317.380073 229.671421 \nL 319.569739 229.530527 \nL 323.949072 229.691687 \nL 341.466404 229.688442 \nL 345.845737 229.707271 \nL 348.035403 229.720126 \nL 350.225069 229.603742 \nL 363.363068 229.743057 \nL 363.363068 229.743057 \n\" style=\"fill:none;stroke:#008000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#p4a46aff5d7)\" d=\"M 61.189098 130.063692 \nL 63.378765 95.176122 \nL 65.568431 89.898061 \nL 67.758098 87.472104 \nL 69.947764 85.651995 \nL 74.327097 83.39547 \nL 78.70643 82.011776 \nL 83.085763 80.933573 \nL 87.465096 80.317457 \nL 89.654762 79.87334 \nL 91.844429 79.637162 \nL 94.034095 79.275194 \nL 98.413428 78.93633 \nL 104.982427 78.410064 \nL 111.551427 77.960813 \nL 113.741093 77.74774 \nL 118.120426 77.542368 \nL 120.310092 77.331861 \nL 124.689425 77.218907 \nL 129.068758 77.028938 \nL 133.448091 77.013535 \nL 142.206757 76.666969 \nL 146.58609 76.561716 \nL 157.534422 76.371747 \nL 159.724088 76.397419 \nL 164.103421 76.240822 \nL 166.293088 76.317837 \nL 172.862087 76.202315 \nL 192.569085 76.040585 \nL 196.948418 76.043152 \nL 227.603748 75.92763 \nL 234.172748 75.9456 \nL 238.552081 75.932764 \nL 363.363068 75.860884 \nL 363.363068 75.860884 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 239.758125 \nL 43.78125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 239.758125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 22.318125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_18\">\n    <!-- PERFORMANCE -->\n    <g transform=\"translate(165.509062 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      <path d=\"M 44.390625 34.1875 \nQ 47.5625 33.109375 50.5625 29.59375 \nQ 53.5625 26.078125 56.59375 19.921875 \nL 66.609375 0 \nL 56 0 \nL 46.6875 18.703125 \nQ 43.0625 26.03125 39.671875 28.421875 \nQ 36.28125 30.8125 30.421875 30.8125 \nL 19.671875 30.8125 \nL 19.671875 0 \nL 9.8125 0 \nL 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.578125 72.90625 50.734375 67.671875 \nQ 56.890625 62.453125 56.890625 51.90625 \nQ 56.890625 45.015625 53.6875 40.46875 \nQ 50.484375 35.9375 44.390625 34.1875 \nz\nM 19.671875 64.796875 \nL 19.671875 38.921875 \nL 32.078125 38.921875 \nQ 39.203125 38.921875 42.84375 42.21875 \nQ 46.484375 45.515625 46.484375 51.90625 \nQ 46.484375 58.296875 42.84375 61.546875 \nQ 39.203125 64.796875 32.078125 64.796875 \nz\n\" id=\"DejaVuSans-82\"/>\n      <path d=\"M 9.8125 72.90625 \nL 51.703125 72.90625 \nL 51.703125 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.109375 \nL 48.578125 43.109375 \nL 48.578125 34.8125 \nL 19.671875 34.8125 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-70\"/>\n      <path d=\"M 39.40625 66.21875 \nQ 28.65625 66.21875 22.328125 58.203125 \nQ 16.015625 50.203125 16.015625 36.375 \nQ 16.015625 22.609375 22.328125 14.59375 \nQ 28.65625 6.59375 39.40625 6.59375 \nQ 50.140625 6.59375 56.421875 14.59375 \nQ 62.703125 22.609375 62.703125 36.375 \nQ 62.703125 50.203125 56.421875 58.203125 \nQ 50.140625 66.21875 39.40625 66.21875 \nz\nM 39.40625 74.21875 \nQ 54.734375 74.21875 63.90625 63.9375 \nQ 73.09375 53.65625 73.09375 36.375 \nQ 73.09375 19.140625 63.90625 8.859375 \nQ 54.734375 -1.421875 39.40625 -1.421875 \nQ 24.03125 -1.421875 14.8125 8.828125 \nQ 5.609375 19.09375 5.609375 36.375 \nQ 5.609375 53.65625 14.8125 63.9375 \nQ 24.03125 74.21875 39.40625 74.21875 \nz\n\" id=\"DejaVuSans-79\"/>\n      <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n      <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n      <path d=\"M 9.8125 72.90625 \nL 23.09375 72.90625 \nL 55.421875 11.921875 \nL 55.421875 72.90625 \nL 64.984375 72.90625 \nL 64.984375 0 \nL 51.703125 0 \nL 19.390625 60.984375 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-78\"/>\n      <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-67\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-80\"/>\n     <use x=\"60.302734\" xlink:href=\"#DejaVuSans-69\"/>\n     <use x=\"123.486328\" xlink:href=\"#DejaVuSans-82\"/>\n     <use x=\"192.96875\" xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"250.488281\" xlink:href=\"#DejaVuSans-79\"/>\n     <use x=\"329.199219\" xlink:href=\"#DejaVuSans-82\"/>\n     <use x=\"398.681641\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"484.960938\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"553.369141\" xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"628.173828\" xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"697.998047\" xlink:href=\"#DejaVuSans-69\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 276.146875 59.674375 \nL 371.58125 59.674375 \nQ 373.58125 59.674375 373.58125 57.674375 \nL 373.58125 29.318125 \nQ 373.58125 27.318125 371.58125 27.318125 \nL 276.146875 27.318125 \nQ 274.146875 27.318125 274.146875 29.318125 \nL 274.146875 57.674375 \nQ 274.146875 59.674375 276.146875 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 278.146875 35.416562 \nL 298.146875 35.416562 \n\" style=\"fill:none;stroke:#008000;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_19\">\n     <!-- average loss -->\n     <g transform=\"translate(306.146875 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n       <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n       <path id=\"DejaVuSans-32\"/>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"181.982422\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"223.095703\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"284.375\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"347.851562\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"409.375\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"441.162109\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"468.945312\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"530.126953\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"582.226562\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 278.146875 50.094687 \nL 298.146875 50.094687 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_20\">\n     <!-- accuracy -->\n     <g transform=\"translate(306.146875 53.594687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4a46aff5d7\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "axis = [i for i in range(completed_epochs+1)]\n",
    "one = [1 for i in range(completed_epochs+1)]\n",
    "zero = [0 for i in range(completed_epochs+1)]\n",
    "_ = plt.plot(axis, one, linestyle = 'dashed', color = 'black')\n",
    "_ = plt.plot(axis, zero, linestyle = 'dashed', color = 'black')\n",
    "_ = plt.plot(axis, performance_training[1], label = 'average loss', color = 'green')\n",
    "__ = plt.xlabel('epoch') \n",
    "axis = [i for i in range(completed_epochs+1)]\n",
    "_ = plt.plot(axis, performance_training[2], label = 'accuracy', color = 'red')\n",
    "__ = plt.xlabel('epoch') \n",
    "___ = plt.ylabel('loss & accuracy')\n",
    "____ = plt.title('PERFORMANCE')\n",
    "_____ = plt.legend()"
   ]
  },
  {
   "source": [
    "#### SGD with momentum"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "learning_rate = .001\n",
    "momentum = .9\n",
    "optimizer_2 = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "Epoch 1\n",
      "\n",
      "TRAINING loss 58548.9101 (avg 0.9758) - TRAINING accuracy 0.7349\n",
      "\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "\n",
      "TRAINING loss 22458.8921 (avg 0.3743) - TRAINING accuracy 0.9063\n",
      "\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "\n",
      "TRAINING loss 16779.3823 (avg 0.2797) - TRAINING accuracy 0.9263\n",
      "\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "\n",
      "TRAINING loss 13954.7608 (avg 0.2326) - TRAINING accuracy 0.9367\n",
      "\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "\n",
      "TRAINING loss 12211.5944 (avg 0.2035) - TRAINING accuracy 0.9444\n",
      "\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "\n",
      "TRAINING loss 10911.8113 (avg 0.1819) - TRAINING accuracy 0.9498\n",
      "\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "\n",
      "TRAINING loss 9923.4348 (avg 0.1654) - TRAINING accuracy 0.9533\n",
      "\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "\n",
      "TRAINING loss 9025.0702 (avg 0.1504) - TRAINING accuracy 0.9589\n",
      "\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "\n",
      "TRAINING loss 8331.5396 (avg 0.1389) - TRAINING accuracy 0.9613\n",
      "\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "\n",
      "TRAINING loss 7838.5648 (avg 0.1306) - TRAINING accuracy 0.9637\n",
      "\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "\n",
      "TRAINING loss 7350.5931 (avg 0.1225) - TRAINING accuracy 0.9651\n",
      "\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "\n",
      "TRAINING loss 6914.2388 (avg 0.1152) - TRAINING accuracy 0.9679\n",
      "\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "\n",
      "TRAINING loss 6544.1898 (avg 0.1091) - TRAINING accuracy 0.9688\n",
      "\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "\n",
      "TRAINING loss 6215.7956 (avg 0.1036) - TRAINING accuracy 0.9710\n",
      "\n",
      "\n",
      "\n",
      "Epoch 15\n",
      "\n",
      "TRAINING loss 5960.2812 (avg 0.0993) - TRAINING accuracy 0.9722\n",
      "\n",
      "\n",
      "\n",
      "Epoch 16\n",
      "\n",
      "TRAINING loss 5559.5434 (avg 0.0927) - TRAINING accuracy 0.9738\n",
      "\n",
      "\n",
      "\n",
      "Epoch 17\n",
      "\n",
      "TRAINING loss 5376.4401 (avg 0.0896) - TRAINING accuracy 0.9753\n",
      "\n",
      "\n",
      "\n",
      "Epoch 18\n",
      "\n",
      "TRAINING loss 5125.5852 (avg 0.0854) - TRAINING accuracy 0.9758\n",
      "\n",
      "\n",
      "\n",
      "Epoch 19\n",
      "\n",
      "TRAINING loss 4940.5475 (avg 0.0823) - TRAINING accuracy 0.9764\n",
      "\n",
      "\n",
      "\n",
      "Epoch 20\n",
      "\n",
      "TRAINING loss 4755.4992 (avg 0.0793) - TRAINING accuracy 0.9782\n",
      "\n",
      "\n",
      "\n",
      "Epoch 21\n",
      "\n",
      "TRAINING loss 4525.1939 (avg 0.0754) - TRAINING accuracy 0.9787\n",
      "\n",
      "\n",
      "\n",
      "Epoch 22\n",
      "\n",
      "TRAINING loss 4401.6961 (avg 0.0734) - TRAINING accuracy 0.9797\n",
      "\n",
      "\n",
      "\n",
      "Epoch 23\n",
      "\n",
      "TRAINING loss 4185.0942 (avg 0.0698) - TRAINING accuracy 0.9805\n",
      "\n",
      "\n",
      "\n",
      "Epoch 24\n",
      "\n",
      "TRAINING loss 4042.0147 (avg 0.0674) - TRAINING accuracy 0.9812\n",
      "\n",
      "\n",
      "\n",
      "Epoch 25\n",
      "\n",
      "TRAINING loss 3834.1155 (avg 0.0639) - TRAINING accuracy 0.9817\n",
      "\n",
      "\n",
      "\n",
      "Epoch 26\n",
      "\n",
      "TRAINING loss 3647.8569 (avg 0.0608) - TRAINING accuracy 0.9831\n",
      "\n",
      "\n",
      "\n",
      "Epoch 27\n",
      "\n",
      "TRAINING loss 3623.3221 (avg 0.0604) - TRAINING accuracy 0.9832\n",
      "\n",
      "\n",
      "\n",
      "Epoch 28\n",
      "\n",
      "TRAINING loss 3475.5525 (avg 0.0579) - TRAINING accuracy 0.9840\n",
      "\n",
      "\n",
      "\n",
      "Epoch 29\n",
      "\n",
      "TRAINING loss 3319.4809 (avg 0.0553) - TRAINING accuracy 0.9846\n",
      "\n",
      "\n",
      "\n",
      "Epoch 30\n",
      "\n",
      "TRAINING loss 3267.4081 (avg 0.0545) - TRAINING accuracy 0.9846\n",
      "\n",
      "\n",
      "\n",
      "Epoch 31\n",
      "\n",
      "TRAINING loss 3058.8774 (avg 0.0510) - TRAINING accuracy 0.9860\n",
      "\n",
      "\n",
      "\n",
      "Epoch 32\n",
      "\n",
      "TRAINING loss 3036.1269 (avg 0.0506) - TRAINING accuracy 0.9859\n",
      "\n",
      "\n",
      "\n",
      "Epoch 33\n",
      "\n",
      "TRAINING loss 2941.5354 (avg 0.0490) - TRAINING accuracy 0.9867\n",
      "\n",
      "\n",
      "\n",
      "Epoch 34\n",
      "\n",
      "TRAINING loss 2869.9300 (avg 0.0478) - TRAINING accuracy 0.9866\n",
      "\n",
      "\n",
      "\n",
      "Epoch 35\n",
      "\n",
      "TRAINING loss 2656.1400 (avg 0.0443) - TRAINING accuracy 0.9882\n",
      "\n",
      "\n",
      "\n",
      "Epoch 36\n",
      "\n",
      "TRAINING loss 2677.4310 (avg 0.0446) - TRAINING accuracy 0.9882\n",
      "\n",
      "\n",
      "\n",
      "Epoch 37\n",
      "\n",
      "TRAINING loss 2556.5988 (avg 0.0426) - TRAINING accuracy 0.9879\n",
      "\n",
      "\n",
      "\n",
      "Epoch 38\n",
      "\n",
      "TRAINING loss 2474.5935 (avg 0.0412) - TRAINING accuracy 0.9889\n",
      "\n",
      "\n",
      "\n",
      "Epoch 39\n",
      "\n",
      "TRAINING loss 2373.8243 (avg 0.0396) - TRAINING accuracy 0.9895\n",
      "\n",
      "\n",
      "\n",
      "Epoch 40\n",
      "\n",
      "TRAINING loss 2385.0103 (avg 0.0398) - TRAINING accuracy 0.9897\n",
      "\n",
      "\n",
      "\n",
      "Epoch 41\n",
      "\n",
      "TRAINING loss 2223.1446 (avg 0.0371) - TRAINING accuracy 0.9904\n",
      "\n",
      "\n",
      "\n",
      "Epoch 42\n",
      "\n",
      "TRAINING loss 2189.3055 (avg 0.0365) - TRAINING accuracy 0.9905\n",
      "\n",
      "\n",
      "\n",
      "Epoch 43\n",
      "\n",
      "TRAINING loss 2159.0482 (avg 0.0360) - TRAINING accuracy 0.9903\n",
      "\n",
      "\n",
      "\n",
      "Epoch 44\n",
      "\n",
      "TRAINING loss 2060.4348 (avg 0.0343) - TRAINING accuracy 0.9906\n",
      "\n",
      "\n",
      "\n",
      "Epoch 45\n",
      "\n",
      "TRAINING loss 2009.5897 (avg 0.0335) - TRAINING accuracy 0.9916\n",
      "\n",
      "\n",
      "\n",
      "Epoch 46\n",
      "\n",
      "TRAINING loss 1993.9560 (avg 0.0332) - TRAINING accuracy 0.9911\n",
      "\n",
      "\n",
      "\n",
      "Epoch 47\n",
      "\n",
      "TRAINING loss 1883.5860 (avg 0.0314) - TRAINING accuracy 0.9924\n",
      "\n",
      "\n",
      "\n",
      "Epoch 48\n",
      "\n",
      "TRAINING loss 1821.7782 (avg 0.0304) - TRAINING accuracy 0.9923\n",
      "\n",
      "\n",
      "\n",
      "Epoch 49\n",
      "\n",
      "TRAINING loss 1820.0861 (avg 0.0303) - TRAINING accuracy 0.9920\n",
      "\n",
      "\n",
      "\n",
      "Epoch 50\n",
      "\n",
      "TRAINING loss 1717.1410 (avg 0.0286) - TRAINING accuracy 0.9928\n",
      "\n",
      "\n",
      "\n",
      "Epoch 51\n",
      "\n",
      "TRAINING loss 1667.3634 (avg 0.0278) - TRAINING accuracy 0.9933\n",
      "\n",
      "\n",
      "\n",
      "Epoch 52\n",
      "\n",
      "TRAINING loss 1640.2625 (avg 0.0273) - TRAINING accuracy 0.9935\n",
      "\n",
      "\n",
      "\n",
      "Epoch 53\n",
      "\n",
      "TRAINING loss 1604.6768 (avg 0.0267) - TRAINING accuracy 0.9935\n",
      "\n",
      "\n",
      "\n",
      "Epoch 54\n",
      "\n",
      "TRAINING loss 1550.5985 (avg 0.0258) - TRAINING accuracy 0.9937\n",
      "\n",
      "\n",
      "\n",
      "Epoch 55\n",
      "\n",
      "TRAINING loss 1500.6849 (avg 0.0250) - TRAINING accuracy 0.9940\n",
      "\n",
      "\n",
      "\n",
      "Epoch 56\n",
      "\n",
      "TRAINING loss 1474.4166 (avg 0.0246) - TRAINING accuracy 0.9940\n",
      "\n",
      "\n",
      "\n",
      "Epoch 57\n",
      "\n",
      "TRAINING loss 1428.1705 (avg 0.0238) - TRAINING accuracy 0.9941\n",
      "\n",
      "\n",
      "\n",
      "Epoch 58\n",
      "\n",
      "TRAINING loss 1396.3224 (avg 0.0233) - TRAINING accuracy 0.9944\n",
      "\n",
      "\n",
      "\n",
      "Epoch 59\n",
      "\n",
      "TRAINING loss 1389.9584 (avg 0.0232) - TRAINING accuracy 0.9944\n",
      "\n",
      "\n",
      "\n",
      "Epoch 60\n",
      "\n",
      "TRAINING loss 1305.2835 (avg 0.0218) - TRAINING accuracy 0.9953\n",
      "\n",
      "\n",
      "\n",
      "Epoch 61\n",
      "\n",
      "TRAINING loss 1284.6820 (avg 0.0214) - TRAINING accuracy 0.9952\n",
      "\n",
      "\n",
      "\n",
      "Epoch 62\n",
      "\n",
      "TRAINING loss 1274.1312 (avg 0.0212) - TRAINING accuracy 0.9950\n",
      "\n",
      "\n",
      "\n",
      "Epoch 63\n",
      "\n",
      "TRAINING loss 1240.9433 (avg 0.0207) - TRAINING accuracy 0.9951\n",
      "\n",
      "\n",
      "\n",
      "Epoch 64\n",
      "\n",
      "TRAINING loss 1192.3603 (avg 0.0199) - TRAINING accuracy 0.9956\n",
      "\n",
      "\n",
      "\n",
      "Epoch 65\n",
      "\n",
      "TRAINING loss 1196.4292 (avg 0.0199) - TRAINING accuracy 0.9953\n",
      "\n",
      "\n",
      "\n",
      "Epoch 66\n",
      "\n",
      "TRAINING loss 1160.3377 (avg 0.0193) - TRAINING accuracy 0.9957\n",
      "\n",
      "\n",
      "\n",
      "Epoch 67\n",
      "\n",
      "TRAINING loss 1137.1854 (avg 0.0190) - TRAINING accuracy 0.9956\n",
      "\n",
      "\n",
      "\n",
      "Epoch 68\n",
      "\n",
      "TRAINING loss 1070.0147 (avg 0.0178) - TRAINING accuracy 0.9964\n",
      "\n",
      "\n",
      "\n",
      "Epoch 69\n",
      "\n",
      "TRAINING loss 1056.5954 (avg 0.0176) - TRAINING accuracy 0.9963\n",
      "\n",
      "\n",
      "\n",
      "Epoch 70\n",
      "\n",
      "TRAINING loss 1068.8478 (avg 0.0178) - TRAINING accuracy 0.9960\n",
      "\n",
      "\n",
      "\n",
      "Epoch 71\n",
      "\n",
      "TRAINING loss 1014.2520 (avg 0.0169) - TRAINING accuracy 0.9963\n",
      "\n",
      "\n",
      "\n",
      "Epoch 72\n",
      "\n",
      "TRAINING loss 1000.9095 (avg 0.0167) - TRAINING accuracy 0.9966\n",
      "\n",
      "\n",
      "\n",
      "Epoch 73\n",
      "\n",
      "TRAINING loss 979.6148 (avg 0.0163) - TRAINING accuracy 0.9966\n",
      "\n",
      "\n",
      "\n",
      "Epoch 74\n",
      "\n",
      "TRAINING loss 970.7956 (avg 0.0162) - TRAINING accuracy 0.9966\n",
      "\n",
      "\n",
      "\n",
      "Epoch 75\n",
      "\n",
      "TRAINING loss 971.9526 (avg 0.0162) - TRAINING accuracy 0.9963\n",
      "\n",
      "\n",
      "\n",
      "Epoch 76\n",
      "\n",
      "TRAINING loss 937.2363 (avg 0.0156) - TRAINING accuracy 0.9970\n",
      "\n",
      "\n",
      "\n",
      "Epoch 77\n",
      "\n",
      "TRAINING loss 915.6137 (avg 0.0153) - TRAINING accuracy 0.9969\n",
      "\n",
      "\n",
      "\n",
      "Epoch 78\n",
      "\n",
      "TRAINING loss 873.8684 (avg 0.0146) - TRAINING accuracy 0.9972\n",
      "\n",
      "\n",
      "\n",
      "Epoch 79\n",
      "\n",
      "TRAINING loss 856.2934 (avg 0.0143) - TRAINING accuracy 0.9974\n",
      "\n",
      "\n",
      "\n",
      "Epoch 80\n",
      "\n",
      "TRAINING loss 849.0235 (avg 0.0142) - TRAINING accuracy 0.9972\n",
      "\n",
      "\n",
      "\n",
      "Epoch 81\n",
      "\n",
      "TRAINING loss 846.1487 (avg 0.0141) - TRAINING accuracy 0.9972\n",
      "\n",
      "\n",
      "\n",
      "Epoch 82\n",
      "\n",
      "TRAINING loss 768.4964 (avg 0.0128) - TRAINING accuracy 0.9979\n",
      "\n",
      "\n",
      "\n",
      "Epoch 83\n",
      "\n",
      "TRAINING loss 824.5862 (avg 0.0137) - TRAINING accuracy 0.9975\n",
      "\n",
      "\n",
      "\n",
      "Epoch 84\n",
      "\n",
      "TRAINING loss 776.7821 (avg 0.0129) - TRAINING accuracy 0.9976\n",
      "\n",
      "\n",
      "\n",
      "Epoch 85\n",
      "\n",
      "TRAINING loss 785.1591 (avg 0.0131) - TRAINING accuracy 0.9973\n",
      "\n",
      "\n",
      "\n",
      "Epoch 86\n",
      "\n",
      "TRAINING loss 772.9788 (avg 0.0129) - TRAINING accuracy 0.9975\n",
      "\n",
      "\n",
      "\n",
      "Epoch 87\n",
      "\n",
      "TRAINING loss 779.1618 (avg 0.0130) - TRAINING accuracy 0.9975\n",
      "\n",
      "\n",
      "\n",
      "Epoch 88\n",
      "\n",
      "TRAINING loss 762.2861 (avg 0.0127) - TRAINING accuracy 0.9974\n",
      "\n",
      "\n",
      "\n",
      "Epoch 89\n",
      "\n",
      "TRAINING loss 714.8877 (avg 0.0119) - TRAINING accuracy 0.9978\n",
      "\n",
      "\n",
      "\n",
      "Epoch 90\n",
      "\n",
      "TRAINING loss 705.5726 (avg 0.0118) - TRAINING accuracy 0.9978\n",
      "\n",
      "\n",
      "\n",
      "Epoch 91\n",
      "\n",
      "TRAINING loss 690.7737 (avg 0.0115) - TRAINING accuracy 0.9979\n",
      "\n",
      "\n",
      "\n",
      "Epoch 92\n",
      "\n",
      "TRAINING loss 700.3315 (avg 0.0117) - TRAINING accuracy 0.9977\n",
      "\n",
      "\n",
      "\n",
      "Epoch 93\n",
      "\n",
      "TRAINING loss 647.9104 (avg 0.0108) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 94\n",
      "\n",
      "TRAINING loss 659.4226 (avg 0.0110) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 95\n",
      "\n",
      "TRAINING loss 659.4162 (avg 0.0110) - TRAINING accuracy 0.9979\n",
      "\n",
      "\n",
      "\n",
      "Epoch 96\n",
      "\n",
      "TRAINING loss 642.3413 (avg 0.0107) - TRAINING accuracy 0.9981\n",
      "\n",
      "\n",
      "\n",
      "Epoch 97\n",
      "\n",
      "TRAINING loss 647.7149 (avg 0.0108) - TRAINING accuracy 0.9978\n",
      "\n",
      "\n",
      "\n",
      "Epoch 98\n",
      "\n",
      "TRAINING loss 642.6330 (avg 0.0107) - TRAINING accuracy 0.9980\n",
      "\n",
      "\n",
      "\n",
      "Epoch 99\n",
      "\n",
      "TRAINING loss 631.0550 (avg 0.0105) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 100\n",
      "\n",
      "TRAINING loss 591.2102 (avg 0.0099) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 101\n",
      "\n",
      "TRAINING loss 568.2542 (avg 0.0095) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 102\n",
      "\n",
      "TRAINING loss 579.4236 (avg 0.0097) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 103\n",
      "\n",
      "TRAINING loss 537.5756 (avg 0.0090) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 104\n",
      "\n",
      "TRAINING loss 539.5558 (avg 0.0090) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 105\n",
      "\n",
      "TRAINING loss 591.0953 (avg 0.0099) - TRAINING accuracy 0.9981\n",
      "\n",
      "\n",
      "\n",
      "Epoch 106\n",
      "\n",
      "TRAINING loss 531.6661 (avg 0.0089) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 107\n",
      "\n",
      "TRAINING loss 557.2303 (avg 0.0093) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 108\n",
      "\n",
      "TRAINING loss 515.7302 (avg 0.0086) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 109\n",
      "\n",
      "TRAINING loss 575.0542 (avg 0.0096) - TRAINING accuracy 0.9980\n",
      "\n",
      "\n",
      "\n",
      "Epoch 110\n",
      "\n",
      "TRAINING loss 524.2857 (avg 0.0087) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 111\n",
      "\n",
      "TRAINING loss 515.8188 (avg 0.0086) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 112\n",
      "\n",
      "TRAINING loss 514.5859 (avg 0.0086) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 113\n",
      "\n",
      "TRAINING loss 490.9678 (avg 0.0082) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 114\n",
      "\n",
      "TRAINING loss 458.1586 (avg 0.0076) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 115\n",
      "\n",
      "TRAINING loss 470.1246 (avg 0.0078) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 116\n",
      "\n",
      "TRAINING loss 462.9981 (avg 0.0077) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 117\n",
      "\n",
      "TRAINING loss 493.0354 (avg 0.0082) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 118\n",
      "\n",
      "TRAINING loss 462.6975 (avg 0.0077) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 119\n",
      "\n",
      "TRAINING loss 432.8411 (avg 0.0072) - TRAINING accuracy 0.9990\n",
      "\n",
      "\n",
      "\n",
      "Epoch 120\n",
      "\n",
      "TRAINING loss 439.4889 (avg 0.0073) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 121\n",
      "\n",
      "TRAINING loss 451.5767 (avg 0.0075) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 122\n",
      "\n",
      "TRAINING loss 488.1847 (avg 0.0081) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 123\n",
      "\n",
      "TRAINING loss 450.9913 (avg 0.0075) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 124\n",
      "\n",
      "TRAINING loss 415.4790 (avg 0.0069) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 125\n",
      "\n",
      "TRAINING loss 423.4260 (avg 0.0071) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 126\n",
      "\n",
      "TRAINING loss 425.4822 (avg 0.0071) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 127\n",
      "\n",
      "TRAINING loss 399.9785 (avg 0.0067) - TRAINING accuracy 0.9990\n",
      "\n",
      "\n",
      "\n",
      "Epoch 128\n",
      "\n",
      "TRAINING loss 428.4378 (avg 0.0071) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 129\n",
      "\n",
      "TRAINING loss 422.3893 (avg 0.0070) - TRAINING accuracy 0.9990\n",
      "\n",
      "\n",
      "\n",
      "Epoch 130\n",
      "\n",
      "TRAINING loss 382.8482 (avg 0.0064) - TRAINING accuracy 0.9992\n",
      "\n",
      "\n",
      "\n",
      "Epoch 131\n",
      "\n",
      "TRAINING loss 362.4453 (avg 0.0060) - TRAINING accuracy 0.9992\n",
      "\n",
      "\n",
      "\n",
      "Epoch 132\n",
      "\n",
      "TRAINING loss 392.9941 (avg 0.0065) - TRAINING accuracy 0.9990\n",
      "\n",
      "\n",
      "\n",
      "Epoch 133\n",
      "\n",
      "TRAINING loss 371.0956 (avg 0.0062) - TRAINING accuracy 0.9992\n",
      "\n",
      "\n",
      "\n",
      "Epoch 134\n",
      "\n",
      "TRAINING loss 370.3330 (avg 0.0062) - TRAINING accuracy 0.9992\n",
      "\n",
      "\n",
      "\n",
      "Epoch 135\n",
      "\n",
      "TRAINING loss 361.1091 (avg 0.0060) - TRAINING accuracy 0.9992\n",
      "\n",
      "\n",
      "\n",
      "Epoch 136\n",
      "\n",
      "TRAINING loss 350.8164 (avg 0.0058) - TRAINING accuracy 0.9992\n",
      "\n",
      "\n",
      "\n",
      "Epoch 137\n",
      "\n",
      "TRAINING loss 351.9356 (avg 0.0059) - TRAINING accuracy 0.9993\n",
      "\n",
      "\n",
      "\n",
      "Epoch 138\n",
      "\n",
      "TRAINING loss 390.0408 (avg 0.0065) - TRAINING accuracy 0.9990\n",
      "\n",
      "\n",
      "\n",
      "Epoch 139\n",
      "\n",
      "TRAINING loss 366.7501 (avg 0.0061) - TRAINING accuracy 0.9991\n",
      "\n",
      "\n",
      "\n",
      "Epoch 140\n",
      "\n",
      "TRAINING loss 351.1204 (avg 0.0059) - TRAINING accuracy 0.9990\n",
      "\n",
      "\n",
      "\n",
      "Epoch 141\n",
      "\n",
      "TRAINING loss 329.8400 (avg 0.0055) - TRAINING accuracy 0.9993\n",
      "\n",
      "\n",
      "\n",
      "Epoch 142\n",
      "\n",
      "TRAINING loss 361.9916 (avg 0.0060) - TRAINING accuracy 0.9992\n",
      "\n",
      "\n",
      "\n",
      "Epoch 143\n",
      "\n",
      "TRAINING loss 323.4886 (avg 0.0054) - TRAINING accuracy 0.9992\n",
      "\n",
      "\n",
      "\n",
      "Epoch 144\n",
      "\n",
      "TRAINING loss 329.7215 (avg 0.0055) - TRAINING accuracy 0.9992\n",
      "\n",
      "\n",
      "\n",
      "Epoch 145\n",
      "\n",
      "TRAINING loss 338.5705 (avg 0.0056) - TRAINING accuracy 0.9991\n",
      "\n",
      "\n",
      "\n",
      "Epoch 146\n",
      "\n",
      "TRAINING loss 285.7838 (avg 0.0048) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 147\n",
      "\n",
      "TRAINING loss 315.3724 (avg 0.0053) - TRAINING accuracy 0.9993\n",
      "\n",
      "\n",
      "\n",
      "Epoch 148\n",
      "\n",
      "TRAINING loss 316.4905 (avg 0.0053) - TRAINING accuracy 0.9993\n",
      "\n",
      "\n",
      "\n",
      "Epoch 149\n",
      "\n",
      "TRAINING loss 311.8335 (avg 0.0052) - TRAINING accuracy 0.9993\n",
      "\n",
      "\n",
      "\n",
      "Epoch 150\n",
      "\n",
      "TRAINING loss 323.0953 (avg 0.0054) - TRAINING accuracy 0.9992\n",
      "\n",
      "\n",
      "\n",
      "Epoch 151\n",
      "\n",
      "TRAINING loss 305.7323 (avg 0.0051) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 152\n",
      "\n",
      "TRAINING loss 300.1059 (avg 0.0050) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 153\n",
      "\n",
      "TRAINING loss 301.8205 (avg 0.0050) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 154\n",
      "\n",
      "TRAINING loss 285.8917 (avg 0.0048) - TRAINING accuracy 0.9993\n",
      "\n",
      "\n",
      "\n",
      "Epoch 155\n",
      "\n",
      "TRAINING loss 281.0252 (avg 0.0047) - TRAINING accuracy 0.9993\n",
      "\n",
      "\n",
      "\n",
      "Epoch 156\n",
      "\n",
      "TRAINING loss 284.8127 (avg 0.0047) - TRAINING accuracy 0.9993\n",
      "\n",
      "\n",
      "\n",
      "Epoch 157\n",
      "\n",
      "TRAINING loss 286.6264 (avg 0.0048) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 158\n",
      "\n",
      "TRAINING loss 299.0818 (avg 0.0050) - TRAINING accuracy 0.9993\n",
      "\n",
      "\n",
      "\n",
      "Epoch 159\n",
      "\n",
      "TRAINING loss 292.0632 (avg 0.0049) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 160\n",
      "\n",
      "TRAINING loss 285.4024 (avg 0.0048) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 161\n",
      "\n",
      "TRAINING loss 289.4632 (avg 0.0048) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 162\n",
      "\n",
      "TRAINING loss 288.5258 (avg 0.0048) - TRAINING accuracy 0.9992\n",
      "\n",
      "\n",
      "\n",
      "Epoch 163\n",
      "\n",
      "TRAINING loss 295.5571 (avg 0.0049) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 164\n",
      "\n",
      "TRAINING loss 280.5135 (avg 0.0047) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 165\n",
      "\n",
      "TRAINING loss 285.0699 (avg 0.0048) - TRAINING accuracy 0.9993\n",
      "\n",
      "\n",
      "\n",
      "Epoch 166\n",
      "\n",
      "TRAINING loss 279.4362 (avg 0.0047) - TRAINING accuracy 0.9992\n",
      "\n",
      "\n",
      "\n",
      "Epoch 167\n",
      "\n",
      "TRAINING loss 263.3608 (avg 0.0044) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 168\n",
      "\n",
      "TRAINING loss 255.9248 (avg 0.0043) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 169\n",
      "\n",
      "TRAINING loss 248.0549 (avg 0.0041) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 170\n",
      "\n",
      "TRAINING loss 251.0203 (avg 0.0042) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 171\n",
      "\n",
      "TRAINING loss 257.8952 (avg 0.0043) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 172\n",
      "\n",
      "TRAINING loss 233.9588 (avg 0.0039) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 173\n",
      "\n",
      "TRAINING loss 246.5094 (avg 0.0041) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 174\n",
      "\n",
      "TRAINING loss 238.5637 (avg 0.0040) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 175\n",
      "\n",
      "TRAINING loss 236.6119 (avg 0.0039) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 176\n",
      "\n",
      "TRAINING loss 255.9024 (avg 0.0043) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 177\n",
      "\n",
      "TRAINING loss 248.2185 (avg 0.0041) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 178\n",
      "\n",
      "TRAINING loss 237.6985 (avg 0.0040) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 179\n",
      "\n",
      "TRAINING loss 243.6738 (avg 0.0041) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 180\n",
      "\n",
      "TRAINING loss 249.4913 (avg 0.0042) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 181\n",
      "\n",
      "TRAINING loss 238.8877 (avg 0.0040) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 182\n",
      "\n",
      "TRAINING loss 237.3668 (avg 0.0040) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 183\n",
      "\n",
      "TRAINING loss 246.2537 (avg 0.0041) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 184\n",
      "\n",
      "TRAINING loss 233.5506 (avg 0.0039) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 185\n",
      "\n",
      "TRAINING loss 223.3382 (avg 0.0037) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 186\n",
      "\n",
      "TRAINING loss 254.9862 (avg 0.0042) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 187\n",
      "\n",
      "TRAINING loss 234.7601 (avg 0.0039) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 188\n",
      "\n",
      "TRAINING loss 234.1274 (avg 0.0039) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 189\n",
      "\n",
      "TRAINING loss 212.4252 (avg 0.0035) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 190\n",
      "\n",
      "TRAINING loss 221.2232 (avg 0.0037) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 191\n",
      "\n",
      "TRAINING loss 215.3516 (avg 0.0036) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 192\n",
      "\n",
      "TRAINING loss 218.0938 (avg 0.0036) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 193\n",
      "\n",
      "TRAINING loss 224.4276 (avg 0.0037) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 194\n",
      "\n",
      "TRAINING loss 236.1651 (avg 0.0039) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 195\n",
      "\n",
      "TRAINING loss 207.2051 (avg 0.0035) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 196\n",
      "\n",
      "TRAINING loss 209.5522 (avg 0.0035) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 197\n",
      "\n",
      "TRAINING loss 222.5374 (avg 0.0037) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 198\n",
      "\n",
      "TRAINING loss 188.3800 (avg 0.0031) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 199\n",
      "\n",
      "TRAINING loss 198.6838 (avg 0.0033) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 200\n",
      "\n",
      "TRAINING loss 195.9241 (avg 0.0033) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 201\n",
      "\n",
      "TRAINING loss 199.4961 (avg 0.0033) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 202\n",
      "\n",
      "TRAINING loss 231.5259 (avg 0.0039) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 203\n",
      "\n",
      "TRAINING loss 207.2682 (avg 0.0035) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 204\n",
      "\n",
      "TRAINING loss 214.8063 (avg 0.0036) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 205\n",
      "\n",
      "TRAINING loss 201.6757 (avg 0.0034) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 206\n",
      "\n",
      "TRAINING loss 219.4159 (avg 0.0037) - TRAINING accuracy 0.9994\n",
      "\n",
      "\n",
      "\n",
      "Epoch 207\n",
      "\n",
      "TRAINING loss 192.7760 (avg 0.0032) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 208\n",
      "\n",
      "TRAINING loss 189.3937 (avg 0.0032) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 209\n",
      "\n",
      "TRAINING loss 189.8660 (avg 0.0032) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 210\n",
      "\n",
      "TRAINING loss 188.8646 (avg 0.0031) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 211\n",
      "\n",
      "TRAINING loss 191.3389 (avg 0.0032) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 212\n",
      "\n",
      "TRAINING loss 198.5388 (avg 0.0033) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 213\n",
      "\n",
      "TRAINING loss 195.8648 (avg 0.0033) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 214\n",
      "\n",
      "TRAINING loss 188.3357 (avg 0.0031) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 215\n",
      "\n",
      "TRAINING loss 193.2752 (avg 0.0032) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 216\n",
      "\n",
      "TRAINING loss 170.3075 (avg 0.0028) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 217\n",
      "\n",
      "TRAINING loss 177.8775 (avg 0.0030) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 218\n",
      "\n",
      "TRAINING loss 176.2837 (avg 0.0029) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 219\n",
      "\n",
      "TRAINING loss 188.0721 (avg 0.0031) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 220\n",
      "\n",
      "TRAINING loss 163.2278 (avg 0.0027) - TRAINING accuracy 0.9999\n",
      "\n",
      "\n",
      "\n",
      "Epoch 221\n",
      "\n",
      "TRAINING loss 164.5136 (avg 0.0027) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 222\n",
      "\n",
      "TRAINING loss 171.9441 (avg 0.0029) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 223\n",
      "\n",
      "TRAINING loss 173.3610 (avg 0.0029) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 224\n",
      "\n",
      "TRAINING loss 184.6816 (avg 0.0031) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 225\n",
      "\n",
      "TRAINING loss 176.2189 (avg 0.0029) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 226\n",
      "\n",
      "TRAINING loss 167.7264 (avg 0.0028) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 227\n",
      "\n",
      "TRAINING loss 165.7035 (avg 0.0028) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 228\n",
      "\n",
      "TRAINING loss 164.9271 (avg 0.0027) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 229\n",
      "\n",
      "TRAINING loss 158.4757 (avg 0.0026) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 230\n",
      "\n",
      "TRAINING loss 153.9232 (avg 0.0026) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 231\n",
      "\n",
      "TRAINING loss 166.4072 (avg 0.0028) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 232\n",
      "\n",
      "TRAINING loss 168.3403 (avg 0.0028) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 233\n",
      "\n",
      "TRAINING loss 159.8930 (avg 0.0027) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 234\n",
      "\n",
      "TRAINING loss 153.7826 (avg 0.0026) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 235\n",
      "\n",
      "TRAINING loss 180.5387 (avg 0.0030) - TRAINING accuracy 0.9996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 236\n",
      "\n",
      "TRAINING loss 160.5534 (avg 0.0027) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 237\n",
      "\n",
      "TRAINING loss 154.2416 (avg 0.0026) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 238\n",
      "\n",
      "TRAINING loss 149.5365 (avg 0.0025) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 239\n",
      "\n",
      "TRAINING loss 153.3512 (avg 0.0026) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 240\n",
      "\n",
      "TRAINING loss 164.8303 (avg 0.0027) - TRAINING accuracy 0.9995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 241\n",
      "\n",
      "TRAINING loss 135.6117 (avg 0.0023) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 242\n",
      "\n",
      "TRAINING loss 159.0360 (avg 0.0027) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 243\n",
      "\n",
      "TRAINING loss 144.7542 (avg 0.0024) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 244\n",
      "\n",
      "TRAINING loss 140.4244 (avg 0.0023) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 245\n",
      "\n",
      "TRAINING loss 143.5055 (avg 0.0024) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 246\n",
      "\n",
      "TRAINING loss 148.4735 (avg 0.0025) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 247\n",
      "\n",
      "TRAINING loss 154.7502 (avg 0.0026) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 248\n",
      "\n",
      "TRAINING loss 137.5769 (avg 0.0023) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 249\n",
      "\n",
      "TRAINING loss 138.2208 (avg 0.0023) - TRAINING accuracy 0.9998\n",
      "\n",
      "\n",
      "\n",
      "Epoch 250\n",
      "\n",
      "TRAINING loss 162.9450 (avg 0.0027) - TRAINING accuracy 0.9997\n",
      "\n",
      "\n",
      "\n",
      "Epoch 251\n",
      "\n",
      "TRAINING loss 133.9298 (avg 0.0022) - TRAINING accuracy 0.9999\n",
      "\n",
      "######################\n",
      "# TRAINING COMPLETED #\n",
      "######################\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "performance_training, completed_epochs = training(model, criterion, optimizer_2, trainloader, epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-02T18:11:33.432914</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 385.78125 277.314375 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \nL 378.58125 22.318125 \nL 43.78125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m79bf8bbc14\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#m79bf8bbc14\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(55.818182 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"119.629638\" xlink:href=\"#m79bf8bbc14\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <g transform=\"translate(113.267138 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.259845\" xlink:href=\"#m79bf8bbc14\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <g transform=\"translate(170.716095 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"240.890051\" xlink:href=\"#m79bf8bbc14\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 150 -->\n      <g transform=\"translate(231.346301 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"301.520258\" xlink:href=\"#m79bf8bbc14\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 200 -->\n      <g transform=\"translate(291.976508 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"362.150464\" xlink:href=\"#m79bf8bbc14\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 250 -->\n      <g transform=\"translate(352.606714 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(195.953125 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m4103afc8de\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4103afc8de\" y=\"229.874489\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 233.673707)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4103afc8de\" y=\"190.339943\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 194.139162)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4103afc8de\" y=\"150.805398\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 154.604616)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4103afc8de\" y=\"111.270852\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.6 -->\n      <g transform=\"translate(20.878125 115.070071)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4103afc8de\" y=\"71.736307\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.8 -->\n      <g transform=\"translate(20.878125 75.535526)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m4103afc8de\" y=\"32.201761\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 36.00098)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- accuracy -->\n     <g transform=\"translate(14.798438 153.5975)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p9953269898)\" d=\"M 58.999432 32.201761 \nL 363.363068 32.201761 \nL 363.363068 32.201761 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p9953269898)\" d=\"M 58.999432 229.874489 \nL 363.363068 229.874489 \nL 363.363068 229.874489 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p9953269898)\" d=\"M 60.212036 36.982443 \nL 61.42464 155.882648 \nL 62.637244 174.594051 \nL 63.849848 183.899895 \nL 65.062452 189.642836 \nL 66.275057 193.92503 \nL 68.700265 200.140985 \nL 69.912869 202.425853 \nL 72.338077 205.657625 \nL 73.550681 207.095215 \nL 75.97589 209.396268 \nL 77.188494 210.238071 \nL 78.401098 211.55832 \nL 79.613702 212.161562 \nL 80.826306 212.988015 \nL 83.251514 214.20728 \nL 84.464119 214.966032 \nL 85.676723 215.372901 \nL 86.889327 216.086505 \nL 88.101931 216.557888 \nL 90.527139 217.856458 \nL 91.739743 217.937289 \nL 94.164952 218.938308 \nL 95.377556 219.109864 \nL 96.59016 219.796878 \nL 97.802764 219.87183 \nL 100.227972 220.419374 \nL 101.440576 221.123715 \nL 102.65318 221.05357 \nL 103.865785 221.451658 \nL 106.290993 222.053817 \nL 107.503597 222.016964 \nL 108.716201 222.550238 \nL 111.141409 222.761406 \nL 112.354013 223.086293 \nL 113.566618 223.253804 \nL 114.779222 223.30531 \nL 115.991826 223.668929 \nL 117.20443 223.872557 \nL 118.417034 223.878132 \nL 119.629638 224.217289 \nL 122.054847 224.470569 \nL 126.905263 225.016956 \nL 129.330471 225.274241 \nL 130.543075 225.295208 \nL 131.75568 225.574173 \nL 140.243908 226.12798 \nL 141.456513 226.349277 \nL 152.36995 226.857958 \nL 154.795158 227.053391 \nL 157.220366 227.086813 \nL 158.43297 227.342642 \nL 159.645575 227.157852 \nL 160.858179 227.315345 \nL 164.495991 227.307505 \nL 175.409428 227.758266 \nL 179.047241 227.795449 \nL 181.472449 228.00235 \nL 182.685053 227.965551 \nL 183.897657 228.103421 \nL 185.110261 228.096897 \nL 186.322865 227.927098 \nL 187.535469 228.122891 \nL 188.748074 228.038668 \nL 189.960678 228.175392 \nL 191.173282 227.979947 \nL 192.385886 228.147205 \nL 205.724531 228.386749 \nL 206.937136 228.266142 \nL 209.362344 228.505674 \nL 211.787552 228.472718 \nL 213.000156 228.556741 \nL 214.21276 228.462981 \nL 216.637969 228.613178 \nL 217.850573 228.680396 \nL 219.063177 228.579752 \nL 221.488385 228.65441 \nL 225.126197 228.715021 \nL 226.338802 228.589481 \nL 229.976614 228.787816 \nL 231.189218 228.681891 \nL 232.401822 228.808741 \nL 234.827031 228.759053 \nL 236.039635 228.932961 \nL 238.464843 228.831796 \nL 254.228697 228.920839 \nL 262.716925 229.031333 \nL 269.99255 229.08853 \nL 283.331196 229.138691 \nL 284.5438 229.034425 \nL 288.181612 229.174644 \nL 294.244633 229.096432 \nL 296.669841 229.184109 \nL 297.882445 229.141329 \nL 299.095049 229.253862 \nL 305.15807 229.191634 \nL 308.795882 229.151613 \nL 311.221091 229.250522 \nL 334.260569 229.328571 \nL 363.363068 229.433251 \nL 363.363068 229.433251 \n\" style=\"fill:none;stroke:#008000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p9953269898)\" d=\"M 60.212036 84.61139 \nL 61.42464 50.713812 \nL 62.637244 46.766947 \nL 63.849848 44.71774 \nL 65.062452 43.198954 \nL 66.275057 42.118343 \nL 67.487661 41.436372 \nL 68.700265 40.316227 \nL 71.125473 39.38387 \nL 72.338077 39.107129 \nL 73.550681 38.553645 \nL 74.763285 38.362561 \nL 75.97589 37.927681 \nL 78.401098 37.374198 \nL 79.613702 37.077689 \nL 82.03891 36.873427 \nL 83.251514 36.517616 \nL 85.676723 36.221107 \nL 92.952347 35.36123 \nL 99.015368 34.830809 \nL 100.227972 34.857165 \nL 101.440576 34.52771 \nL 103.865785 34.593601 \nL 106.290993 34.267441 \nL 112.354013 34.059885 \nL 113.566618 33.858918 \nL 114.779222 33.95446 \nL 115.991826 33.710663 \nL 118.417034 33.783143 \nL 120.842242 33.51958 \nL 147.519533 32.873849 \nL 152.36995 32.82443 \nL 154.795158 32.71571 \nL 157.220366 32.75195 \nL 158.43297 32.616874 \nL 160.858179 32.68606 \nL 163.283387 32.68606 \nL 168.133803 32.630052 \nL 185.110261 32.501565 \nL 186.322865 32.574045 \nL 187.535469 32.481798 \nL 188.748074 32.518038 \nL 189.960678 32.429085 \nL 191.173282 32.600401 \nL 193.59849 32.485092 \nL 305.15807 32.28083 \nL 308.795882 32.320365 \nL 311.221091 32.28083 \nL 363.363068 32.218234 \nL 363.363068 32.218234 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 239.758125 \nL 43.78125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 239.758125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 22.318125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_15\">\n    <!-- PERFORMANCE -->\n    <g transform=\"translate(165.509062 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      <path d=\"M 44.390625 34.1875 \nQ 47.5625 33.109375 50.5625 29.59375 \nQ 53.5625 26.078125 56.59375 19.921875 \nL 66.609375 0 \nL 56 0 \nL 46.6875 18.703125 \nQ 43.0625 26.03125 39.671875 28.421875 \nQ 36.28125 30.8125 30.421875 30.8125 \nL 19.671875 30.8125 \nL 19.671875 0 \nL 9.8125 0 \nL 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.578125 72.90625 50.734375 67.671875 \nQ 56.890625 62.453125 56.890625 51.90625 \nQ 56.890625 45.015625 53.6875 40.46875 \nQ 50.484375 35.9375 44.390625 34.1875 \nz\nM 19.671875 64.796875 \nL 19.671875 38.921875 \nL 32.078125 38.921875 \nQ 39.203125 38.921875 42.84375 42.21875 \nQ 46.484375 45.515625 46.484375 51.90625 \nQ 46.484375 58.296875 42.84375 61.546875 \nQ 39.203125 64.796875 32.078125 64.796875 \nz\n\" id=\"DejaVuSans-82\"/>\n      <path d=\"M 9.8125 72.90625 \nL 51.703125 72.90625 \nL 51.703125 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.109375 \nL 48.578125 43.109375 \nL 48.578125 34.8125 \nL 19.671875 34.8125 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-70\"/>\n      <path d=\"M 39.40625 66.21875 \nQ 28.65625 66.21875 22.328125 58.203125 \nQ 16.015625 50.203125 16.015625 36.375 \nQ 16.015625 22.609375 22.328125 14.59375 \nQ 28.65625 6.59375 39.40625 6.59375 \nQ 50.140625 6.59375 56.421875 14.59375 \nQ 62.703125 22.609375 62.703125 36.375 \nQ 62.703125 50.203125 56.421875 58.203125 \nQ 50.140625 66.21875 39.40625 66.21875 \nz\nM 39.40625 74.21875 \nQ 54.734375 74.21875 63.90625 63.9375 \nQ 73.09375 53.65625 73.09375 36.375 \nQ 73.09375 19.140625 63.90625 8.859375 \nQ 54.734375 -1.421875 39.40625 -1.421875 \nQ 24.03125 -1.421875 14.8125 8.828125 \nQ 5.609375 19.09375 5.609375 36.375 \nQ 5.609375 53.65625 14.8125 63.9375 \nQ 24.03125 74.21875 39.40625 74.21875 \nz\n\" id=\"DejaVuSans-79\"/>\n      <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n      <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n      <path d=\"M 9.8125 72.90625 \nL 23.09375 72.90625 \nL 55.421875 11.921875 \nL 55.421875 72.90625 \nL 64.984375 72.90625 \nL 64.984375 0 \nL 51.703125 0 \nL 19.390625 60.984375 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-78\"/>\n      <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-67\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-80\"/>\n     <use x=\"60.302734\" xlink:href=\"#DejaVuSans-69\"/>\n     <use x=\"123.486328\" xlink:href=\"#DejaVuSans-82\"/>\n     <use x=\"192.96875\" xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"250.488281\" xlink:href=\"#DejaVuSans-79\"/>\n     <use x=\"329.199219\" xlink:href=\"#DejaVuSans-82\"/>\n     <use x=\"398.681641\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"484.960938\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"553.369141\" xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"628.173828\" xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"697.998047\" xlink:href=\"#DejaVuSans-69\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 276.146875 147.21625 \nL 371.58125 147.21625 \nQ 373.58125 147.21625 373.58125 145.21625 \nL 373.58125 116.86 \nQ 373.58125 114.86 371.58125 114.86 \nL 276.146875 114.86 \nQ 274.146875 114.86 274.146875 116.86 \nL 274.146875 145.21625 \nQ 274.146875 147.21625 276.146875 147.21625 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 278.146875 122.958437 \nL 298.146875 122.958437 \n\" style=\"fill:none;stroke:#008000;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_18\"/>\n    <g id=\"text_16\">\n     <!-- average loss -->\n     <g transform=\"translate(306.146875 126.458437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n       <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n       <path id=\"DejaVuSans-32\"/>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"181.982422\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"223.095703\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"284.375\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"347.851562\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"409.375\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"441.162109\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"468.945312\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"530.126953\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"582.226562\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 278.146875 137.636562 \nL 298.146875 137.636562 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_17\">\n     <!-- accuracy -->\n     <g transform=\"translate(306.146875 141.136562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p9953269898\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "axis = [i for i in range(completed_epochs+1)]\n",
    "one = [1 for i in range(completed_epochs+1)]\n",
    "zero = [0 for i in range(completed_epochs+1)]\n",
    "_ = plt.plot(axis, one, linestyle = 'dashed', color = 'black')\n",
    "_ = plt.plot(axis, zero, linestyle = 'dashed', color = 'black')\n",
    "_ = plt.plot(axis, performance_training[1], label = 'average loss', color = 'green')\n",
    "__ = plt.xlabel('epoch') \n",
    "axis = [i for i in range(completed_epochs+1)]\n",
    "_ = plt.plot(axis, performance_training[2], label = 'accuracy', color = 'red')\n",
    "__ = plt.xlabel('epoch') \n",
    "___ = plt.ylabel('loss & accuracy')\n",
    "____ = plt.title('PERFORMANCE')\n",
    "_____ = plt.legend()"
   ]
  },
  {
   "source": [
    "One can notice that the chosen Adam optimizer (with $\\lambda = 5\\cdot 10^{-5}$, where $\\lambda$ is the learning rate) takes less epochs to reach target accuracy with respect to the SGD optimizer (with $\\lambda = 10^{-3}$ and $\\rho = 0.9$, where $\\lambda$ is the learning rate and $\\rho$ the momentum)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Assignment 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size_train = 256\n",
    "minibatch_size_test = 512\n",
    "\n",
    "_, testloader, trainset, testset = mnist.get_data(batch_size_train=minibatch_size_test, batch_size_test=minibatch_size_test)"
   ]
  },
  {
   "source": [
    "In order to complete the second task is required to permute the targets in the training set: this is done by creating new permuted indeces in the range of the size of the targets' tensor and shuffling the `trainset.targets` according to this new indeces. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "targets before permutation\ntensor([5, 0, 4,  ..., 5, 6, 8])\n\ntargets after permutation\ntensor([2, 9, 2,  ..., 5, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "print('targets before permutation')\n",
    "print(trainset.targets)\n",
    "# compute new permuted indeces\n",
    "perm_idx = torch.randperm(trainset.targets.shape[0])\n",
    "# get new trainset by shuffling targets according to new indeces\n",
    "trainset_targets_permuted = trainset.targets[perm_idx]\n",
    "# set new targets as trainset\n",
    "trainset.targets = trainset_targets_permuted\n",
    "print('\\ntargets after permutation')\n",
    "print(trainset.targets)\n",
    "# new trainloader\n",
    "trainloader_permuted = torch.utils.data.DataLoader(trainset, batch_size=minibatch_size_train, shuffle=True)"
   ]
  },
  {
   "source": [
    "### Training function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, criterion, optimizer, trainloader, epochs = 20):\n",
    "    # creating container for saving loss and accuracy measurments\n",
    "    performance_training = [[None],[None],[None]]\n",
    "    steps = 0\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('\\n')\n",
    "        print('Epoch {}\\n'.format(i+1))\n",
    "        steps += 1\n",
    "\n",
    "        ############\n",
    "        # TRAINING #\n",
    "        ############\n",
    "        loss_meter_train = AverageMeter()\n",
    "        accuracy_meter_train = AverageMeter()\n",
    "        model.train()\n",
    "        for X, y in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc = accuracy(y_hat, y)\n",
    "            loss_meter_train.update(val=loss.item(), n=X.shape[0])\n",
    "            accuracy_meter_train.update(val=acc, n=X.shape[0])\n",
    "        print('TRAINING loss {:.4f} (avg {:.4f}) - TRAINING accuracy {:.4f}\\n'.format(loss_meter_train.sum, loss_meter_train.avg, accuracy_meter_train.avg))\n",
    "        performance_training[0].append(loss_meter_train.sum)\n",
    "        performance_training[1].append(loss_meter_train.avg)\n",
    "        performance_training[2].append(accuracy_meter_train.avg)\n",
    "\n",
    "        if accuracy_meter_train.avg >= 0.9999:\n",
    "            print('######################')\n",
    "            print('# TRAINING COMPLETED #')\n",
    "            print('######################')\n",
    "            break\n",
    "\n",
    "    return performance_training, steps"
   ]
  },
  {
   "source": [
    "### Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Since in this case training is expected to be harder, the network has been enlarged."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_wider(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=1024),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=.2), # we add a dropout here. it's referred to the previous layer (with 32 neurons)\n",
    "\n",
    "            nn.BatchNorm1d(num_features=2048),\n",
    "            nn.Linear(2048, 1536),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=1536),\n",
    "            nn.Linear(1536, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "source": [
    "#### Adam optimizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_wider()\n",
    "learning_rate = .00005\n",
    "optimizer_3 = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "54) - TRAINING accuracy 0.9981\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1745\n",
      "\n",
      "TRAINING loss 391.9581 (avg 0.0065) - TRAINING accuracy 0.9978\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1746\n",
      "\n",
      "TRAINING loss 283.9016 (avg 0.0047) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1747\n",
      "\n",
      "TRAINING loss 247.0490 (avg 0.0041) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1748\n",
      "\n",
      "TRAINING loss 252.3882 (avg 0.0042) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1749\n",
      "\n",
      "TRAINING loss 260.9426 (avg 0.0043) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1750\n",
      "\n",
      "TRAINING loss 270.1106 (avg 0.0045) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1751\n",
      "\n",
      "TRAINING loss 342.1277 (avg 0.0057) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1752\n",
      "\n",
      "TRAINING loss 292.1978 (avg 0.0049) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1753\n",
      "\n",
      "TRAINING loss 308.4148 (avg 0.0051) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1754\n",
      "\n",
      "TRAINING loss 254.2664 (avg 0.0042) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1755\n",
      "\n",
      "TRAINING loss 305.7201 (avg 0.0051) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1756\n",
      "\n",
      "TRAINING loss 277.6719 (avg 0.0046) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1757\n",
      "\n",
      "TRAINING loss 316.2303 (avg 0.0053) - TRAINING accuracy 0.9981\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1758\n",
      "\n",
      "TRAINING loss 240.9217 (avg 0.0040) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1759\n",
      "\n",
      "TRAINING loss 297.7042 (avg 0.0050) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1760\n",
      "\n",
      "TRAINING loss 302.9803 (avg 0.0050) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1761\n",
      "\n",
      "TRAINING loss 294.7951 (avg 0.0049) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1762\n",
      "\n",
      "TRAINING loss 340.9788 (avg 0.0057) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1763\n",
      "\n",
      "TRAINING loss 291.8826 (avg 0.0049) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1764\n",
      "\n",
      "TRAINING loss 260.5191 (avg 0.0043) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1765\n",
      "\n",
      "TRAINING loss 269.0117 (avg 0.0045) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1766\n",
      "\n",
      "TRAINING loss 262.5442 (avg 0.0044) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1767\n",
      "\n",
      "TRAINING loss 292.5761 (avg 0.0049) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1768\n",
      "\n",
      "TRAINING loss 351.9486 (avg 0.0059) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1769\n",
      "\n",
      "TRAINING loss 298.4504 (avg 0.0050) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1770\n",
      "\n",
      "TRAINING loss 289.2449 (avg 0.0048) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1771\n",
      "\n",
      "TRAINING loss 335.3444 (avg 0.0056) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1772\n",
      "\n",
      "TRAINING loss 324.9665 (avg 0.0054) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1773\n",
      "\n",
      "TRAINING loss 331.3483 (avg 0.0055) - TRAINING accuracy 0.9981\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1774\n",
      "\n",
      "TRAINING loss 252.8678 (avg 0.0042) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1775\n",
      "\n",
      "TRAINING loss 291.2223 (avg 0.0049) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1776\n",
      "\n",
      "TRAINING loss 291.0140 (avg 0.0049) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1777\n",
      "\n",
      "TRAINING loss 286.6104 (avg 0.0048) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1778\n",
      "\n",
      "TRAINING loss 327.3838 (avg 0.0055) - TRAINING accuracy 0.9980\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1779\n",
      "\n",
      "TRAINING loss 318.4203 (avg 0.0053) - TRAINING accuracy 0.9980\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1780\n",
      "\n",
      "TRAINING loss 320.1611 (avg 0.0053) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1781\n",
      "\n",
      "TRAINING loss 328.3222 (avg 0.0055) - TRAINING accuracy 0.9980\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1782\n",
      "\n",
      "TRAINING loss 350.9760 (avg 0.0058) - TRAINING accuracy 0.9980\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1783\n",
      "\n",
      "TRAINING loss 337.2915 (avg 0.0056) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1784\n",
      "\n",
      "TRAINING loss 278.2894 (avg 0.0046) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1785\n",
      "\n",
      "TRAINING loss 331.6096 (avg 0.0055) - TRAINING accuracy 0.9981\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1786\n",
      "\n",
      "TRAINING loss 316.9537 (avg 0.0053) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1787\n",
      "\n",
      "TRAINING loss 227.7144 (avg 0.0038) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1788\n",
      "\n",
      "TRAINING loss 284.9240 (avg 0.0047) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1789\n",
      "\n",
      "TRAINING loss 301.4307 (avg 0.0050) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1790\n",
      "\n",
      "TRAINING loss 313.4808 (avg 0.0052) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1791\n",
      "\n",
      "TRAINING loss 309.5010 (avg 0.0052) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1792\n",
      "\n",
      "TRAINING loss 357.4228 (avg 0.0060) - TRAINING accuracy 0.9981\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1793\n",
      "\n",
      "TRAINING loss 377.6534 (avg 0.0063) - TRAINING accuracy 0.9981\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1794\n",
      "\n",
      "TRAINING loss 348.4008 (avg 0.0058) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1795\n",
      "\n",
      "TRAINING loss 302.3498 (avg 0.0050) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1796\n",
      "\n",
      "TRAINING loss 338.7464 (avg 0.0056) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1797\n",
      "\n",
      "TRAINING loss 278.5064 (avg 0.0046) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1798\n",
      "\n",
      "TRAINING loss 240.2987 (avg 0.0040) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1799\n",
      "\n",
      "TRAINING loss 240.2114 (avg 0.0040) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1800\n",
      "\n",
      "TRAINING loss 233.6046 (avg 0.0039) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1801\n",
      "\n",
      "TRAINING loss 311.8592 (avg 0.0052) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1802\n",
      "\n",
      "TRAINING loss 287.6563 (avg 0.0048) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1803\n",
      "\n",
      "TRAINING loss 333.7280 (avg 0.0056) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1804\n",
      "\n",
      "TRAINING loss 332.2660 (avg 0.0055) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1805\n",
      "\n",
      "TRAINING loss 263.8121 (avg 0.0044) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1806\n",
      "\n",
      "TRAINING loss 283.3492 (avg 0.0047) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1807\n",
      "\n",
      "TRAINING loss 296.7611 (avg 0.0049) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1808\n",
      "\n",
      "TRAINING loss 278.9275 (avg 0.0046) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1809\n",
      "\n",
      "TRAINING loss 282.5386 (avg 0.0047) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1810\n",
      "\n",
      "TRAINING loss 289.0833 (avg 0.0048) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1811\n",
      "\n",
      "TRAINING loss 383.5143 (avg 0.0064) - TRAINING accuracy 0.9979\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1812\n",
      "\n",
      "TRAINING loss 305.0794 (avg 0.0051) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1813\n",
      "\n",
      "TRAINING loss 300.0703 (avg 0.0050) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1814\n",
      "\n",
      "TRAINING loss 241.7943 (avg 0.0040) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1815\n",
      "\n",
      "TRAINING loss 364.0042 (avg 0.0061) - TRAINING accuracy 0.9981\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1816\n",
      "\n",
      "TRAINING loss 311.1511 (avg 0.0052) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1817\n",
      "\n",
      "TRAINING loss 292.5098 (avg 0.0049) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1818\n",
      "\n",
      "TRAINING loss 252.8712 (avg 0.0042) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1819\n",
      "\n",
      "TRAINING loss 278.9266 (avg 0.0046) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1820\n",
      "\n",
      "TRAINING loss 274.7227 (avg 0.0046) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1821\n",
      "\n",
      "TRAINING loss 312.1022 (avg 0.0052) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1822\n",
      "\n",
      "TRAINING loss 247.4351 (avg 0.0041) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1823\n",
      "\n",
      "TRAINING loss 256.8375 (avg 0.0043) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1824\n",
      "\n",
      "TRAINING loss 345.1530 (avg 0.0058) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1825\n",
      "\n",
      "TRAINING loss 271.4880 (avg 0.0045) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1826\n",
      "\n",
      "TRAINING loss 285.7406 (avg 0.0048) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1827\n",
      "\n",
      "TRAINING loss 250.9832 (avg 0.0042) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1828\n",
      "\n",
      "TRAINING loss 281.4972 (avg 0.0047) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1829\n",
      "\n",
      "TRAINING loss 270.7756 (avg 0.0045) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1830\n",
      "\n",
      "TRAINING loss 255.6007 (avg 0.0043) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1831\n",
      "\n",
      "TRAINING loss 302.1841 (avg 0.0050) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1832\n",
      "\n",
      "TRAINING loss 362.6481 (avg 0.0060) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1833\n",
      "\n",
      "TRAINING loss 300.5273 (avg 0.0050) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1834\n",
      "\n",
      "TRAINING loss 293.8426 (avg 0.0049) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1835\n",
      "\n",
      "TRAINING loss 225.0487 (avg 0.0038) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1836\n",
      "\n",
      "TRAINING loss 257.6252 (avg 0.0043) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1837\n",
      "\n",
      "TRAINING loss 284.1561 (avg 0.0047) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1838\n",
      "\n",
      "TRAINING loss 354.3770 (avg 0.0059) - TRAINING accuracy 0.9981\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1839\n",
      "\n",
      "TRAINING loss 326.7950 (avg 0.0054) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1840\n",
      "\n",
      "TRAINING loss 309.9247 (avg 0.0052) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1841\n",
      "\n",
      "TRAINING loss 286.3647 (avg 0.0048) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1842\n",
      "\n",
      "TRAINING loss 295.2650 (avg 0.0049) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1843\n",
      "\n",
      "TRAINING loss 263.6684 (avg 0.0044) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1844\n",
      "\n",
      "TRAINING loss 252.9889 (avg 0.0042) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1845\n",
      "\n",
      "TRAINING loss 245.8937 (avg 0.0041) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1846\n",
      "\n",
      "TRAINING loss 261.9960 (avg 0.0044) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1847\n",
      "\n",
      "TRAINING loss 350.9190 (avg 0.0058) - TRAINING accuracy 0.9980\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1848\n",
      "\n",
      "TRAINING loss 351.4250 (avg 0.0059) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1849\n",
      "\n",
      "TRAINING loss 283.5317 (avg 0.0047) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1850\n",
      "\n",
      "TRAINING loss 262.0550 (avg 0.0044) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1851\n",
      "\n",
      "TRAINING loss 261.7606 (avg 0.0044) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1852\n",
      "\n",
      "TRAINING loss 269.2524 (avg 0.0045) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1853\n",
      "\n",
      "TRAINING loss 251.2805 (avg 0.0042) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1854\n",
      "\n",
      "TRAINING loss 293.7898 (avg 0.0049) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1855\n",
      "\n",
      "TRAINING loss 294.5754 (avg 0.0049) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1856\n",
      "\n",
      "TRAINING loss 207.0797 (avg 0.0035) - TRAINING accuracy 0.9990\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1857\n",
      "\n",
      "TRAINING loss 221.4209 (avg 0.0037) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1858\n",
      "\n",
      "TRAINING loss 277.0406 (avg 0.0046) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1859\n",
      "\n",
      "TRAINING loss 292.9331 (avg 0.0049) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1860\n",
      "\n",
      "TRAINING loss 244.8134 (avg 0.0041) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1861\n",
      "\n",
      "TRAINING loss 218.6064 (avg 0.0036) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1862\n",
      "\n",
      "TRAINING loss 266.9016 (avg 0.0044) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1863\n",
      "\n",
      "TRAINING loss 263.0260 (avg 0.0044) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1864\n",
      "\n",
      "TRAINING loss 316.1550 (avg 0.0053) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1865\n",
      "\n",
      "TRAINING loss 273.6115 (avg 0.0046) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1866\n",
      "\n",
      "TRAINING loss 283.7824 (avg 0.0047) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1867\n",
      "\n",
      "TRAINING loss 292.5078 (avg 0.0049) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1868\n",
      "\n",
      "TRAINING loss 324.9624 (avg 0.0054) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1869\n",
      "\n",
      "TRAINING loss 327.6926 (avg 0.0055) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1870\n",
      "\n",
      "TRAINING loss 267.8709 (avg 0.0045) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1871\n",
      "\n",
      "TRAINING loss 299.7851 (avg 0.0050) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1872\n",
      "\n",
      "TRAINING loss 208.2074 (avg 0.0035) - TRAINING accuracy 0.9990\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1873\n",
      "\n",
      "TRAINING loss 258.4746 (avg 0.0043) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1874\n",
      "\n",
      "TRAINING loss 271.5274 (avg 0.0045) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1875\n",
      "\n",
      "TRAINING loss 277.2943 (avg 0.0046) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1876\n",
      "\n",
      "TRAINING loss 268.1022 (avg 0.0045) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1877\n",
      "\n",
      "TRAINING loss 280.7818 (avg 0.0047) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1878\n",
      "\n",
      "TRAINING loss 274.6437 (avg 0.0046) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1879\n",
      "\n",
      "TRAINING loss 304.7075 (avg 0.0051) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1880\n",
      "\n",
      "TRAINING loss 381.1128 (avg 0.0064) - TRAINING accuracy 0.9980\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1881\n",
      "\n",
      "TRAINING loss 272.5771 (avg 0.0045) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1882\n",
      "\n",
      "TRAINING loss 288.2740 (avg 0.0048) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1883\n",
      "\n",
      "TRAINING loss 318.1287 (avg 0.0053) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1884\n",
      "\n",
      "TRAINING loss 244.2500 (avg 0.0041) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1885\n",
      "\n",
      "TRAINING loss 281.9447 (avg 0.0047) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1886\n",
      "\n",
      "TRAINING loss 270.1387 (avg 0.0045) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1887\n",
      "\n",
      "TRAINING loss 263.5735 (avg 0.0044) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1888\n",
      "\n",
      "TRAINING loss 271.2214 (avg 0.0045) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1889\n",
      "\n",
      "TRAINING loss 272.4805 (avg 0.0045) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1890\n",
      "\n",
      "TRAINING loss 337.7396 (avg 0.0056) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1891\n",
      "\n",
      "TRAINING loss 341.3817 (avg 0.0057) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1892\n",
      "\n",
      "TRAINING loss 249.0873 (avg 0.0042) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1893\n",
      "\n",
      "TRAINING loss 265.8517 (avg 0.0044) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1894\n",
      "\n",
      "TRAINING loss 311.1441 (avg 0.0052) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1895\n",
      "\n",
      "TRAINING loss 360.8363 (avg 0.0060) - TRAINING accuracy 0.9980\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1896\n",
      "\n",
      "TRAINING loss 265.4081 (avg 0.0044) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1897\n",
      "\n",
      "TRAINING loss 215.1730 (avg 0.0036) - TRAINING accuracy 0.9990\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1898\n",
      "\n",
      "TRAINING loss 301.5875 (avg 0.0050) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1899\n",
      "\n",
      "TRAINING loss 266.5837 (avg 0.0044) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1900\n",
      "\n",
      "TRAINING loss 311.3121 (avg 0.0052) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1901\n",
      "\n",
      "TRAINING loss 292.7430 (avg 0.0049) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1902\n",
      "\n",
      "TRAINING loss 344.2442 (avg 0.0057) - TRAINING accuracy 0.9981\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1903\n",
      "\n",
      "TRAINING loss 228.5103 (avg 0.0038) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1904\n",
      "\n",
      "TRAINING loss 221.7263 (avg 0.0037) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1905\n",
      "\n",
      "TRAINING loss 251.9957 (avg 0.0042) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1906\n",
      "\n",
      "TRAINING loss 299.6032 (avg 0.0050) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1907\n",
      "\n",
      "TRAINING loss 245.5278 (avg 0.0041) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1908\n",
      "\n",
      "TRAINING loss 243.2497 (avg 0.0041) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1909\n",
      "\n",
      "TRAINING loss 213.1969 (avg 0.0036) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1910\n",
      "\n",
      "TRAINING loss 240.1082 (avg 0.0040) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1911\n",
      "\n",
      "TRAINING loss 317.1131 (avg 0.0053) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1912\n",
      "\n",
      "TRAINING loss 350.3142 (avg 0.0058) - TRAINING accuracy 0.9981\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1913\n",
      "\n",
      "TRAINING loss 304.1362 (avg 0.0051) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1914\n",
      "\n",
      "TRAINING loss 356.5333 (avg 0.0059) - TRAINING accuracy 0.9980\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1915\n",
      "\n",
      "TRAINING loss 266.6541 (avg 0.0044) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1916\n",
      "\n",
      "TRAINING loss 281.7326 (avg 0.0047) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1917\n",
      "\n",
      "TRAINING loss 228.4006 (avg 0.0038) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1918\n",
      "\n",
      "TRAINING loss 291.7152 (avg 0.0049) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1919\n",
      "\n",
      "TRAINING loss 222.7385 (avg 0.0037) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1920\n",
      "\n",
      "TRAINING loss 256.5982 (avg 0.0043) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1921\n",
      "\n",
      "TRAINING loss 294.6584 (avg 0.0049) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1922\n",
      "\n",
      "TRAINING loss 272.1145 (avg 0.0045) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1923\n",
      "\n",
      "TRAINING loss 291.4461 (avg 0.0049) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1924\n",
      "\n",
      "TRAINING loss 286.8072 (avg 0.0048) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1925\n",
      "\n",
      "TRAINING loss 259.3633 (avg 0.0043) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1926\n",
      "\n",
      "TRAINING loss 287.5366 (avg 0.0048) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1927\n",
      "\n",
      "TRAINING loss 262.6201 (avg 0.0044) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1928\n",
      "\n",
      "TRAINING loss 211.9017 (avg 0.0035) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1929\n",
      "\n",
      "TRAINING loss 247.1585 (avg 0.0041) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1930\n",
      "\n",
      "TRAINING loss 233.5793 (avg 0.0039) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1931\n",
      "\n",
      "TRAINING loss 235.7851 (avg 0.0039) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1932\n",
      "\n",
      "TRAINING loss 258.3817 (avg 0.0043) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1933\n",
      "\n",
      "TRAINING loss 277.3619 (avg 0.0046) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1934\n",
      "\n",
      "TRAINING loss 211.0735 (avg 0.0035) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1935\n",
      "\n",
      "TRAINING loss 243.7558 (avg 0.0041) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1936\n",
      "\n",
      "TRAINING loss 249.9050 (avg 0.0042) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1937\n",
      "\n",
      "TRAINING loss 369.2899 (avg 0.0062) - TRAINING accuracy 0.9980\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1938\n",
      "\n",
      "TRAINING loss 291.7955 (avg 0.0049) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1939\n",
      "\n",
      "TRAINING loss 247.5105 (avg 0.0041) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1940\n",
      "\n",
      "TRAINING loss 205.8978 (avg 0.0034) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1941\n",
      "\n",
      "TRAINING loss 242.2784 (avg 0.0040) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1942\n",
      "\n",
      "TRAINING loss 329.6481 (avg 0.0055) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1943\n",
      "\n",
      "TRAINING loss 356.4917 (avg 0.0059) - TRAINING accuracy 0.9979\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1944\n",
      "\n",
      "TRAINING loss 329.1139 (avg 0.0055) - TRAINING accuracy 0.9981\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1945\n",
      "\n",
      "TRAINING loss 362.6443 (avg 0.0060) - TRAINING accuracy 0.9979\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1946\n",
      "\n",
      "TRAINING loss 394.4888 (avg 0.0066) - TRAINING accuracy 0.9980\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1947\n",
      "\n",
      "TRAINING loss 410.9609 (avg 0.0068) - TRAINING accuracy 0.9977\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1948\n",
      "\n",
      "TRAINING loss 293.2521 (avg 0.0049) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1949\n",
      "\n",
      "TRAINING loss 215.1768 (avg 0.0036) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1950\n",
      "\n",
      "TRAINING loss 225.7959 (avg 0.0038) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1951\n",
      "\n",
      "TRAINING loss 287.8541 (avg 0.0048) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1952\n",
      "\n",
      "TRAINING loss 200.8122 (avg 0.0033) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1953\n",
      "\n",
      "TRAINING loss 274.8158 (avg 0.0046) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1954\n",
      "\n",
      "TRAINING loss 263.2466 (avg 0.0044) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1955\n",
      "\n",
      "TRAINING loss 272.8572 (avg 0.0045) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1956\n",
      "\n",
      "TRAINING loss 302.8738 (avg 0.0050) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1957\n",
      "\n",
      "TRAINING loss 250.4801 (avg 0.0042) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1958\n",
      "\n",
      "TRAINING loss 201.5970 (avg 0.0034) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1959\n",
      "\n",
      "TRAINING loss 204.8893 (avg 0.0034) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1960\n",
      "\n",
      "TRAINING loss 244.0890 (avg 0.0041) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1961\n",
      "\n",
      "TRAINING loss 286.9291 (avg 0.0048) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1962\n",
      "\n",
      "TRAINING loss 276.6599 (avg 0.0046) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1963\n",
      "\n",
      "TRAINING loss 274.3628 (avg 0.0046) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1964\n",
      "\n",
      "TRAINING loss 256.5760 (avg 0.0043) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1965\n",
      "\n",
      "TRAINING loss 243.7497 (avg 0.0041) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1966\n",
      "\n",
      "TRAINING loss 263.8247 (avg 0.0044) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1967\n",
      "\n",
      "TRAINING loss 283.2267 (avg 0.0047) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1968\n",
      "\n",
      "TRAINING loss 358.6028 (avg 0.0060) - TRAINING accuracy 0.9979\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1969\n",
      "\n",
      "TRAINING loss 288.1028 (avg 0.0048) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1970\n",
      "\n",
      "TRAINING loss 324.3176 (avg 0.0054) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1971\n",
      "\n",
      "TRAINING loss 259.6033 (avg 0.0043) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1972\n",
      "\n",
      "TRAINING loss 242.9784 (avg 0.0040) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1973\n",
      "\n",
      "TRAINING loss 287.5481 (avg 0.0048) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1974\n",
      "\n",
      "TRAINING loss 295.2484 (avg 0.0049) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1975\n",
      "\n",
      "TRAINING loss 269.5245 (avg 0.0045) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1976\n",
      "\n",
      "TRAINING loss 283.2546 (avg 0.0047) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1977\n",
      "\n",
      "TRAINING loss 271.2640 (avg 0.0045) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1978\n",
      "\n",
      "TRAINING loss 273.8707 (avg 0.0046) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1979\n",
      "\n",
      "TRAINING loss 253.2018 (avg 0.0042) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1980\n",
      "\n",
      "TRAINING loss 247.3804 (avg 0.0041) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1981\n",
      "\n",
      "TRAINING loss 203.3943 (avg 0.0034) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1982\n",
      "\n",
      "TRAINING loss 201.2063 (avg 0.0034) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1983\n",
      "\n",
      "TRAINING loss 277.5268 (avg 0.0046) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1984\n",
      "\n",
      "TRAINING loss 327.1659 (avg 0.0055) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1985\n",
      "\n",
      "TRAINING loss 246.3740 (avg 0.0041) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1986\n",
      "\n",
      "TRAINING loss 244.9870 (avg 0.0041) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1987\n",
      "\n",
      "TRAINING loss 271.5881 (avg 0.0045) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1988\n",
      "\n",
      "TRAINING loss 258.4171 (avg 0.0043) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1989\n",
      "\n",
      "TRAINING loss 317.8652 (avg 0.0053) - TRAINING accuracy 0.9982\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1990\n",
      "\n",
      "TRAINING loss 334.6162 (avg 0.0056) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1991\n",
      "\n",
      "TRAINING loss 302.4646 (avg 0.0050) - TRAINING accuracy 0.9984\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1992\n",
      "\n",
      "TRAINING loss 285.6706 (avg 0.0048) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1993\n",
      "\n",
      "TRAINING loss 240.2204 (avg 0.0040) - TRAINING accuracy 0.9988\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1994\n",
      "\n",
      "TRAINING loss 228.3956 (avg 0.0038) - TRAINING accuracy 0.9989\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1995\n",
      "\n",
      "TRAINING loss 239.4708 (avg 0.0040) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1996\n",
      "\n",
      "TRAINING loss 305.4402 (avg 0.0051) - TRAINING accuracy 0.9983\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1997\n",
      "\n",
      "TRAINING loss 260.1707 (avg 0.0043) - TRAINING accuracy 0.9986\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1998\n",
      "\n",
      "TRAINING loss 263.7670 (avg 0.0044) - TRAINING accuracy 0.9985\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1999\n",
      "\n",
      "TRAINING loss 252.7331 (avg 0.0042) - TRAINING accuracy 0.9987\n",
      "\n",
      "\n",
      "\n",
      "Epoch 2000\n",
      "\n",
      "TRAINING loss 351.0749 (avg 0.0059) - TRAINING accuracy 0.9980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "performance_training, completed_epochs = training(model, criterion, optimizer_3, trainloader_permuted, epochs = num_epochs)"
   ]
  },
  {
   "source": [
    "One can notice that, by permuting targets' labels, training requires much more effort; in order to achieve better performance one could further enhance the network by adding more units, or maybe use a learning rate scheduler to increase precision (by decreasing the learning rate) when needed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-04T10:59:27.555934</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 385.78125 277.314375 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \nL 378.58125 22.318125 \nL 43.78125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m50d9045bd7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#m50d9045bd7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(55.818182 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"97.044886\" xlink:href=\"#m50d9045bd7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 250 -->\n      <g transform=\"translate(87.501136 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"135.090341\" xlink:href=\"#m50d9045bd7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 500 -->\n      <g transform=\"translate(125.546591 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"173.135795\" xlink:href=\"#m50d9045bd7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 750 -->\n      <g transform=\"translate(163.592045 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"211.18125\" xlink:href=\"#m50d9045bd7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1000 -->\n      <g transform=\"translate(198.45625 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"249.226705\" xlink:href=\"#m50d9045bd7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1250 -->\n      <g transform=\"translate(236.501705 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"287.272159\" xlink:href=\"#m50d9045bd7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 1500 -->\n      <g transform=\"translate(274.547159 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"325.317614\" xlink:href=\"#m50d9045bd7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 1750 -->\n      <g transform=\"translate(312.592614 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.363068\" xlink:href=\"#m50d9045bd7\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 2000 -->\n      <g transform=\"translate(350.638068 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- epoch -->\n     <g transform=\"translate(195.953125 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m617ebfe526\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m617ebfe526\" y=\"229.874489\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 233.673707)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m617ebfe526\" y=\"188.849046\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.5 -->\n      <g transform=\"translate(20.878125 192.648265)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m617ebfe526\" y=\"147.823604\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 151.622823)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m617ebfe526\" y=\"106.798161\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.5 -->\n      <g transform=\"translate(20.878125 110.59738)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m617ebfe526\" y=\"65.772719\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 2.0 -->\n      <g transform=\"translate(20.878125 69.571938)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m617ebfe526\" y=\"24.747277\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 2.5 -->\n      <g transform=\"translate(20.878125 28.546495)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- loss &amp; accuracy -->\n     <g transform=\"translate(14.798438 170.332656)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n       <path id=\"DejaVuSans-32\"/>\n       <path d=\"M 24.3125 39.203125 \nQ 19.875 35.25 17.796875 31.3125 \nQ 15.71875 27.390625 15.71875 23.09375 \nQ 15.71875 15.96875 20.890625 11.234375 \nQ 26.078125 6.5 33.890625 6.5 \nQ 38.53125 6.5 42.578125 8.03125 \nQ 46.625 9.578125 50.203125 12.703125 \nz\nM 31.203125 44.671875 \nL 56 19.28125 \nQ 58.890625 23.640625 60.5 28.59375 \nQ 62.109375 33.546875 62.40625 39.109375 \nL 71.484375 39.109375 \nQ 70.90625 32.671875 68.359375 26.359375 \nQ 65.828125 20.0625 61.28125 13.921875 \nL 74.90625 0 \nL 62.59375 0 \nL 55.609375 7.171875 \nQ 50.53125 2.828125 44.96875 0.703125 \nQ 39.40625 -1.421875 33.015625 -1.421875 \nQ 21.234375 -1.421875 13.765625 5.296875 \nQ 6.296875 12.015625 6.296875 22.515625 \nQ 6.296875 28.765625 9.5625 34.25 \nQ 12.84375 39.75 19.390625 44.578125 \nQ 17.046875 47.65625 15.8125 50.703125 \nQ 14.59375 53.765625 14.59375 56.6875 \nQ 14.59375 64.59375 20.015625 69.40625 \nQ 25.4375 74.21875 34.421875 74.21875 \nQ 38.484375 74.21875 42.5 73.34375 \nQ 46.53125 72.46875 50.6875 70.703125 \nL 50.6875 61.8125 \nQ 46.4375 64.109375 42.578125 65.296875 \nQ 38.71875 66.5 35.40625 66.5 \nQ 30.28125 66.5 27.078125 63.78125 \nQ 23.875 61.078125 23.875 56.78125 \nQ 23.875 54.296875 25.3125 51.78125 \nQ 26.765625 49.265625 31.203125 44.671875 \nz\n\" id=\"DejaVuSans-38\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"193.164062\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"224.951172\" xlink:href=\"#DejaVuSans-38\"/>\n      <use x=\"302.929688\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"334.716797\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"395.996094\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"450.976562\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"505.957031\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"569.335938\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"610.449219\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"671.728516\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"726.708984\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#pecf1bd482f)\" d=\"M 58.999432 147.823604 \nL 363.363068 147.823604 \nL 363.363068 147.823604 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#pecf1bd482f)\" d=\"M 58.999432 229.874489 \nL 363.363068 229.874489 \nL 363.363068 229.874489 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#pecf1bd482f)\" d=\"M 59.151614 32.201761 \nL 59.455977 48.230413 \nL 60.369068 82.184421 \nL 62.651795 180.423096 \nL 63.260523 194.511789 \nL 64.173614 207.461896 \nL 65.086705 214.729767 \nL 65.391068 215.960694 \nL 65.695432 218.053583 \nL 65.847614 218.381743 \nL 66.456341 220.609218 \nL 66.608523 221.048545 \nL 66.760705 221.072189 \nL 67.21725 222.334484 \nL 67.369432 222.544112 \nL 67.673795 223.186067 \nL 68.130341 223.636795 \nL 68.739068 224.805284 \nL 69.043432 224.511539 \nL 69.804341 225.426375 \nL 69.956523 225.676181 \nL 70.260886 225.285516 \nL 70.413068 225.548023 \nL 70.56525 225.531654 \nL 70.869614 226.017625 \nL 71.173977 225.730195 \nL 71.478341 226.118941 \nL 71.630523 226.145244 \nL 71.782705 226.020959 \nL 72.087068 226.175351 \nL 72.543614 226.571585 \nL 72.695795 226.39588 \nL 72.847977 226.477348 \nL 73.000159 226.289633 \nL 73.152341 226.731286 \nL 73.608886 226.724409 \nL 73.761068 226.739451 \nL 73.91325 226.582234 \nL 74.065432 226.867235 \nL 74.217614 226.880388 \nL 74.369795 227.208819 \nL 74.674159 226.888391 \nL 74.978523 226.409978 \nL 75.282886 227.108843 \nL 75.435068 227.08369 \nL 75.58725 227.212751 \nL 75.739432 227.04715 \nL 76.195977 227.156899 \nL 76.348159 227.191093 \nL 76.500341 226.94069 \nL 76.804705 227.518273 \nL 77.109068 227.218485 \nL 77.26125 227.527567 \nL 77.413432 227.04211 \nL 77.717795 227.18914 \nL 77.869977 227.565897 \nL 78.022159 227.318409 \nL 78.174341 227.384863 \nL 78.326523 227.101664 \nL 78.478705 227.095793 \nL 78.783068 227.632317 \nL 78.93525 227.69504 \nL 79.087432 227.416192 \nL 79.848341 227.32532 \nL 80.152705 227.767669 \nL 80.304886 227.491819 \nL 80.457068 227.481682 \nL 80.60925 227.638535 \nL 80.761432 227.559868 \nL 81.370159 227.734167 \nL 81.522341 227.643166 \nL 81.674523 227.367635 \nL 82.131068 227.687758 \nL 82.28325 227.586225 \nL 82.435432 227.876082 \nL 82.739795 227.731926 \nL 82.891977 227.774527 \nL 83.044159 227.658432 \nL 83.196341 227.184988 \nL 83.348523 227.808349 \nL 83.500705 227.931558 \nL 83.652886 227.795232 \nL 83.95725 228.10526 \nL 84.109432 228.011083 \nL 84.261614 227.592401 \nL 84.565977 227.539432 \nL 84.718159 227.888629 \nL 85.022523 227.860462 \nL 85.174705 228.058865 \nL 85.326886 227.865391 \nL 85.479068 227.929194 \nL 85.63125 228.196807 \nL 85.783432 228.145528 \nL 85.935614 228.230553 \nL 86.239977 227.826295 \nL 86.544341 227.505611 \nL 86.848705 227.905331 \nL 87.30525 228.187451 \nL 87.457432 227.914794 \nL 87.761795 228.028837 \nL 88.218341 228.135992 \nL 88.370523 227.915134 \nL 88.522705 228.11432 \nL 88.674886 227.839924 \nL 89.131432 228.185753 \nL 89.435795 227.717692 \nL 89.740159 228.001913 \nL 89.892341 228.038023 \nL 90.044523 228.263157 \nL 90.196705 228.062974 \nL 90.501068 228.093641 \nL 90.805432 227.942001 \nL 90.957614 228.177948 \nL 91.414159 227.849075 \nL 91.718523 228.244908 \nL 91.870705 228.252123 \nL 92.022886 228.099433 \nL 92.175068 228.237392 \nL 92.479432 228.10791 \nL 92.631614 228.093493 \nL 92.783795 227.935652 \nL 93.088159 227.861597 \nL 93.240341 228.168666 \nL 93.849068 228.191093 \nL 94.00125 228.439731 \nL 94.305614 228.050506 \nL 94.609977 228.215083 \nL 94.762159 227.892054 \nL 94.914341 227.91631 \nL 95.523068 228.485888 \nL 95.827432 228.375133 \nL 95.979614 228.464376 \nL 96.283977 228.22674 \nL 96.588341 228.344651 \nL 96.892705 227.994344 \nL 97.197068 228.2819 \nL 97.34925 228.414202 \nL 97.501432 228.152038 \nL 97.653614 228.112808 \nL 97.805795 228.241006 \nL 97.957977 228.095492 \nL 98.262341 228.486569 \nL 98.566705 228.56631 \nL 99.02325 228.071724 \nL 99.327614 228.47591 \nL 99.479795 228.285523 \nL 99.784159 228.416784 \nL 99.936341 228.187793 \nL 100.240705 228.449838 \nL 100.69725 228.370251 \nL 100.849432 228.218332 \nL 101.001614 228.317929 \nL 101.153795 228.23768 \nL 101.458159 228.364514 \nL 101.610341 228.353224 \nL 101.762523 228.566152 \nL 101.914705 228.50724 \nL 102.219068 228.171161 \nL 102.37125 228.212417 \nL 102.675614 228.662776 \nL 103.284341 228.231424 \nL 103.893068 228.573442 \nL 104.197432 228.380227 \nL 104.349614 228.660839 \nL 104.958341 228.222893 \nL 105.110523 228.214443 \nL 105.262705 228.463022 \nL 105.414886 228.357971 \nL 105.567068 228.603074 \nL 106.023614 228.683287 \nL 106.327977 228.191767 \nL 106.480159 228.287594 \nL 106.632341 228.234784 \nL 106.936705 228.600828 \nL 107.088886 228.406054 \nL 107.241068 228.728057 \nL 107.545432 228.620828 \nL 107.697614 228.666608 \nL 107.849795 228.318671 \nL 108.001977 228.538954 \nL 108.154159 228.478579 \nL 108.306341 228.297307 \nL 108.458523 228.636146 \nL 108.915068 228.663379 \nL 109.06725 228.449276 \nL 109.371614 228.485448 \nL 109.523795 228.392209 \nL 109.675977 228.511674 \nL 110.132523 228.344869 \nL 110.284705 228.511322 \nL 110.436886 228.419251 \nL 111.197795 228.525334 \nL 111.349977 228.474747 \nL 111.654341 228.55922 \nL 111.958705 228.446116 \nL 112.263068 228.743975 \nL 112.719614 228.4212 \nL 112.871795 228.663026 \nL 113.480523 228.588604 \nL 113.632705 228.331896 \nL 114.08925 228.635492 \nL 114.241432 228.43238 \nL 114.697977 228.80834 \nL 114.850159 228.801398 \nL 115.154523 228.511427 \nL 115.458886 228.563285 \nL 115.611068 228.267242 \nL 115.915432 228.412425 \nL 116.219795 228.729603 \nL 116.828523 228.693917 \nL 117.132886 228.424711 \nL 117.285068 228.577959 \nL 117.589432 228.553033 \nL 117.893795 228.737984 \nL 118.198159 228.790337 \nL 118.350341 228.597737 \nL 118.502523 228.689847 \nL 118.806886 228.228471 \nL 119.11125 228.48877 \nL 119.263432 228.488419 \nL 119.567795 228.86259 \nL 119.719977 228.853575 \nL 119.872159 228.510034 \nL 120.024341 228.478304 \nL 120.480886 228.828733 \nL 120.633068 228.807246 \nL 120.937432 228.411444 \nL 121.089614 228.816216 \nL 121.546159 228.55305 \nL 121.850523 228.835676 \nL 122.002705 228.583478 \nL 122.154886 228.554529 \nL 122.45925 228.736524 \nL 122.611432 228.694655 \nL 122.763614 228.507279 \nL 123.067977 228.649727 \nL 123.220159 228.629343 \nL 123.372341 228.488565 \nL 123.676705 228.877831 \nL 124.13325 228.587534 \nL 124.285432 228.874659 \nL 124.437614 228.871305 \nL 124.741977 228.58585 \nL 124.894159 228.691499 \nL 125.046341 228.624839 \nL 125.655068 228.941136 \nL 125.959432 228.606886 \nL 126.111614 228.722949 \nL 126.720341 228.666012 \nL 126.872523 228.903254 \nL 127.024705 228.736289 \nL 127.633432 228.733192 \nL 127.785614 228.615081 \nL 127.937795 228.651219 \nL 128.089977 228.390487 \nL 128.242159 228.717167 \nL 128.394341 228.612853 \nL 128.850886 228.884509 \nL 129.003068 228.652637 \nL 129.459614 228.556978 \nL 129.763977 229.004736 \nL 130.82925 228.707921 \nL 131.437977 228.456225 \nL 131.894523 228.863157 \nL 132.198886 228.777632 \nL 132.50325 228.764667 \nL 133.264159 228.915258 \nL 133.568523 228.896427 \nL 133.872886 228.568308 \nL 134.17725 229.04305 \nL 134.633795 228.659029 \nL 134.938159 228.978037 \nL 135.242523 228.803604 \nL 135.546886 228.647747 \nL 135.699068 228.594669 \nL 135.85125 228.963061 \nL 136.003432 228.942341 \nL 136.155614 228.772744 \nL 136.307795 228.845341 \nL 136.459977 228.723718 \nL 136.916523 228.891272 \nL 137.52525 228.783482 \nL 138.742705 228.874895 \nL 138.894886 228.844528 \nL 139.047068 229.062555 \nL 140.264523 228.8276 \nL 140.416705 228.571868 \nL 140.568886 228.905466 \nL 140.721068 228.953235 \nL 140.87325 228.821144 \nL 141.177614 228.951919 \nL 141.634159 228.702077 \nL 142.242886 229.000053 \nL 142.395068 228.707227 \nL 142.54725 228.944904 \nL 142.851614 228.816786 \nL 143.308159 228.729743 \nL 143.460341 228.864757 \nL 143.764705 228.841312 \nL 144.069068 228.954784 \nL 144.677795 228.734987 \nL 144.829977 228.929275 \nL 144.982159 228.928939 \nL 145.134341 229.11029 \nL 145.286523 229.10106 \nL 145.89525 228.663324 \nL 146.199614 228.915173 \nL 146.351795 228.888205 \nL 146.656159 228.941151 \nL 146.960523 228.820214 \nL 147.264886 229.054865 \nL 147.56925 229.027459 \nL 147.873614 228.888686 \nL 148.330159 229.079282 \nL 148.634523 228.923911 \nL 149.091068 229.034861 \nL 149.699795 228.808878 \nL 150.004159 228.996378 \nL 150.156341 228.798844 \nL 151.221614 229.104393 \nL 151.525977 228.896911 \nL 152.134705 228.97947 \nL 152.286886 228.909844 \nL 152.439068 229.047855 \nL 153.504341 228.853365 \nL 153.656523 228.966484 \nL 153.808705 228.853405 \nL 153.960886 229.06767 \nL 154.113068 229.017968 \nL 154.26525 229.118088 \nL 155.330523 228.718064 \nL 155.482705 228.905154 \nL 155.787068 228.82183 \nL 155.93925 229.092262 \nL 156.091432 228.945135 \nL 156.395795 229.106185 \nL 156.700159 228.996798 \nL 156.852341 228.958969 \nL 157.156705 229.193041 \nL 157.308886 228.917381 \nL 157.765432 229.01908 \nL 158.069795 228.975618 \nL 158.526341 229.063258 \nL 159.28725 228.881716 \nL 159.439432 228.930472 \nL 159.591614 229.100304 \nL 159.743795 229.006547 \nL 159.895977 229.101174 \nL 160.200341 228.796685 \nL 160.504705 229.060529 \nL 160.656886 229.01432 \nL 160.809068 228.803599 \nL 160.96125 228.993272 \nL 161.113432 228.926633 \nL 161.265614 229.130533 \nL 161.569977 229.05309 \nL 161.722159 228.908507 \nL 161.874341 229.118814 \nL 162.026523 229.005667 \nL 162.178705 229.070142 \nL 162.330886 228.995646 \nL 162.483068 229.142451 \nL 162.787432 229.046757 \nL 163.548341 229.016103 \nL 163.700523 229.114683 \nL 164.004886 229.077959 \nL 164.30925 229.049138 \nL 164.613614 228.91669 \nL 164.917977 229.100093 \nL 165.831068 228.84412 \nL 166.135432 229.134633 \nL 166.439795 228.902475 \nL 166.744159 229.062425 \nL 167.352886 229.023414 \nL 167.65725 228.95109 \nL 168.418159 229.158056 \nL 168.570341 229.003895 \nL 168.722523 229.092315 \nL 169.179068 228.856488 \nL 169.33125 229.076469 \nL 169.635614 229.086948 \nL 170.244341 229.323248 \nL 170.548705 229.075594 \nL 170.700886 229.070256 \nL 170.853068 228.844794 \nL 171.00525 229.01851 \nL 171.157432 228.948829 \nL 171.461795 229.097594 \nL 171.613977 229.024819 \nL 171.918341 229.182053 \nL 172.374886 228.990044 \nL 172.831432 229.142501 \nL 173.135795 229.004711 \nL 173.287977 229.136613 \nL 173.592341 228.980485 \nL 174.048886 229.115484 \nL 174.201068 229.037518 \nL 174.35325 229.112652 \nL 174.505432 229.0232 \nL 174.961977 229.13462 \nL 175.266341 228.972907 \nL 175.418523 229.043557 \nL 175.570705 228.987417 \nL 175.875068 229.28393 \nL 176.179432 228.885825 \nL 176.331614 229.166428 \nL 176.635977 229.184973 \nL 176.788159 229.267542 \nL 176.940341 229.142485 \nL 177.092523 229.201053 \nL 177.396886 228.960905 \nL 177.70125 229.031256 \nL 178.005614 229.159781 \nL 178.614341 229.094853 \nL 178.918705 228.987913 \nL 179.070886 228.998057 \nL 179.37525 229.199929 \nL 179.527432 229.037244 \nL 179.831795 229.268358 \nL 180.288341 229.169418 \nL 180.592705 228.876352 \nL 181.04925 229.11455 \nL 181.353614 229.217621 \nL 181.962341 229.123529 \nL 182.266705 229.154343 \nL 182.571068 229.086159 \nL 182.875432 229.104246 \nL 185.158159 229.239163 \nL 185.310341 229.016676 \nL 185.462523 229.202316 \nL 185.614705 229.167999 \nL 185.766886 229.006686 \nL 186.07125 229.180213 \nL 186.375614 229.163156 \nL 186.527795 229.231277 \nL 186.679977 229.065129 \nL 186.984341 229.151717 \nL 187.440886 228.956952 \nL 187.74525 229.281217 \nL 187.897432 229.068461 \nL 188.049614 229.125877 \nL 188.201795 229.015567 \nL 188.353977 229.049617 \nL 188.658341 229.247965 \nL 188.810523 229.127344 \nL 189.267068 229.173492 \nL 189.723614 229.158689 \nL 192.158523 229.174178 \nL 192.462886 229.269835 \nL 192.76725 229.066135 \nL 193.223795 229.271427 \nL 193.528159 228.993998 \nL 193.832523 229.195856 \nL 194.136886 229.234243 \nL 194.897795 229.079131 \nL 195.049977 229.192768 \nL 195.202159 229.114053 \nL 195.506523 229.28652 \nL 195.810886 229.17862 \nL 196.11525 229.183736 \nL 198.550159 229.185597 \nL 198.702341 228.974516 \nL 199.158886 229.347554 \nL 199.919795 229.050519 \nL 200.071977 229.188923 \nL 200.376341 229.108531 \nL 200.528523 229.316797 \nL 201.13725 229.167813 \nL 201.745977 229.181796 \nL 201.898159 229.274835 \nL 202.050341 229.162379 \nL 202.202523 229.197914 \nL 202.354705 229.06229 \nL 202.659068 229.272678 \nL 203.115614 229.113693 \nL 203.572159 229.295747 \nL 204.028705 229.242195 \nL 204.333068 229.135 \nL 204.48525 229.183589 \nL 204.637432 229.106881 \nL 204.789614 229.284609 \nL 205.550523 229.204982 \nL 205.702705 229.107621 \nL 205.854886 229.182661 \nL 206.007068 229.102642 \nL 206.15925 229.222708 \nL 207.072341 229.10791 \nL 207.376705 229.269071 \nL 207.528886 229.143371 \nL 207.985432 229.352592 \nL 208.594159 229.229941 \nL 208.746341 229.291772 \nL 208.898523 229.168827 \nL 209.050705 229.192249 \nL 209.202886 229.024902 \nL 209.50725 229.295827 \nL 209.659432 229.344085 \nL 209.811614 229.265266 \nL 209.963795 229.376012 \nL 210.268159 229.196432 \nL 210.420341 229.323747 \nL 210.572523 229.182723 \nL 210.724705 229.288911 \nL 211.029068 229.149268 \nL 211.485614 229.253215 \nL 211.637795 229.322876 \nL 211.789977 229.189063 \nL 212.094341 229.262869 \nL 212.550886 229.193117 \nL 216.659795 229.250633 \nL 216.964159 229.414637 \nL 217.420705 229.21523 \nL 218.181614 229.255733 \nL 218.333795 229.349095 \nL 219.094705 229.179928 \nL 219.399068 229.238861 \nL 220.768705 229.282651 \nL 221.22525 229.333993 \nL 221.377432 229.140285 \nL 221.529614 229.273583 \nL 221.986159 229.336334 \nL 222.290523 229.356473 \nL 222.594886 229.189628 \nL 223.051432 229.293863 \nL 223.203614 229.147611 \nL 223.507977 229.289272 \nL 223.812341 229.192269 \nL 224.116705 229.323271 \nL 224.57325 229.231853 \nL 225.486341 229.386991 \nL 225.638523 229.274713 \nL 225.790705 229.373022 \nL 226.095068 229.079549 \nL 226.551614 229.265495 \nL 227.008159 229.301435 \nL 227.464705 229.350457 \nL 227.769068 229.288766 \nL 228.529977 229.306348 \nL 228.682159 229.230679 \nL 229.138705 229.389657 \nL 229.290886 229.261718 \nL 229.747432 229.22579 \nL 230.051795 229.136547 \nL 230.356159 229.400793 \nL 230.964886 229.216175 \nL 231.117068 229.379166 \nL 231.573614 229.230061 \nL 232.030159 229.341585 \nL 232.182341 229.35642 \nL 232.638886 229.102445 \nL 233.095432 229.348881 \nL 233.247614 229.206303 \nL 233.399795 229.359737 \nL 233.551977 229.246286 \nL 233.704159 229.338981 \nL 234.008523 229.24606 \nL 234.160705 229.381887 \nL 234.61725 229.341261 \nL 234.921614 229.354424 \nL 235.378159 229.231508 \nL 235.682523 229.292922 \nL 235.834705 229.289794 \nL 235.986886 229.429688 \nL 237.660886 229.308796 \nL 238.269614 229.341904 \nL 238.421795 229.307013 \nL 238.573977 229.151728 \nL 239.030523 229.383926 \nL 239.63925 229.422761 \nL 240.095795 229.268688 \nL 240.247977 229.356672 \nL 240.552341 229.272455 \nL 240.704523 229.328934 \nL 241.008886 229.222242 \nL 241.161068 229.338447 \nL 241.465432 229.308695 \nL 242.226341 229.459814 \nL 242.835068 229.210213 \nL 243.291614 229.301741 \nL 243.595977 229.191366 \nL 243.900341 229.310827 \nL 244.204705 229.312089 \nL 245.422159 229.252917 \nL 245.726523 229.387978 \nL 246.030886 229.355754 \nL 246.183068 229.204273 \nL 246.33525 229.377732 \nL 246.943977 229.251558 \nL 247.096159 229.414491 \nL 247.400523 229.232274 \nL 247.704886 229.406136 \nL 247.857068 229.310205 \nL 248.00925 229.369649 \nL 248.161432 229.293424 \nL 248.313614 229.418957 \nL 248.465795 229.246201 \nL 249.531068 229.359262 \nL 249.68325 229.204583 \nL 250.596341 229.382515 \nL 250.748523 229.448397 \nL 251.35725 229.320679 \nL 251.661614 229.330444 \nL 251.813795 229.430507 \nL 252.118159 229.226857 \nL 252.422523 229.279978 \nL 252.574705 229.444283 \nL 253.792159 229.382616 \nL 254.248705 229.406519 \nL 254.400886 229.224329 \nL 254.857432 229.432843 \nL 255.466159 229.282052 \nL 256.074886 229.399728 \nL 256.227068 229.442448 \nL 256.531432 229.36928 \nL 257.140159 229.304982 \nL 257.292341 229.436184 \nL 257.748886 229.341598 \nL 258.05325 229.360152 \nL 258.205432 229.409892 \nL 258.357614 229.311277 \nL 258.661977 229.30447 \nL 258.814159 229.308406 \nL 258.966341 229.189893 \nL 259.118523 229.249957 \nL 259.270705 229.114856 \nL 259.575068 229.330588 \nL 259.72725 229.281342 \nL 260.335977 229.492582 \nL 260.640341 229.305334 \nL 261.40125 229.374009 \nL 261.705614 229.310227 \nL 262.009977 229.321276 \nL 262.314341 229.429881 \nL 262.770886 229.517446 \nL 263.531795 229.280047 \nL 263.683977 229.375516 \nL 263.988341 229.329221 \nL 264.292705 229.37398 \nL 264.597068 229.431743 \nL 265.053614 229.328778 \nL 265.357977 229.421797 \nL 265.662341 229.298891 \nL 265.814523 229.317285 \nL 265.966705 229.478104 \nL 266.727614 229.3372 \nL 267.184159 229.191727 \nL 267.488523 229.469605 \nL 267.640705 229.517233 \nL 267.945068 229.412021 \nL 269.314705 229.279676 \nL 269.466886 229.421676 \nL 270.836523 229.36757 \nL 270.988705 229.484158 \nL 271.140886 229.385758 \nL 271.44525 229.510357 \nL 271.597432 229.384335 \nL 271.749614 229.437231 \nL 272.206159 229.320888 \nL 272.814886 229.371644 \nL 273.11925 229.382172 \nL 273.271432 229.303225 \nL 273.423614 229.396874 \nL 273.727977 229.330162 \nL 273.880159 229.499794 \nL 274.336705 229.224298 \nL 274.641068 229.375672 \nL 274.945432 229.450474 \nL 275.097614 229.362526 \nL 275.249795 229.439044 \nL 275.401977 229.362088 \nL 275.706341 229.431261 \nL 276.010705 229.423279 \nL 276.162886 229.378559 \nL 276.315068 229.453146 \nL 276.46725 229.401877 \nL 276.771614 229.495411 \nL 277.228159 229.426712 \nL 277.380341 229.290173 \nL 277.684705 229.379655 \nL 277.836886 229.342623 \nL 277.989068 229.449591 \nL 278.293432 229.415601 \nL 279.206523 229.361544 \nL 279.510886 229.345621 \nL 281.184886 229.440203 \nL 282.250159 229.37295 \nL 282.402341 229.464021 \nL 283.011068 229.41303 \nL 284.83725 229.377726 \nL 285.141614 229.533066 \nL 285.902523 229.399313 \nL 286.054705 229.279375 \nL 286.359068 229.391398 \nL 286.663432 229.35139 \nL 287.119977 229.476007 \nL 287.424341 229.40754 \nL 287.728705 229.529595 \nL 288.18525 229.424646 \nL 288.337432 229.546401 \nL 288.489614 229.381004 \nL 288.793977 229.417292 \nL 288.946159 229.296389 \nL 289.098341 229.409642 \nL 289.402705 229.365122 \nL 289.85925 229.424327 \nL 291.228886 229.444308 \nL 291.53325 229.306017 \nL 291.837614 229.434331 \nL 291.989795 229.389942 \nL 292.294159 229.503728 \nL 292.598523 229.394337 \nL 292.750705 229.490101 \nL 293.055068 229.399144 \nL 293.663795 229.398771 \nL 293.815977 229.464148 \nL 294.272523 229.292872 \nL 294.576886 229.415582 \nL 294.729068 229.370488 \nL 294.88125 229.508736 \nL 295.033432 229.427654 \nL 295.337795 229.441626 \nL 296.55525 229.318179 \nL 296.859614 229.51048 \nL 297.163977 229.35292 \nL 297.316159 229.297649 \nL 297.468341 229.484211 \nL 298.381432 229.442502 \nL 298.685795 229.401717 \nL 298.837977 229.452228 \nL 298.990159 229.345166 \nL 299.294523 229.466113 \nL 299.598886 229.424249 \nL 300.511977 229.400898 \nL 300.816341 229.331243 \nL 300.968523 229.495974 \nL 301.425068 229.459266 \nL 301.729432 229.36651 \nL 301.881614 229.350751 \nL 302.033795 229.488198 \nL 302.185977 229.360489 \nL 302.490341 229.499905 \nL 303.099068 229.481331 \nL 303.25125 229.281356 \nL 303.555614 229.443013 \nL 304.468705 229.411988 \nL 304.773068 229.48092 \nL 305.077432 229.492366 \nL 305.533977 229.458109 \nL 305.838341 229.216212 \nL 305.990523 229.521375 \nL 306.294886 229.508081 \nL 306.751432 229.443092 \nL 307.055795 229.346844 \nL 307.207977 229.47738 \nL 307.816705 229.465174 \nL 308.121068 229.417732 \nL 308.27325 229.333839 \nL 308.425432 229.483079 \nL 308.729795 229.487919 \nL 308.881977 229.605377 \nL 309.186341 229.479894 \nL 309.490705 229.412686 \nL 309.795068 229.503725 \nL 310.251614 229.370953 \nL 311.62125 229.432015 \nL 311.925614 229.448707 \nL 312.686523 229.460895 \nL 312.990886 229.295351 \nL 313.143068 229.498151 \nL 314.056159 229.381119 \nL 314.360523 229.434916 \nL 314.512705 229.389976 \nL 314.817068 229.505649 \nL 314.96925 229.442434 \nL 315.273614 229.517522 \nL 315.577977 229.476875 \nL 316.186705 229.5268 \nL 316.338886 229.421079 \nL 316.491068 229.482425 \nL 316.795432 229.356372 \nL 317.251977 229.432554 \nL 317.708523 229.495111 \nL 318.31725 229.284071 \nL 318.621614 229.513406 \nL 319.534705 229.431265 \nL 319.686886 229.369137 \nL 319.99125 229.46697 \nL 320.143432 229.420262 \nL 320.295614 229.521128 \nL 321.056523 229.398173 \nL 321.360886 229.533977 \nL 321.513068 229.342267 \nL 322.121795 229.560396 \nL 322.730523 229.398328 \nL 322.882705 229.373644 \nL 323.034886 229.485884 \nL 323.33925 229.387181 \nL 323.947977 229.497595 \nL 324.252341 229.329584 \nL 324.404523 229.430109 \nL 324.556705 229.33848 \nL 324.861068 229.536645 \nL 327.600341 229.506611 \nL 327.904705 229.474387 \nL 328.056886 229.393194 \nL 328.36125 229.478942 \nL 328.665614 229.430092 \nL 328.817795 229.421365 \nL 328.969977 229.528688 \nL 329.578705 229.426786 \nL 331.252705 229.462278 \nL 332.317977 229.411248 \nL 332.774523 229.545996 \nL 332.926705 229.555031 \nL 333.078886 229.448017 \nL 333.38325 229.418111 \nL 339.927068 229.516205 \nL 340.231432 229.39391 \nL 340.535795 229.516125 \nL 344.949068 229.457797 \nL 345.10125 229.353311 \nL 345.253432 229.501735 \nL 346.014341 229.50507 \nL 346.470886 229.501868 \nL 346.77525 229.407644 \nL 346.927432 229.533858 \nL 347.536159 229.511539 \nL 347.688341 229.580236 \nL 347.840523 229.462063 \nL 348.144886 229.448765 \nL 348.905795 229.529881 \nL 349.057977 229.464777 \nL 349.514523 229.582939 \nL 350.275432 229.386924 \nL 350.427614 229.509835 \nL 350.884159 229.475564 \nL 351.036341 229.569891 \nL 351.645068 229.475932 \nL 353.623432 229.53274 \nL 353.775614 229.369479 \nL 354.232159 229.59292 \nL 355.297432 229.312494 \nL 355.601795 229.580231 \nL 356.81925 229.531953 \nL 357.123614 229.5943 \nL 357.580159 229.496152 \nL 358.341068 229.487172 \nL 358.49325 229.384094 \nL 358.645432 229.480504 \nL 358.797614 229.43098 \nL 359.101977 229.542212 \nL 359.406341 229.470732 \nL 360.015068 229.499966 \nL 360.623795 229.599336 \nL 360.928159 229.427084 \nL 361.232523 229.539465 \nL 361.993432 229.460864 \nL 362.602159 229.547009 \nL 362.754341 229.456795 \nL 363.210886 229.528872 \nL 363.363068 229.394389 \nL 363.363068 229.394389 \n\" style=\"fill:none;stroke:#008000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#pecf1bd482f)\" d=\"M 59.151614 221.778801 \nL 59.608159 211.66056 \nL 62.347432 165.58215 \nL 63.260523 157.982871 \nL 64.021432 154.561349 \nL 64.477977 153.230757 \nL 65.086705 151.856405 \nL 65.391068 151.561022 \nL 65.695432 150.879999 \nL 65.847614 150.866324 \nL 66.304159 150.322053 \nL 66.912886 149.950089 \nL 67.369432 149.692996 \nL 67.825977 149.508382 \nL 68.434705 149.264964 \nL 68.586886 149.068042 \nL 68.739068 149.070777 \nL 68.89125 149.207529 \nL 69.652159 149.037957 \nL 69.956523 148.841035 \nL 70.413068 148.957274 \nL 70.56525 149.017444 \nL 70.869614 148.779497 \nL 71.173977 148.928556 \nL 71.478341 148.790437 \nL 72.087068 148.797274 \nL 72.543614 148.702916 \nL 72.695795 148.764454 \nL 72.847977 148.687873 \nL 73.000159 148.817787 \nL 73.152341 148.63454 \nL 73.608886 148.65095 \nL 74.065432 148.615395 \nL 74.674159 148.626335 \nL 74.978523 148.812317 \nL 75.435068 148.60719 \nL 75.739432 148.59625 \nL 76.652523 148.512831 \nL 76.804705 148.43625 \nL 77.109068 148.534711 \nL 77.26125 148.462233 \nL 77.413432 148.641378 \nL 77.717795 148.583942 \nL 77.869977 148.42531 \nL 78.022159 148.564797 \nL 78.174341 148.516934 \nL 78.478705 148.622232 \nL 78.783068 148.440353 \nL 79.087432 148.522404 \nL 79.543977 148.581207 \nL 80.152705 148.41437 \nL 80.304886 148.545652 \nL 81.370159 148.413003 \nL 81.674523 148.571634 \nL 82.131068 148.440353 \nL 82.28325 148.500524 \nL 82.435432 148.355567 \nL 82.739795 148.422575 \nL 83.044159 148.500524 \nL 83.196341 148.637275 \nL 83.348523 148.402063 \nL 84.109432 148.363772 \nL 84.413795 148.549754 \nL 86.087795 148.348729 \nL 86.544341 148.567532 \nL 86.848705 148.404798 \nL 87.30525 148.330952 \nL 87.457432 148.451293 \nL 87.913977 148.356935 \nL 88.218341 148.332319 \nL 88.370523 148.432148 \nL 88.522705 148.333687 \nL 88.674886 148.426678 \nL 89.131432 148.321379 \nL 89.435795 148.508729 \nL 89.740159 148.399327 \nL 91.261977 148.3542 \nL 91.414159 148.423943 \nL 91.870705 148.298131 \nL 92.175068 148.310439 \nL 93.544705 148.326849 \nL 94.00125 148.225653 \nL 94.305614 148.38155 \nL 94.609977 148.350097 \nL 94.762159 148.452661 \nL 95.979614 148.236593 \nL 96.283977 148.333687 \nL 96.588341 148.289926 \nL 96.892705 148.400695 \nL 97.34925 148.239328 \nL 97.501432 148.380182 \nL 98.871068 148.311807 \nL 99.02325 148.41437 \nL 99.327614 148.258473 \nL 99.631977 148.266679 \nL 100.088523 148.288559 \nL 100.69725 148.280354 \nL 101.153795 148.340524 \nL 102.523432 148.259841 \nL 102.675614 148.16685 \nL 103.284341 148.330952 \nL 103.740886 148.240696 \nL 104.04525 148.258473 \nL 104.197432 148.285824 \nL 104.349614 148.195568 \nL 104.958341 148.339157 \nL 106.175795 148.295396 \nL 106.327977 148.385652 \nL 106.936705 148.196935 \nL 107.088886 148.284456 \nL 107.241068 148.147705 \nL 107.849795 148.318644 \nL 108.154159 148.244798 \nL 108.306341 148.343259 \nL 108.610705 148.184628 \nL 110.589068 148.273516 \nL 112.41525 148.195568 \nL 112.719614 148.283089 \nL 113.023977 148.201038 \nL 113.937068 148.277619 \nL 114.08925 148.1942 \nL 114.241432 148.296764 \nL 114.697977 148.132662 \nL 115.611068 148.371977 \nL 116.371977 148.181893 \nL 116.828523 148.179158 \nL 117.43725 148.261209 \nL 118.045977 148.143602 \nL 119.263432 148.278986 \nL 119.719977 148.132662 \nL 120.024341 148.266679 \nL 120.633068 148.164115 \nL 120.937432 148.285824 \nL 121.241795 148.175055 \nL 122.307068 148.190098 \nL 125.350705 148.164115 \nL 125.80725 148.180525 \nL 126.568159 148.184628 \nL 127.633432 148.181893 \nL 128.089977 148.283089 \nL 128.242159 148.180525 \nL 128.394341 148.251636 \nL 128.850886 148.149072 \nL 129.15525 148.217448 \nL 129.459614 148.258473 \nL 130.068341 148.109414 \nL 130.524886 148.165482 \nL 130.677068 148.088902 \nL 130.981432 148.202405 \nL 131.590159 148.240696 \nL 132.046705 148.132662 \nL 132.50325 148.15591 \nL 133.111977 148.138132 \nL 133.568523 148.135397 \nL 133.872886 148.244798 \nL 134.17725 148.073859 \nL 134.633795 148.216081 \nL 134.938159 148.106679 \nL 135.699068 148.233858 \nL 135.85125 148.099842 \nL 137.068705 148.151807 \nL 137.373068 148.170953 \nL 138.894886 148.136765 \nL 139.19925 148.095739 \nL 139.351432 148.157277 \nL 139.655795 148.129927 \nL 140.416705 148.231123 \nL 140.721068 148.117619 \nL 141.025432 148.151807 \nL 141.481977 148.147705 \nL 141.786341 148.157277 \nL 142.090705 148.102577 \nL 142.242886 148.093004 \nL 142.395068 148.206508 \nL 142.699432 148.143602 \nL 144.677795 148.181893 \nL 145.134341 148.047876 \nL 146.047432 148.158645 \nL 146.808341 148.136765 \nL 147.112705 148.114884 \nL 147.56925 148.114884 \nL 148.177977 148.069756 \nL 149.851977 148.168218 \nL 150.004159 148.103944 \nL 150.308523 148.16138 \nL 150.765068 148.121722 \nL 151.069432 148.116252 \nL 151.221614 148.054714 \nL 151.525977 148.12856 \nL 152.59125 148.099842 \nL 153.808705 148.165482 \nL 154.26525 148.068389 \nL 155.178341 148.173688 \nL 155.787068 148.165482 \nL 155.93925 148.057449 \nL 156.091432 148.127192 \nL 156.395795 148.072491 \nL 156.852341 148.120354 \nL 157.156705 148.036936 \nL 157.308886 148.14497 \nL 157.765432 148.109414 \nL 158.374159 148.091637 \nL 158.982886 148.106679 \nL 159.28725 148.15044 \nL 160.048159 148.118987 \nL 160.200341 148.18873 \nL 160.504705 148.079329 \nL 161.113432 148.157277 \nL 161.417795 148.073859 \nL 163.243977 148.106679 \nL 164.157068 148.036936 \nL 164.613614 148.12309 \nL 165.070159 148.094372 \nL 165.526705 148.099842 \nL 165.98325 148.103944 \nL 166.287614 148.091637 \nL 166.591977 148.077962 \nL 167.048523 148.091637 \nL 167.200705 148.010953 \nL 167.505068 148.12309 \nL 168.874705 148.117619 \nL 169.483432 148.083432 \nL 170.548705 148.073859 \nL 171.309614 148.080697 \nL 172.983614 148.099842 \nL 175.570705 148.110782 \nL 175.875068 148.005483 \nL 176.179432 148.146337 \nL 176.331614 148.041039 \nL 176.635977 148.038304 \nL 177.092523 148.039671 \nL 177.396886 148.132662 \nL 178.614341 148.095739 \nL 179.070886 148.131295 \nL 179.37525 148.056081 \nL 179.679614 148.035569 \nL 180.440523 148.083432 \nL 180.744886 148.112149 \nL 181.810159 148.069756 \nL 183.331977 148.090269 \nL 183.788523 148.034201 \nL 184.245068 148.075227 \nL 184.701614 148.046509 \nL 185.158159 148.012321 \nL 185.310341 148.106679 \nL 185.614705 148.051979 \nL 185.919068 148.068389 \nL 186.527795 148.023261 \nL 186.832159 148.050611 \nL 188.201795 148.099842 \nL 193.375977 148.065654 \nL 193.528159 148.125825 \nL 193.984705 148.024628 \nL 195.202159 148.072491 \nL 196.267432 148.023261 \nL 196.876159 148.054714 \nL 197.180523 148.051979 \nL 197.637068 148.061551 \nL 199.006705 148.031466 \nL 199.311068 148.010953 \nL 203.419977 148.012321 \nL 308.729795 147.94121 \nL 309.034159 147.949415 \nL 311.316886 147.971295 \nL 311.925614 147.95215 \nL 321.66525 147.97403 \nL 322.121795 147.915227 \nL 322.882705 147.975398 \nL 323.947977 147.93574 \nL 324.556705 148.000013 \nL 324.861068 147.933005 \nL 326.078523 147.956253 \nL 326.991614 147.943945 \nL 327.600341 147.94668 \nL 328.209068 147.964458 \nL 337.339977 147.950783 \nL 337.948705 147.965825 \nL 344.796886 147.94668 \nL 346.014341 147.928902 \nL 356.81925 147.931637 \nL 357.580159 147.93574 \nL 358.797614 147.965825 \nL 359.406341 147.94668 \nL 363.363068 147.986338 \nL 363.363068 147.986338 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 239.758125 \nL 43.78125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 239.758125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 22.318125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_18\">\n    <!-- PERFORMANCE -->\n    <g transform=\"translate(165.509062 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      <path d=\"M 44.390625 34.1875 \nQ 47.5625 33.109375 50.5625 29.59375 \nQ 53.5625 26.078125 56.59375 19.921875 \nL 66.609375 0 \nL 56 0 \nL 46.6875 18.703125 \nQ 43.0625 26.03125 39.671875 28.421875 \nQ 36.28125 30.8125 30.421875 30.8125 \nL 19.671875 30.8125 \nL 19.671875 0 \nL 9.8125 0 \nL 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.578125 72.90625 50.734375 67.671875 \nQ 56.890625 62.453125 56.890625 51.90625 \nQ 56.890625 45.015625 53.6875 40.46875 \nQ 50.484375 35.9375 44.390625 34.1875 \nz\nM 19.671875 64.796875 \nL 19.671875 38.921875 \nL 32.078125 38.921875 \nQ 39.203125 38.921875 42.84375 42.21875 \nQ 46.484375 45.515625 46.484375 51.90625 \nQ 46.484375 58.296875 42.84375 61.546875 \nQ 39.203125 64.796875 32.078125 64.796875 \nz\n\" id=\"DejaVuSans-82\"/>\n      <path d=\"M 9.8125 72.90625 \nL 51.703125 72.90625 \nL 51.703125 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.109375 \nL 48.578125 43.109375 \nL 48.578125 34.8125 \nL 19.671875 34.8125 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-70\"/>\n      <path d=\"M 39.40625 66.21875 \nQ 28.65625 66.21875 22.328125 58.203125 \nQ 16.015625 50.203125 16.015625 36.375 \nQ 16.015625 22.609375 22.328125 14.59375 \nQ 28.65625 6.59375 39.40625 6.59375 \nQ 50.140625 6.59375 56.421875 14.59375 \nQ 62.703125 22.609375 62.703125 36.375 \nQ 62.703125 50.203125 56.421875 58.203125 \nQ 50.140625 66.21875 39.40625 66.21875 \nz\nM 39.40625 74.21875 \nQ 54.734375 74.21875 63.90625 63.9375 \nQ 73.09375 53.65625 73.09375 36.375 \nQ 73.09375 19.140625 63.90625 8.859375 \nQ 54.734375 -1.421875 39.40625 -1.421875 \nQ 24.03125 -1.421875 14.8125 8.828125 \nQ 5.609375 19.09375 5.609375 36.375 \nQ 5.609375 53.65625 14.8125 63.9375 \nQ 24.03125 74.21875 39.40625 74.21875 \nz\n\" id=\"DejaVuSans-79\"/>\n      <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n      <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n      <path d=\"M 9.8125 72.90625 \nL 23.09375 72.90625 \nL 55.421875 11.921875 \nL 55.421875 72.90625 \nL 64.984375 72.90625 \nL 64.984375 0 \nL 51.703125 0 \nL 19.390625 60.984375 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-78\"/>\n      <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-67\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-80\"/>\n     <use x=\"60.302734\" xlink:href=\"#DejaVuSans-69\"/>\n     <use x=\"123.486328\" xlink:href=\"#DejaVuSans-82\"/>\n     <use x=\"192.96875\" xlink:href=\"#DejaVuSans-70\"/>\n     <use x=\"250.488281\" xlink:href=\"#DejaVuSans-79\"/>\n     <use x=\"329.199219\" xlink:href=\"#DejaVuSans-82\"/>\n     <use x=\"398.681641\" xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"484.960938\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"553.369141\" xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"628.173828\" xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"697.998047\" xlink:href=\"#DejaVuSans-69\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 276.146875 59.674375 \nL 371.58125 59.674375 \nQ 373.58125 59.674375 373.58125 57.674375 \nL 373.58125 29.318125 \nQ 373.58125 27.318125 371.58125 27.318125 \nL 276.146875 27.318125 \nQ 274.146875 27.318125 274.146875 29.318125 \nL 274.146875 57.674375 \nQ 274.146875 59.674375 276.146875 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 278.146875 35.416562 \nL 298.146875 35.416562 \n\" style=\"fill:none;stroke:#008000;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_19\">\n     <!-- average loss -->\n     <g transform=\"translate(306.146875 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n       <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"181.982422\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"223.095703\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"284.375\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"347.851562\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"409.375\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"441.162109\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"468.945312\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"530.126953\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"582.226562\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 278.146875 50.094687 \nL 298.146875 50.094687 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_20\">\n     <!-- accuracy -->\n     <g transform=\"translate(306.146875 53.594687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pecf1bd482f\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "axis = [i for i in range(completed_epochs+1)]\n",
    "one = [1 for i in range(completed_epochs+1)]\n",
    "zero = [0 for i in range(completed_epochs+1)]\n",
    "_ = plt.plot(axis, one, linestyle = 'dashed', color = 'black')\n",
    "_ = plt.plot(axis, zero, linestyle = 'dashed', color = 'black')\n",
    "_ = plt.plot(axis, performance_training[1], label = 'average loss', color = 'green')\n",
    "__ = plt.xlabel('epoch') \n",
    "axis = [i for i in range(completed_epochs+1)]\n",
    "_ = plt.plot(axis, performance_training[2], label = 'accuracy', color = 'red')\n",
    "__ = plt.xlabel('epoch') \n",
    "___ = plt.ylabel('loss & accuracy')\n",
    "____ = plt.title('PERFORMANCE')\n",
    "_____ = plt.legend()"
   ]
  },
  {
   "source": [
    "### Testing function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, criterion = None):\n",
    "    if criterion is not None:\n",
    "        loss_meter_test = AverageMeter()\n",
    "    accuracy_meter_test = AverageMeter()\n",
    "\n",
    "    ###########\n",
    "    # TESTING #\n",
    "    ###########\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y) if criterion is not None else None\n",
    "            acc = accuracy(y_hat, y)\n",
    "            if criterion is not None:\n",
    "                loss_meter_test.update(loss.item(), X.shape[0])\n",
    "            accuracy_meter_test.update(acc, X.shape[0])\n",
    "    print('#####################')\n",
    "    print('# TESTING COMPLETED #')\n",
    "    print('#####################')\n",
    "    print(f\"loss {loss_meter_test.sum if criterion is not None else '--':.4f} (avg {loss_meter_test.avg if criterion is not None else '--':.4f}) - accuracy {accuracy_meter_test.avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#####################\n# TESTING COMPLETED #\n#####################\nloss 2.8705 (avg 0.0000) - accuracy 1.0000\n"
     ]
    }
   ],
   "source": [
    "test(model, testloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}